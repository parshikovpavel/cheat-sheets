https://blog.zihengni.com/

# Порядок решения алгоритмических задач

- написать тестовые примеры.
- разработать алгоритмы решения
- оценить алгоритмы решения с точки зрения сложности по памяти (*space*) и времени (*time*)
- как правило, подходит алгоритм с наименьшей сложностью по времени и приемлемой сложностью по памяти.
- написать код

## Примеры граничных случаев

Нужно начать с написания тестовых примеров. Прежде всего продумать граничные случаи. Также эти тестовые примеры можно рассматривать, как способ решения задачи через тестирование (TDD). Они позже позволят протестировать полученный алгоритм.

Например, аргументом является бинарный массив. 

Граничные варианты:

```php 
[1, 1, 1, 1, 1] # одни 1
[0, 0, 0, 0, 0] # одни 0
[1, 1, 1, 0, 0] # 1 в начале
[0, 0, 1, 1, 1] # 1 в конце
[]              # пустой массив 
```

## Разработать алгоритм

Если требуется написать некоторый алгоритм, его можно выразить на листе в виде функции:

```
f(arg1, arg2, ...) = result
```

Алгоритм следует разрабатывать так:

- написать алгоритм для решения задачи в общем случае. 
- если этот алгоритм не покрывает некоторые граничные случаи (например, пустые входные параметры), отдельно добавить обработку граничных случаев.

Из всех алгоритмов лучше всего выбрать:

- более компактный и понятный
- который не обрабатывает отдельно какие-то краевые условия, а уже их учитывает в общем решении. Это упрощает код и поддержку кода.

# Алгоритмы

Репозиторий https://github.com/parshikovpavel/algorithmic-problems

## Итерация и рекурсия 

### Iteration 

Итерация (*iteration*) – многократное повторение действия, при котором выходные данные (*output*) одной итерации являются входными данными (*input*) для следующей.

Для итерации результат вычисляется:

- начальное значение равно некоторому константному значению:

   *f<sub>0</sub> = const*

- на каждой следующей итерации значение рассчитывается на основе результата предыдущей итерации:

  *f<sub>n+1</sub> = f ( f<sub>n</sub> )*

Пример вычисления *x<sup>n</sup>*:

```php
function pow($x, $n) {
  $result = 1;

  for ($i = 0; $i < $n; $i++) {
    $result *= $x;
  }

  return $result;
}
```

Преимущество:

- по сравнению с *recursion* требуется меньше памяти для хранения только одного контекста.

### Recursion

Рекурсия (*recursion*) — вызов функции из неё же самой, непосредственно (*простая рекурсия*) или через другие функции (*сложная* или *косвенная рекурсия*).

Одна и та же задача может быть решена через рекурсии и через итерации. Однако рекурсия – это совершенно другой подход к осмыслению задачи.

Пример вычисления *x<sup>n</sup>*:

```php
function pow($x, $n) {
  if ($n == 0) {
    return 1;
  } else {
    return $x * pow($x, $n - 1);
  }
}
```



Рекурсивная функция определяется:

- базовый случай (*base case*, база рекурсии) или терминальный случай (*terminating case*) – условие и формула, которые могут быть вычислены тривиально, не выполняя рекурсивного вызова. При этом  последовательность рекурсивных вызовов прерывается и происходит возврат. Правильно написанная рекурсивная функция должна гарантировать, что через конечное число рекурсивных вызовов будет достигнут *terminating case*.

  Вычисления продолжаются до тех пор, пока не будет достигнут *base case*:

  *f<sub>0</sub> = const*

  Для функции `pow()` :

  - условие – `n == 0`
  - формула – `1`

- рекурсивный случай (*recursive case*) или шаг рекурсии (*recursive step*) – условие, при котором функция вызывает сама себя. И формула, которая содержит хотя бы один рекурсивный вызов функции. 

  При этом значение определяется как некоторая функция *f* от более простого варианта той же задачи. Каждый последующий шаг упрощает задачу все больше и больше:

  *f<sub>n</sub> = f ( f <sub>n-1</sub> )*

  Для функции `pow()` :

  - условие – `n > 0`
  - формула – `$x * pow($x, $n - 1)`

- список параметров, которые передаются в рекурсивную функцию. Это как правило:
  - один из параметров – счетчик, изменяющий свое значение  (`$n`) . 
  - другие параметры  – те, над которым осуществляется обработка (`$x`).

С каждым рекурсивным вызовом исходная задача (*input problem*) должна упрощаться, пока не будет достигнут *terminating case*. 

Недостаток:

- рекурсия требует много памяти для хранения контекстов, количество хранимых контекстов зависит от глубины рекурсии. При чрезмерно большой глубине рекурсии может наступить переполнение стека вызовов.

### Замена *iteration* и *recursion*

Рекурсию можно заменить на цикл. Варианты:

- Иногда цикл – более нагляден и естественен для задачи:

  ```php
  # Цикл
  $array = [];
  for ($i = 0; $i < 10; $i++) {
      $array[] = $i;
  }
  
  # Рекурсия
  $array = [];
  recursion(0, 10, $array);
  function recursion($i, $n, &$array)
  {
      if ($i >= $n) {
          return;
      }
      $array[] = $i;
      recursion(++$i, $n, $array);
  }
  ```

- Иногда рекурсивная форма может быть структурно проще и нагляднее, в особенности, когда сам реализуемый алгоритм по сути рекурсивен. 

Один из простейших подходов к замене рекурсии на цикл – заменить автоматическое сохранение контекста в стеке вызовов на ручное выполнение тех же операций. 

## Brute-force search

*Brute-force search* (поиск полным перебором) – алгоритм перебора всех возможных *candidate*'s с проверкой, удовлетворяет ли он требованиям задачи.

Основной недостаток – для многих реальных проблем число *candidate*'s очень велико. 

<u>Алгоритм</u>

Дано:

- `P` – исходные данные
- функции:
  - `first(P)` – сгенерировать первый *candidate*.
  - `next(P, c)` – сгенерировать следующий *candidate*, сразу после `c`.
  - `valid(P, c)` – возвращает `true`, если `c` – это решение `P`
  - `output(P, c)` – вывести `c` как решение `P`.

```php
c = first(P);
while (c !== null) {
   if (valid(P, c)) {
        output(P, c);
   }
   c = next(P, c); 
}
```

## Backtracking

*Backtraching* (перебор с возвратом) – алгоритм, который постепенно создает кандидатов для решения и отбрасывает кандидатов (*backtracks*), как только определяет, что кандидат не может привести к *valid solution*. Используется для решения *constraint satisfaction problems* (задач удовлетворения ограничений). 

*Backtracking* сводится к последовательному *extension* (расширению) *of partial candidate* (частичного кандидата). Если на очередном шаге такое *extension* провести не удается, то возвращаются к более короткому *partial candidate* и продолжают поиск дальше. Для ускорения необходимо как можно раньше выявлять заведомо неподходящие варианты.

*Backtracking* может быть применен только для *problem*'s, которые позволяют проверить *partial candidate solution* (решение частичного кандидата), т.е. протестировать – может ли он привести к *valid solution*. В этом случае *backtracking* позволяет гораздо быстрее найти решение, чем *brute-force search*, т.к. позволяет заранее исключить многие *candidate*'s. 

<u>Алгоритм</u>

Дано:

- `P` – исходные данные
- функции:
  - `root(P)` – сгенерировать *partial candidate* в корне *search tree*.
  - `reject(P,c)` – возвращает `true`, если *partial candidate* должен быть отброшен и не приводит к *valid solution*
  - `first(P, c)` – сгенерировать первое *extension* (расширение) *of candidate* `c`.
  - `next(P, c)` – сгенерировать следующее *extension* (расширение) *of candidate* `c`.
  - `accept(P, c)` – возвращает `true`, если `c` – это решение `P`
  - `output(P, c)` – вывести `c` как решение `P`.

```php
bt(root(P));

function bt(c) 
{
    if reject(P, c) {
        return
    }
    if accept(P, c) {
        output(P, c)
    }
    s = first(P, c)
    while (s !== null) {
        bt(s);
        s = next(P, s);
    }
}    
```



## Sliding window

https://medium.com/outco/how-to-solve-sliding-window-problems-28d67601a66

Sliding window (скользящее окно)

Используется для решения задач, где 

- Задача включает упорядоченную структуру данных (`array` или `string`)
- Необходимо найти какой-то наилучший поддиапазон в массиве/подстроку в строке, например, самая длинная или самая короткая последовательность чего-то, что точно соответствует заданному условию (например, подстрока с уникальными символами).
- Существует очевидное *brute force* решение, которое работает со сложность *O(n<sup>k</sup>)* или *O(k<sup>n</sup>)*.

В итоге получается алгоритм с *time complexity* *O(n)* и *space complexity* *O(1)*.

При *brute force* выбрасывается много полезной информации, когда рассматриваются *window*'s фиксированной длины и многократно пересматриваются одни и те же части строки.

*Sliding window* – некоторый текущий поддиапазон в массиве/строке, который определяется двумя указателями:

- индекс начала окна
- индекс конца окна.

Эти указатели скользят в процессе решения задачи.

Обычно также хранится текущее лучшее решение и некоторая другая текущая информацию об окне, которая занимает *O(1)* места. 

<u>Виды задач.</u>

По размеру окна:

- фиксированный размер окна
- нефиксированный размер окна. Необходимы два указателя для хранения индекса начала и конца окна.

Виды движения указателя для нефиксированного размера окна:

- при движении вдоль строки, один указатель идет впереди, а другой идет следом за ним ([Longest Substring Without Repeating Characters](https://github.com/parshikovpavel/algorithmic-problems/blob/master/src/LongestSubstringWithoutRepeatingCharacters/description.md))
- один указатель двигается сначала строки, а другой с конца строки (задача про воду).

## Space-time trade-off

Компромисс времени и памяти(space-time trade-off [speɪs-taɪm ˈtreɪdɒf]) —подход к решению ряда задач, учитывающий обратную зависимость требуемого объёма памяти и скорости выполнения программы. Благодаря значительному снижению стоимости хранения мегабайта (по сравнению с ростом тактовой частоты процессора), в последнее время широко используются приемы, использующие дополнительную память для уменьшения времени вычислений.

Как правило, *space-time trade-off* решается так – используется самый быстрый алгоритм, который поместился бы в доступной памяти.

Ниже приведены примеры применения:

#### Lookup table

Таблица поиска (lookup table) — это структура данных, обычно массив или ассоциативный массив, используемая с целью заменить вычисления на извлечение значения из памяти.

Некоторые задачи, например, обращение односторонней функции, вычисление функции для отображения элементов одного множества (например, символов) в другое множество (например, в UTF8 коды символов) решаются либо в результате трудоемких вычислений, либо полным перебором. Однако результат может быть получен за константное время с использованием т. н. таблиц поиска (lookup tables). 

Идея такова: вместо того, чтобы производить вычисления или перебирать все варианты на ходу, можно их все вычислить заранее один раз и хранить в памяти. Все предвычисленные варианты сводятся в таблицу поиска, в которой можно найти решение за константное время. Увеличение скорости может быть значительным, так как получить данные из памяти зачастую быстрее, чем выполнить трудоёмкие вычисления.

Классический пример использования таблиц поиска — вычисление значений тригонометрических функций, например, синуса.

Размер таблицы поиска со всеми возможными вариантами может быть довольно большим, настолько что она не влезает в доступную память. Поэтому в некоторых алгоритмах вычисляют только часть вариантов, а затем их как-то используют для нахождения окончательного решения. Т.е. space-time trade-off  увеличивается в сторону time за счет уменьшения space. Например:

·  при построении радужных таблиц. 

·  при построении таблицы непрерывной функции, можно использовать таблицу поиска в сочетании с простыми вычислениями — интерполяцией.

Однако использование таблиц поиска в тех задачах, где извлечение из памяти данных оказывается медленнее, чем их вычисление, приводит к понижению скорости работы. Извлечение из памяти данных может быть неэффективным для больших таблиц поиска, которые не помещаются в кеш, также PHP выполняет ряд служебных операций при обращении к элементу ассоциативного массива.

#### Радужные таблицы

Радужная таблица (rainbow table) — специальный вариант таблиц поиска(lookup table) для обращения односторонних криптографических хеш-функций, использующий механизм разумного компромисса между временем поиска по таблице и занимаемой памятью(space-time trade-off). Р

Использование обычной таблицы поиска невозможно, т.к. ее размер быстро растет с ростом длины строки. Альтернативным вариантом является хранение только первых элементов т.н. «цепочек хешей». Это потребует больше вычислений для поиска пароля, но значительно уменьшит количество требуемой памяти

Пусть у нас есть хеш-функция H с длиной хеша n и конечное множество паролей P. Построить полную таблицу соответствия                                 невозможно, т.к. это требует много памяти.

Цепочки хешей — метод для уменьшения этого требования к объёму памяти. Главная идея — определение функции редукции   , т.е. хешу пароля   сопоставляет какой-то другой пароль   . Заметим, что R не является обращением хеш-функции. Начиная с исходного пароля и попеременно применяя к каждому полученному значению H и R, мы получим цепочку перемежающихся паролей и хешей. Например, цепочка может выглядеть так:

  

Для генерации таблицы мы выбираем случайное множество начальных паролей из P, вычисляем цепочки некоторой фиксированной длины k для каждого пароля и сохраняем только первый и последний пароль из каждой цепочки.

Для каждого хеша h, значение которого мы хотим обратить (найти соответствующий ему пароль), вычисляем последовательность R(…R(H(R(h)))…). Если какое-то из промежуточных значений совпадет с каким-нибудь концом какой-либо цепочки, мы берём начало этой цепочки и восстанавливаем её полностью.

Увеличение длины цепочки уменьшает размер таблицы, но увеличивает время поиска нужного элемента в цепочке.

Недостаток такого алгоритма: возможность слияния цепочек – генерация одного и того же значения в разных цепочках в разных позициях последовательностей. В итоге все значения, сгенерированные после совпадения, будут одинаковыми в обеих цепочках, что сужает количество покрываемых паролей

В радужных таблицах используется усовершенствованный вариант цепочек хешей. Вместо одной функции редукции R, на каждом шаге используется своя функция редукции R1, R2, …, Rk. При таком подходе две цепочки могут слиться только при совпадении значений на одной и той же итерации. Следовательно, достаточно проверять на коллизии только конечные значения цепочек, что не требует дополнительной памяти. На конечном этапе составления таблицы можно найти все слившиеся цепочки, оставить из них только одну и сгенерировать новые, чтобы заполнить таблицу необходимым количеством различных цепочек.

Такой алгоритм позволяет наращивать таблицу поиска без опасности получить большое количество слияний цепочек. Процесс генерации радужных таблиц легко поддается распараллеливанию.

##### Защита с помощью соли

Методом защиты от взлома с помощью радужных таблиц является использование в функции хеширования «соли».  Соль  (salt [sɔːlt])— строка, которая подмешивается в хеш-функцию вместе с паролем. Применение соли называется «посолить» пароль.

Существует множество возможных схем смешения затравки и пароля. Например:

$hash = *md5*($password.$salt);
 $hash = *md5*(*md5*($password).$salt);

Для восстановления такого пароля взломщику необходимы таблицы для всех возможных значений соли. По сути, соль увеличивает длину и сложность пароля. Для высокой надежность должна использоваться только динамическая соль, иначе она не имеет смысла, т.к. возможно будет известна злоумышленнику.

Подробнее про реализацию в PHP смотреть Хэширование паролей.

В большинстве UNIX-систем в качестве односторонней функции используется системная библиотека crypt(3). Результатом работы функции является строка, содержащая метку алгоритма хеширования, соль, собственно хеш и, опционально, другие данные (например, число раундов хеш-функции).

 

https://ru.wikipedia.org/wiki/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_%D0%BF%D0%BE%D0%B8%D1%81%D0%BA%D0%B0

 

 

## Computational complexity. Big О

Вычислительная сложность (*computational complexity*) алгоритма – зависимость количества ресурсов, необходимых для его запуска, от размера входных данных. Т.е. функция *f(n)*, где *n* – размер входных данных.

Основное внимание уделяется двум ресурсам:

- время (*time*) – определяется количеством элементарных шагов, необходимых для решения задачи
- память (*memory*), пространство (*space*) – объёмом памяти для хранения данных

*Computational complexity* зависит не только от алгоритма, но и от самих данных. Поэтому говорят о *computational complexity* :

- в лучшем случае
- в худшем случае
- в среднем случае

Некоторые  *data structure*'s, например, *Hash table*, имеют очень плохое поведение в худшем случае, но при правильном проектировании (для *Hashtabke* при правильном выборе размера) статистически никогда не дают худшего случая.

### Сравнение алгоритмов

Оценка *computational complexity* необходима для сравнения алгоритмов между собой.

Для любой задачи существует множество разных алгоритмов решения, каждый из которых обладает своими характеристиками *computational complexity*. 

Часто существует *space-time trade-off* ([1](#space-time-trade-off)), когда наблюдается обратная зависимость требуемого объёма памяти и скорости выполнения программы. Как правило, *space-time trade-off* решается так – используется самый быстрый алгоритм, который поместился бы в доступной памяти.

### Asymptotic computational complexity

Хорошая *computational complexity* и использование ресурсов (*time* и *space*) имеют существенное значение только для больших значений *n*. Т.к. иначе, простота реализации экономит больше ресурсов, чем построение эффективного алгоритма.

Поэтому наибольший смысл имеет оценка *computational complexity* для больших *n* , когда *n → ∞*.  Т.е. необходимо выполнять асимптотический анализ функции *computational complexity*. Для оценки алгоритма, обрабатывающего большие объемы данных, рассматривают понятие асимптотической вычислительной сложности (*asymptotic computational complexity*). 

Асимптотический анализ — метод описания поведения функции в пределе, при *n → ∞*.

<u>Упрощение функции</u>

Несмотря на то, что функция *computational complexity* в некоторых случаях может быть определена точно, в большинстве случаев искать точное её значение бессмысленно. Так как при *n → ∞* вклад константных множителей и слагаемых низших порядков становится незначительным. Поэтому при оценке *asymptotic computational complexity* учитывается только слагаемое самого высокого порядка, и не учитывают константные множители (коэффициенты).

Кроме того, мы не имеем возможности точно оценить коэффициенты в функции *computational complexity*. В частности для *tIme complexity*, неизвестно количество инструкций для выполнения каждого шага алгоритма. И это количество инструкций зависит от используемого языка программирования, компилятора, процессора.

Например, в функции *f(n)=n<sup>2</sup>+3n* при *n → ∞*, слагаемое *3n* становится пренебрежимо малым по сравнению с *n<sup>2</sup>*, поэтому про функцию *f(n)* говорят, что она «асимптотически эквивалентна *n<sup>2</sup>* при *n → ∞*, что записывают как *f(n) ∼ n<sup>2</sup>*.

В этом смысле анализ алгоритма напоминает другие математические дисциплины тем, что он фокусируется на основных свойствах алгоритма, а не на специфике какой-либо конкретной реализации.

### Big O

*Big O* (*O*, "O большое")  – **математическое** (не термин из *computer science*!!!) обозначение, которое описывает асимптотическое поведение (асимптотику при *n → ∞*) одной функции в виде другой функции. Это математическое обозначение часто используется в информатике и при анализе алгоритмов.

"*Computational complexity* алгоритма *O(f(n))* " – означает, что при *n → ∞*, количество ресурсов, необходимых для его работы, будет возрастать не быстрее, чем *K ⋅ f(n)*, где *K* – некоторая константа. Поэтому  *Big O* описывает верхнюю границу скорости роста функции. 

Буква *O* используется потому, что скорость роста функции также называется порядком функции, порядком роста функции (**order** of function, order of growth).

Существуют такие оценки *asymptotic computational complexity*:

| Notation         | Name                           | Description                                                  |
| ---------------- | ------------------------------ | ------------------------------------------------------------ |
| *f(n) = O(g(n))* | *Big O*                        | *\|f\|* сверху ограничен *g* (с точностью до постоянного множителя) асимптотически, \|f(x)\|≤ С \|g(x)\| |
| *f(n) = o(g(n))* | *Little-o, Small o,* малое *o* | *g* доминирует над *f* асимптотически, *g(x)* растет намного быстрее чем *f(x)* |
| *f(n) = Ω(g(n))* | *Big Omega*                    | *f* ограничено снизу асимптотически *g*, по нижней границе   |
| *f(n) = Θ(g(n))* | *Big Theta*                    | *f* ограничен как сверху, так и снизу асимптотически *g*, асимптотические верхняя и нижняя границы совпадают |

*Big O* может использоваться для оценки любого случая: наихудшего, наилучшего, среднего. По умолчанию (если явно не указано), указывается *Big O* для наихудшего случая, т.е. это верхняя граница используемых ресурсов для наихудшего случая.

Список классов функций, которые обычно встречаются при анализе *computational complexity* с помощью *big O* нотации. От медленно растущих к быстро растущим.

| Notation                    | Name                    | Example                                                      |
| --------------------------- | ----------------------- | ------------------------------------------------------------ |
| *O(1)*                      | Константное             | Нахождение среднего значения в отсортированном массиве<br/>Определение четности числа<br/>Поиск в *lookup table* |
| *O(log N)*                  | Логарифмическое         | Двоичный поиск в отсортированном массиве<br/>Операции в [heap](#heap) |
| *O(n)*                      | Линейное                | Поиск в неотсортированном массиве                            |
| *O(n log N)*                | Линейно-логарифмическое | Наиболее быстрые виды сортировки: *heapsort*, *mergesort* *quicksort* |
| *O(n<sup>2</sup>)*          | Квадратичное            | Перемножение двух чисел в столбик<br/>Простые алгоритмы сортировки: *bubble sort, insertion sort, selection sort* |
| *O(n<sup>3</sup>)*          | Кубическое              | Умножение двух матриц *n x n*                                |
| *O(n<sup>c</sup>)*, *c > 1* | Полиномиальное          |                                                              |
| *O(c<sup>n</sup>)*, *c > 1* | Экспоненциальное        | Решение задачи комивояжера через динамическое программирование<br/>Определение эквивалентности двух логических выражений через *brute-force search* |
| *O(n!)*                     | Факториал               | Генерация всех перестановок множества                        |

#### O(1)

При *O(1)*, *computational complexity* не зависит от *n*.

#### O(log N)

Поскольку *log<sub>b</sub>a = log<sub>c</sub>a / log<sub>c</sub>b*, т.е. логарифмы с разным основанием отличаются на постоянный множитель, в асимптотической оценке сложности часто пишут «логарифм» без упоминания основания — например, *O(log n)*.￼

#### O(n)

*O(n)* – наименьшая *tIme complexity*, когда алгоритм должен последовательно считывать весь ввод. Т.к. *tIme complexity* ввода размера *n* равна *O(n)*. 

### TIme complexity

*TIme complexity* (временная сложность)

Обычные единицы времени (секунды, минуты) не используются в оценке, т.к. они зависят от конкретного вычислителя.

*TIme complexity* оценивается путем подсчета количества элементарных операций (шагов), которые выполняются во время вычислений. Предполагается, что каждый шаг занимает постоянное время.

### P и NP классы

Буква *P* в названии означает полиномиальную сложность алгоритма (функция сложности имеет вид a<sub>0</sub>x<sup>0</sup>+…+a<sub>n</sub>a<sup>0</sup>). Если в названии есть буква *N*, то оценка сложности соответствует выполнению на недетерминированной машине Тьюринга, иначе — обычной. Детерминированная машина Тьюринга – аналог компьютера. 

Любая программа, выполненная на недетерминированной МТ за *t* шагов, может быть выполнена на детерминированной за число шагов, экспоненциально зависящее от t (*2<sup>t</sup>*). 

Класс P вмещает все те проблемы, решение которых считается «быстрым», то есть время решения которых полиномиально зависит от размера входа. Сюда относится сортировка, поиск в массиве.

Класс NP включает в себя класс P, а также некоторые проблемы, для решения которых известны лишь алгоритмы, экспоненциально зависящие от размера входа (то есть неэффективные для больших входов). В класс NP входят многие знаменитые проблемы, такие как задача коммивояжёра.







https://m.habr.com/ru/post/196560/

Существуют инструменты, измеряющие, насколько быстро работает код. Это программы, называемые *профайлерами* (profilers), которые определяют время выполнения в миллисекундах, помогая нам выявлять узкие места и оптимизировать их. Сложность алгоритма — это то, что основывается на сравнении двух алгоритмов на идеальном уровне, игнорируя низкоуровневые детали вроде реализации языка программирования, «железа», на котором запущена программа, или набора команд в данном CPU.

Максимальный элемент массива

var M = A[ 0 ]; 

for ( var i = 0; i < n; ++i ) { 

if ( A[ i ] >= M ) { 

M = A[ i ]; 

} }

В процессе анализа данного кода, имеет смысл разбить его на простые инструкции — задания, которые могут быть выполнены процессором тотчас же или близко к этому.

Для первой строки в коде выше: var M = A[ 0 ]; 

требуются две инструкции: для поиска A[0] и для присвоения значения M (мы предполагаем, что nвсегда как минимум 1). 

если мы проигнорируем содержимое тела цикла, то количество инструкций у этого алгоритма `4 + 2n` — четыре на начало цикла `for `и по две на каждую итерацию, которых мы имеем `n`штук. математическую функцию `f(n)` такую, что зная `n`, мы будем знать и необходимое алгоритму количество инструкций. Для цикла `for `с пустым телом `f( n ) = 4 + 2n`. 

Когда мы анализируем алгоритмы, мы чаще всего рассматриваем наихудший сценарий, в данном случае когда массив упорядочен по возрастанию, как, например, `A = [ 1, 2, 3, 4 ]. `*анализ наиболее неблагоприятного случая*, в наихудшем случае в теле цикла из нашего кода запускается четыре инструкции, и мы имеем `f( n ) = 4 + 2n + 4n = 6n + 4`. При анализе сложности важность имеет только то, что происходит с функцией подсчёта инструкций при значительном возрастании `n `**мы отбрасываем те элементы функции, которые при росте** `**n**` **возрастают медленно, и оставляем только те, что растут сильно.** отбросим 4 и оставим только `f( n ) = 6n`. Второй вещью, на которую можно не обращать внимания, является множитель перед `n`. Так что наша функция превращается в `f( n ) = n`. константный множитель имеет смысл отбрасывать, если мы думаем о различиях во времени компиляции разных языков программирования (ЯП). (в одном языке на операцию приходится 3 элементарных, в другом одна). Описанные выше фильтры — «отбрось все факторы» и «оставляй только наибольший элемент» — в совокупности дают то, что мы называем *асимптотическим поведением*. Для `f( n ) = 2n + 8` оно будет описываться функцией `f( n ) = n`. Говоря языком математики, нас интересует предел функции `f` при `n`, стремящемся к бесконечности. 

Например, f( n ) = 109 даст f( n ) = 1.
 Мы отбрасываем множитель в 109 * 1 , но 1 по-прежнему нужен, чтобы показать, что функция не равна нулю

юбая программа, не содержащая циклы, имеет `f( n ) = 1`, потому что в этом случае требуется константное число инструкций (конечно, при отсутствии рекурсии — см. далее). Одиночный цикл от `1` до `n`, даёт асимптотику `f( n ) = n`

Следующая [PHP](http://php.net/)-программа проверяет, содержится ли в массиве `A` размера `n `заданное значение:

 


<?php

  $exists = false;

  for ( $i = 0; $i < n; ++$i ) {

​    if ( $A[ $i ] == $value ) {

​      $exists = true;

​      break;

​    }

  }

?>

Такой метод поиска значения внутри массива называется *линейным поиском*. Это обоснованное название, поскольку программа имеет `f( n ) = n`.

Два вложенных цикла дадут нам асимптотику вида `f( n ) = n`2.

если у нас имеется серия из последовательных for-циклов, то асимптотическое поведение программы определяет наиболее медленный из них.

Когда мы выясняем точную асимптотику `f`, мы говорим, что наша программа — `Θ( f( n ) )`. Например, в примерах выше программы `Θ( 1 )`, `Θ( n`2 `)` и `Θ( n`2` )`, соответственно. `Θ( n )` произносится как «тета от n»

Можно так же написать, что `2n ``∈`` ``Θ``( n )`, что произносится: «два n принадлежит тета от n».

\1.  n6 + 3n ∈ Θ( n6)

\2.  2n + 12 ∈ Θ( 2n )

\3.  3n + 2n ∈ Θ( 3n )

\4.  nn + n ∈ Θ( nn )

**Мы называем эту функцию (т.е. то, что пишем** `**Θ( здесь )**`**) \*временной сложностью\*, или просто \*сложностью\* нашего алгоритма.** Таким образом, алгоритм с `Θ( n )` имеет сложность `n`. Также существуют специальные названия для `Θ( 1 )`, `Θ( n )`, `Θ( n`2` )` и `Θ( log( n ) )`, потому что они встречаются очень часто. Говорят, что `Θ( 1 )` — алгоритм с *константным временем*, `Θ( n )` — *линейный*, `Θ( n`2` )` — *квадратичный*, а `Θ( log( n ) )` — *логарифмический* (не беспокойтесь, если вы ещё не знаете, что такое логарифм — скоро мы об этом поговорим).

В реальной жизни иногда проблематично выяснить точное поведение алгоритма тем способом, который мы рассматривали выше. Особенно для более сложных примеров. Однако, мы можем сказать, что поведение нашего алгоритма никогда не пересечёт некой границы.

`O( n`2` )` произносится как «большое О от `n` в квадрате». Это говорит о том, что наша программа асимптотически не хуже, чем `n`2. Она будет работать или лучше, или также. Таким образом, `Θ( n`2`)` для программы — это и `O( n`2` )` тоже.

А вот обратное верно не всегда. Например, любой код с `Θ( n )` также и `O( n`2` )` в дополнение к `O( n )`. Обобщая: любая программа с `Θ( a )` является `O( b )` при `b` худшем, чем `a`. 

Поскольку `О`-сложность алгоритма представляет собой *верхний предел* его настоящей сложности, которую, в свою очередь, отображает `Θ`, то иногда мы говорим, что `Θ` даёт нам *точную оценку*. Если мы знаем, что найденная нами граница не точна, то можем использовать строчное `о`, чтобы её обозначить. Например, если алгоритм является `Θ( n )`, то его точная сложность — `n`. Следовательно, этот алгоритм `O( n )` и `O( n`2` )` одновременно. Поскольку алгоритм `Θ( n )`, то `O( n )` определяет границу более точно. А `O( n`2` )` мы можем записать как `о( n`2` )` (произносится: «маленькое o от n в квадрате»), чтобы показать, что мы знаем о нестрогости границы.

Если же мы поступим противоположным образом, сделав имеющийся код **лучше**, и найдём сложность того, что получится, то мы задействуем Ω-нотацию. Таким образом, `Ω` даёт нам сложность, лучше которой наша программа быть не может. `Ω` даёт нам *нижнюю границу* сложности нашего алгоритма  Аналогично `ο`, мы можем писать `ω`, если знаем, что этот предел нестрогий. Например, `Θ( n`3` )` алгоритм является `ο( n`4` )` и `ω( n`2` )`. `Ω( n )` произносится как «омега большое от n», в то время как `ω( n )` произносится «омега маленькое от n».

\1.  Θ( √n ) Строгие пределы будут теми же, что и Θ-сложность, т.е. O( √n ) и Ω( √n ) соответственно.

*Логарифм* — это операция, которая при применении её к числу делает его гораздо меньше (подобно взятию квадратного корня). Далее: подобно тому, как взятие квадратного корня является операцией, обратной возведению в квадрат, логарифм — обратная операция возведению чего-либо в степень. 

Как только вы проанализировали сложность вашего алгоритма, так сразу можете получить и грубую оценку того, как быстро он будет работать, приняв, что в секунду выполняется 1 000 000 команд. Их количество считается из полученной вами функции асимптотической оценки, описывающей алгоритм. Например, вычисление по алгоритму с Θ( n ) займёт около секунды при n = 1 000 000.

Рекурсивная функция вычисления факториала  будет вычисляться `n` раз. эта функция является `Θ( n )`.

Поиск в отсортированном массиве может выполняться половинным делением (бинарный поиск)

Наихудшим сценарием для данной задачи будет вариант, когда массив в принципе не содержит искомое значение. В этом случае мы начинаем с массива размером `n` на первом рекурсивном вызове, `n / 2` на втором, `n / 4` на третьем и так далее. В общем, наш массив разбивается пополам на каждом вызове до тех пор, пока мы не достигнем единицы.

0-я итерация: `n`
 1-я итерация: `n / 2`
 2-я итерация: `n / 4`
 3-я итерация: `n / 8`
 …
 i-я итерация: `n / 2`i
 …
 последняя итерация: `1`

на i-й итерации массив имеет `n / 2`i элементов. Количество итераций разбиения массива `1 = n / 2`i  i = log( n ) улучшение асимптотического времени выполнения программы часто чрезвычайно повышает её производительность.

Сортировка слиянием


Сложность слияния Θ( n ) в каждой строке, а всего строк – г*лубина* рекурсивного дерева, будет `log( n )`. общая сложность `mergeSort`: `Θ( n * log( n ) )`

## Алгоритмы кеширования

Также **алгоритмы/политики вытеснения/замещения** – программа для управления кэшем. Идея алгоритмов: когда кэш заполнен нужно выбрать, что именно удалить, чтобы записать новую, более актуальную информацию.  

**Least recently used (LRU, вытеснение давно неиспользуемых)**: в первую очередь вытесняются записи, неиспользованные дольше всех. Требует отслеживания того, что и когда использовалось, что может оказаться довольно накладно.

**Most Recently Used (****MRU, наиболее недавно использовавшийся)**: в первую очередь вытесняется последний использованный элемент. Полезны, когда чем старше элемент, тем больше обращений к нему происходит.

**Least-Frequently Used (****LFU, наименее часто используемый)** – подсчитывает, как часто используется элемент. Те элементы, обращения к которым происходят реже всего, вытесняются в первую очередь.

Существуют улучшенные версии алгоритмов, промежуточные между LRU и LFU. Кроме того можно учитывать: 

·  Хранить дольше данные, которые дороже получать.

·  Вытеснять быстрее записи большого размера, т.к. это позволит сохранить несколько записей поменьше. 

## Проблемы многозадачности

### Проблема ABA

Проблема ABA возникает при синхронизации, когда ячейка памяти читается дважды, оба раза прочитано одинаковое значение, и признак «значение одинаковое» трактуется как «ничего не менялось». Однако, другой поток может выполниться между этими двумя чтениями, поменять значение (A->B), сделать что-нибудь ещё (например, обратиться к разделяемой памяти) и восстановить старое значение (B->A). Таким образом, первый поток обманется, считая, что не поменялось ничего, хотя второй поток уже разрушил это предположение.

Пример для примера с разделяемой памятью:

·  Процесс   читает значение A из разделяемой памяти

·    , вытесняется, позволяя выполняться   

·    меняет значение A на B и обратно на A перед вытеснением

·    возобновляет работу, видит, что значение не изменилось, и продолжает работу с разделяемой памятью, хотя, возможно, что его поведение будет неправильным из-за других, скрытых изменений общей памяти (которые он не отслеживал).

Обычно с проблемой ABA сталкиваются при реализации lock-free структур и алгоритмов. Пример для lock-free стека на C++ и функции выталкивания с вершины стека:

Obj* Pop() {

   while(1) {

​    Obj* ret_ptr = top_ptr;

​    if (!ret_ptr) return NULL;

​    Obj* next_ptr = ret_ptr->next;

​    // Если верхний элемент - всё ещё ret, считаем, что никто не менял стек.

​    // (Это утверждение не всегда истинно из-за проблемы ABA)

​     // Атомарно заменяем top на next.

​    if (CompareAndSwap(top_ptr, ret_ptr, next_ptr)) {

​     return ret_ptr;

​    }

​    // Иначе - стек изменён, пробуем заново.

   }

}

Пример проблемной последовательности:

На стеке *top* → A → B → C

Процесс   :

ret_ptr = A;

next_ptr = B;

Вытеснение   . Процесс   меняет стек на *top* → A → C. Вытеснение   .

Процесс   :

CompareAndSwap(A, A, B)

Эта инструкция выполняется успешно, поскольку top == ret (оба равны A), в результате top_ptr присваивается значение указателя next_ptr, на уничтоженное значение B. 

Общий способ избавиться от проблемы: добавить к объекту, изменение которого критично, номер версии. Например, алгоритм CAS ([compare-and-swap](https://ru.wikipedia.org/wiki/Сравнение_с_обменом)) может помимо изменения самого значения также сохранять в младших битах номер версии, так чтобы последующий CAS не смог быть выполнен из-за несовпадения этих бит. Этот прием используется в распределенных транзакциях (смотрите Без требования идемпотентности (из документации Mongo, CouchDB…) )

### Сравнение с обменом (compare and swap, CAS)

Атомарная инструкция, сравнивающая значение в памяти с одним из аргументов, и в случае успеха записывающая второй аргумент в память. Поддерживается в семействах процессоров x86.

/* Псевдокод работы инструкции, возвращающей булево значение в синтаксисе языка C */

int cas( int* addr, int old, int new )

{

 if ( *addr != old )

  return 0;

 

 *addr = new;

 return 1;

}

**Пример использования**: У нас есть n процессоров, каждый из которых иногда хочет получить доступ к какому-то общему ресурсу, например, к разделяемой памяти. До начала основной работы назначим им уникальные номера от 0 до n-1. Выберем ячейку памяти, которая будет указывать, какой процессор сейчас использует ресурс. Значение -1 будет указывать, что ресурс никем не занят. Изначально поместим в неё −1. При основной работе каждый процессор должен проверить, что в ячейке находится −1, и если это так, то записать в неё свой номер. Если же ячейка занята — процессор обязан ждать, пока она не освободится.

### Неблокирующая синхронизация

Отказ от традиционных примитивов блокировки, таких, как семафоры, мьютексы и события. Разделение доступа между потоками идёт за счёт атомарных операций и специальных, разработанных под конкретную задачу, механизмов блокировки. Преимущество неблокирующих алгоритмов — в лучшей масштабируемости по количеству процессоров.

## Fulltext search

Полнотекстовый поиск (*full text searching*) – поиск ключевых слов в содержимом поля документов, хранящихся в базе данных. Результатом поиска, является набор документов, отранжированных по реливантности. 

Для организации эффективного *fulltext search* необходимо построить *fulltext index*. Для формирования *fulltext index*'а и разбора на части поискового запроса в тексте необходимо выделить *term*'ы. 

*Term* – это минимальная единица текста, по которой осуществляется индексирование и которая хранится в словаре индекса. Чаще всего *term*'ом является слово, приведенное к нормальной форме (`навыками` → `навык`). 

Процесс подготовки текста к индексированию включает:

- выделение *term*'ов

- удаление из *stop word*'s. *Stop word*'s – это часто встречающиеся в тексте слова (например, союзы), знаки препинания и т.д.

Реализация:

- для поиска используется [*inverted index*](#inverted-index)
- сложные запросы называются [булевским поиском](#булевский-поиск)
- ранжируются записи в соответствии с релевантностью по величине [TF-IDF](#tf-idf)

### Булевский поиск

Булевский поиск – полнотекстовый поиск с возможностью использовать операторы булевой алгебры в поисковом запросе (`AND`, `OR` и `NOT`).

Самый простой тип запроса – по одному *term*'у. Для такого запроса достаточно в *inverted index*'е найти список документов, соответствующий *term*'у.

Когда несколько слов указаны через пробел `java android`, под этим обычно подразумевается логическое `AND` –  `java AND android`. 

Принципы реализации:

- `AND` – в индексе по термам `java` и `android` находятся списки документов, а затем по ним делается пересечение. 

- `OR` – по спискам документов делает объединение

- префиксный поиск вида `jav*` – отлично работает, когда словарь реализован на *prefix tree*, и тогда мы просто доходим до вложенности префикса и берем все *term*'ы, которые лежат ниже.


### Ранжирование

После получения списка документов их необходимо отранжировать. 

Самые элементарные принципы ранжирования – основанные на сортировке по какому-либо свойству документа. Например, ранжирование по дате или по количеству найденных слов в документе. Такие подходы применяются редко. 

Релева́нтность — семантическое соответствие найденных документов, заданному поисковому запросу.

Наиболее часто документы ранжируются по величине *TF-IDF*, которая для конкретного документа определяется по формуле:

![tfIdf1](https://parshikovpavel.github.io/img/algorithm/tfIdf1.svg)

TF (*term frequency*, частота терма) — насколько часто *term* встречается в документе:

![tfIdf1](https://parshikovpavel.github.io/img/algorithm/tfIdf2.svg)

IDF (*inverse document frequency*, обратная частота документа) — инверсия частоты, с которой *term* встречается в документах коллекции:

![tfIdf1](https://parshikovpavel.github.io/img/algorithm/tfIdf3.svg)

Большой вес в *TF-IDF* получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах.

Пертине́нтность — соответствие найденных информационно-поисковой системой документов информационным потребностям пользователя.

## Лексикографический порядок

Лексикографический порядок — отношение порядка на множестве слов над некоторым упорядоченным алфавитом `A`. 

Слово `a` предшествует слову `b`, т.е. `a < b`, если

- либо первые `m` символов этих слов совпадают, а `m+1`-й символ слова `a` меньше `m+1`-го символа слова `b`
- либо слово `a` является началом слова `b`

Пример:

- Порядок слов в словаре:

  ```
  А < АА < ААА < ААБ < ААВ 
  ```

###Greedy algorithm

Greedy algorithm (жадный алгоритм) – алгоритм, заключающийся в принятии локально оптимальных решений на каждом этапе, допуская, что конечное решение также окажется оптимальным.

Greedy algorithm  подходят только для задач, имеющих «оптимальную подструктуру» – последовательность локально оптимальных выборов даёт глобально оптимальное решение.

Greedy algorithm дает неверное решение в том случае, если задача имеет локальные оптимальные решения, не совпадающие с глобальным оптимальным решением. Например на рисунке ниже, начиная с A, жадный алгоритм, который пытается найти максимум по наибольшему наклону, найдет локальный максимум в точке «*m*», не обращая внимания на глобальный максимум в точке «*M*».

![greedy_algorithm](https://parshikovpavel.github.io/img/algorithm/greedy_algorithm.png?2)

Поэтому алгоритм нужно преобразовать к виду, при котором промежуточные решения не будут локально оптимальными.

При поиске среди элементов массива с помощью *greedy algorithm*, данные упорядочивают (за *O(n long)*), а затем 

- двигают от худших решений к лучшим ([(167) Two Sum II - Input array is sorted](https://github.com/parshikovpavel/algorithmic-problems/blob/master/src/TwoSumII/description.md)) 
- выбирают наилучшее решение, а затем пытаются его ухудшить, чтобы сделать допустимым ([Википедия](https://ru.wikipedia.org/wiki/%D0%96%D0%B0%D0%B4%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC#%D0%9F%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D1%8B))

# Структуры данных

## Hash table

*Hash table* (хеш таблица) — это структура данных, которая может отображать ключи на значения, т.е. позволяет реализовывать ассоциативный массив. 

<u>Вычисление index'а bucket'а</u>

Имеется массив из элементов (*entry's*) размером `number_of_entries`. *Entries*  размещаются среди множества бакетов (*bucket)* размером `number_of_buckets`. Ключ (*index*) нужного *bucket*'а вычисляется на основе ключа (`key`) *entry* и размера массива *bucket*'ов (`number_of_buckets`):

```
index = f(key, number_of_buckets)
```

Часто эта функция реализуется с помощью некоторой хеш-функции (*hash function*):

```
index = hash_function(key) % number_of_buckets
```

![hashtable](https://parshikovpavel.github.io/img/algorithm/hashtable.png)

<u>`number_of_buckets`</u>

Рекомендуется выбирать `number_of_buckets = 2^N`. Тогда вместо взятия по модулю `%`, достаточно наложить маску (*mask*, *bitmask*). 

Основной показатель *hash tabl*'ицы – коэффициент загрузки (*load factor*):

```
load_factor = number_of_entries / number_of_buckets
```

При увеличении *load factor*'а выше некоторого порога, `number_of_buckets` увеличивается (обычно удваивается).

<u>Коллизии</u>

В идеале *hash function* будет назначать каждому ключу уникальный *bucket*, но как правило возникают хеш-коллизии (*hash collision*). *Hash collision* – ситуация, при которой *hash function* генерирует один и тот же *index* для нескольких ключей. Причем вероятность *collision* быстро нарастает с увеличением числа *entry*'s. Эта проблема называется "[парадокс дней рождений](#парадокс-дней-рождений)".

В *HashTable* должен быть реализован механизм разрешения коллизий. Есть два подхода:

- Метод цепочек (*separate chaining*). 
- Открытая адресация (*open adressing*)

### Метод цепочек

Метод цепочек (*separate chaining*)

<u>Связные списки</u>

Наиболее частая реализация – с использованием связных списков (*linked list*). Каждый *bucket* хранит *linked list of entry*'s, которые отражаются хеш-функцией на один *index*. *Collision* приводит к тому, что появляется цепочка длиной более одной *entry*.

При поиске и удалении, просматриваются все *entry's* в соответствующем *bucket*'е, чтобы найти *entry* с нужным ключом. При добавлении, *entry* помещается в *linked list* соответствующего *bucket*'а.

![hashtable](https://parshikovpavel.github.io/img/algorithm/hashtable1.png?1)

Получаем формулу для времени поиска *entry*:

```
время_поиска = время_поиска_bucket'а + время_поиска_в_linked_list
```

Поэтому алгоритмическая сложность `O(1 + load_factor)`.

В хорошей *hash tabl*'ице каждый *bucket* хранит *linked list* размером 0 или 1 *entry*, т.е. `load_factor < 1`. Хотя даже если `load_factor > 1` поиск с использованием *hash tabl*'ицы гораздо быстрее последовательного поиска.

<u>Разновидности</u>

Если `load_factor >> 1`, можно для хранения *entry*'s внутри *bucket*'а вместо *linked list* использовать более быстрые структуры поиска, например *search tree's*

### Характеристики

По всем характеристикам *hash tabl*'ицы в среднем более эффективны, чем *search tree*'s (в т.ч. *binary search tree*'s) или *lookup table* структуры. 

| Algorithm        | Средний | Худший |
| ---------------- | ------- | ------ |
| *Space* (память) | `O(n)`  | `O(n)` |
| *Search*         | `O(1)`  | `O(n)` |
| *Insert*         | `O(1)`  | `O(n)` |
| *Delete*         | `O(1)`  | `O(n)` |

Применение:

- построение *associative array*'s
- database indexing
- построение *cache*'й

## Heap

Куча (*heap*) — структура данных типа дерево (*tree-based data structure*), которая удовлетворяет следующему свойству (свойство кучи, *heap property*): 

- для *max-heap*: если `B` является узлом-потомком узла `A`, то `ключ(A) ≥ ключ(B)`. Т.е. элемент с наибольшим ключом всегда является корневым узлом кучи
- для *min-heap*: если `B` является узлом-потомком узла `A`, то `ключ(A) ≤ ключ(B)`. Т.е. элемент с наименьшим ключом всегда является корневым узлом кучи .

![hashtable](https://parshikovpavel.github.io/img/algorithm/heap.svg)

Не существует никаких ограничений относительно того, сколько узлов-потомков имеет каждый узел кучи, хотя на практике их число обычно не более двух. 

Структуру данных *heap* не следует путать с понятием *heap* в динамическом распределении памяти. В некоторых ранних популярных языках программирования (Lisp) обеспечивалось динамическое распределение памяти с использованием структуры данных «куча», которая и дала своё имя выделяемому объёму памяти.

Кучи обычно реализуются в виде массивов, что исключает наличие указателей между её элементами.

Над *heap* производят операции:

- *find min (max)* – найти максимум (минимум)
- *delete max (min)* – удалить максимум (минимум)
- *increase (decrease) key* – увеличить (уменьшить) ключ – обновить некоторый ключ в *heap*
- *insert* добавить – добавление нового ключа в кучу
- *meld* слияние – соединение двух куч с целью создания новой кучи, содержащей все элементы обеих исходных (и наверное создание *heap* из `array`).

| Operation | find min | delete min | increase key | insert    | meld   |
| --------- | -------- | ---------- | ------------ | --------- | ------ |
| Binary    | `O(1)`   | `O(logN)`  | `O(logN)`    | `O(logN)` | `O(N)` |

<ins>Применение</ins>:

- пирамидальная сортировка (*heapsort*)
- очередь с приоритетами (*priority queue*, [1](#priority-queue))

## Priority queue

*Priority queue* (очередь с приоритетом) – структура данных, которая похожа на обычный *queue* или *stack*, но каждый элемент имеет связанный с ним *priority* (приоритет).  Элемент с более высоким *priority* должен быть извлечен из *priority queue* перед элементом с более низким приоритетом.

Основные операции:

- `insert($value, $priority)` – добавить значение `$value` в очередь со связанным приоритетом `$priority`.
- `extract()` – извлечь элемент (его `$value` и `$priority`) из начала очереди (с наивысшим или наинизшим приоритетом) и пересортировать очередь. 
- `top()` (также `find_max()` или `find_min()` )  – вернуть элемент с наивысшим приоритетом, всегда выполняется за время `O(1)` . 

<ins>Примеры</ins>:

- список задач работника c приоритетом каждой задачи. Когда он заканчивает одну задачу, он переходит к очередной — самой приоритетной то есть выполняет операцию извлечения максимума. 

<ins>Реализация</ins>:

Чаще всего реализация выполняется на основе *heap* структуры, где производительность `O(log N)` для вставок и удалений и `O(N)` для первоначального построения *heap* из `N` элементов.

## Tree

### B-tree

#### B-tree

*B-tree* (B-дерево)  – сбалансированное, сильно ветвистое дерево порядка `t` (обычно `t` принимает значения от 50 до 2000 в зависимости от того, сколько записей помещается на страницу). Предназначено для хранения данных во внешней памяти. Сбалансированность означает, что длина любых двух путей от корня до листьев различается не более, чем на единицу. Ветвистость дерева – это свойство каждого узла дерева ссылаться на большое число узлов-потомков. 

*B-tree* обладает свойствами:

- Ключи в каждом узле упорядочены для быстрого доступа к ним. Корень содержит от `1` до `2t-1`  ключей. Любой другой узел содержит от `t-1`  до `2t-1` ключей.


- Любой узел (кроме листа), содержащий ключи *K<sub>1</sub>, ..., K<sub>n</sub>*  имеет `n+1`  потомков. При этом:
  - Первый потомок и все его потомки содержат ключи из интервала *(–∞; K<sub>1</sub>)*
  - Для *2 ≤ i ≤ n*, *i*-й потомок и все его потомки содержат ключи из интервала *(K<sub>i-1</sub>; K<sub>i</sub>)* 
  - *(n+1)*-й потомок и все его потомки содержат ключи из интервала *(K<sub>n</sub>; ∞)*

- Глубина всех листьев одинакова.


Алгоритм поиска: 

- если ключ содержится в корне, он найден. 
- Иначе определяем интервал и идём к соответствующему потомку. 

Преимущества:

- снижение количества операций ввода-вывода при поиске элемента


- операции над данными выполняются блоками, блок (страница) читается с диска и блоком размещается в памяти.


- при поиске выполняется последовательное чтение элементов из узлов с больши́м количеством ключей, а не произвольное (в случае бинарного).

![btree](https://parshikovpavel.github.io/img/algorithm/btree.png)

Высота *B-tree* с *n* узлами не превышает *log<sub>t</sub> n*  (намного меньше чем у бинарного), соответственно при поиске нужно выполнить *log<sub>t</sub> n* дисковых операций, сложность поиска *O(t ⋅ log<sub>t</sub> n)*.

#### B+ tree

Отличие B+ дерева:

- Все ключи хранятся только в листьях, там же хранится и информационная часть узла (в стандартных B-деревьях ключи хранятся также во внутренних узлах). Во внутренних узлах хранятся копии ключей – они помогают искать нужный лист.


- Поскольку все ключи хранятся только в листьях, листья связываются последовательно, что позволяет быстро обходить дерево в порядке возрастания ключей


![bplustree](https://parshikovpavel.github.io/img/algorithm/bplustree.png)  

#### B\*, B+* tree's

*B\*-tree* – разновидность *B-tree*, в которой каждый узел *tree* заполнен не менее чем на ⅔ (в отличие от *B-tree*, где этот показатель составляет 1/2). *B+-tree*, удовлетворяющее таким требованиям называется *B+*-tree*. Это свойство приводит к лучшему использованию места, занимаемого деревом, и чуть лучшей производительности.

### Prefix tree

Префиксное дерево (*prefix tree*, *trie*, бор)

Особенности префиксного дерева:

- Корню дерева соответствует пустая строка.


- Каждому ребру префиксного дерева соответствует символ. 


- Получить ключ узла можно выписыванием подряд символов, помечающих рёбра на пути от корня до узла. 

На рисунке произвольными цифрами помечены *node*'s, которые хранят полные key's. В данном случае это различные английские слова (*I*, *to*, *tea*...)

![prefix tree](https://parshikovpavel.github.io/img/algorithm/prefixTree.svg)

Преимущества префиксного дерева:

- отличный расход памяти: хорошо поддается сжатию и повторяющиеся части префиксов будут храниться всего один раз. 


- получаем возможность делать префиксные запросы. 

## Inverted index

*Term* – это минимальная единица текста, по которой осуществляется индексирование и которая хранится в словаре индекса. Чаще всего *term*'ом является слово, приведенное к нормальной форме (`навыками` → `навык`). 

Прямой индекс (*forward index*) – структура данных, в которой для каждого документа хранится список *term*'ов, встречающихся в документе:

| Документ  | Слова                   |
| --------- | ----------------------- |
| Документ1 | Корова, говорит, мычать |

Инвертированный индекс (*inverted index*) — структура данных, в которой для каждого *term*'a из общего словаря (*term dictionary*) сопоставлен список документов (*posting list*), в которых этот *term* присутствует. 

| Слово  | Документы                       |
| ------ | ------------------------------- |
| Корова | Документ1, Документ2, Документ3 |

Для хранения *term dictionary* могут использоваться любые структуры поиска:

- *tree*
  - *B-tree* (в MySQL)
  - [prefix tree](#prefix-tree) (префиксное дерево, *trie*).  
- *Hash table*, где *key* – *term*, *value* – ссылка на список документов этого  *term*'а.

- *sorted list*, по которому можно искать бинарным поиском.

Применение:

- [*full text search*](#fulltext-search) 

## Set

*Set* (множество) – коллекция элементов, удовлетворяющая свойствам:

- элементы различны
- порядок элементов не имеет значения. 

Пример *Set*: `{1, 2, 3}`. Этот *Set* эквивалентен `{1, 3, 2}`.

*Power set* – все *subset*'s любой длины, которые можно получить из *set*, включая пустой *set* и сам *set* целиком. Для *set* длиной *n* – размер *power set* *2<sup>n</sup>*.

## Sequence

*Sequence* (последовательность) – коллекция элементов, удовлетворяющая свойствам:

- элементы могут повторяться.
- порядок элементов имеет значение.

Пример *Sequence*: `{1, 1, 2}`. Эта *Sequence* отличается от `{1, 2, 1}`.

## Permutation

*Permutation* (перестановка) – изменение порядка элементов в *sequence*, а также сам порядок элементов после переупорядочивания в *sequence*.

Свойства *Permutation*:

- порядок элементов имеет значение
- элементы, как правило, не повторяются

*Permutation* для `{1,2,3}` – `(1,2,3)`, `(1,3,2)`, `(2,1,3)`, `(2,3,1)`, `(3,1,2)` и `(3,2,1)`.

Найти все *permutation*'s строки – то же самое, что найти все анаграммы строки.

![prefix tree](https://parshikovpavel.github.io/img/algorithm/permutations.svg)

Количество *permutation*'s *n* различных объектов – *n!*, т.к.:

- (2 объяснение, 2 решение) 1-ый элемент мы можем разместить в одну позицию, 2-ой элемент – в две позиции (слева и справа от первого), 3-й элемент – в 3 позиции (слева, в центре, справа) для 2 вариантов на прошлом шаге, и т.д.
- (1 объяснение, 1 решение) – мы выбираем 1-ый элемент из *(n)* элементов (т.е. всего возможно *(n)* вариантов выбора 1-го элемента). После этого убираем этот элемент из *sequence*, делаем *permutation*'s оставшихся *(n-1)* элемента. 2-ой элемент мы можем выбрать из оставшихся *(n-1)* элементов, третий из оставшихся (n-2) элементов и так далее. 

Это дает нам общее количество *n × (n-1) × (n-2)... × 2 × 1 = n!* *permutation*'s.

## Combination

*Combination* (сочетание) *k* элементов из *n*  – *subset* из *k* (различных?) элементов, выбранных из *set*, содержащего *n* (различных?) элементов.

Свойства *Combination*:

- порядок элементов не имеет значения
- элементы (как правило) не повторяются

Пример 3-combination из 5-ти элементного set'а `(1,2,3,4,5)` – `(1,2,3)`. Причем *combination*'s `(1,2,3)` и `(1,3,2)` – эквивалентны.



![prefix tree](https://parshikovpavel.github.io/img/algorithm/combinations.svg)

Количество k-combination's из *n* элементов обозначается *C<sub>n</sub><sup>k</sup>*.

*C<sub>n</sub><sup>k</sup>=(n!) / (k!⋅(n-k)!)*

# Математические проблемы

## Парадокс дней рождений

Парадокс дней рождения (*birthday problem*) – вероятность совпадения дня рождения у двух любых человек в группе не равна `1 / 365 * N`,  что противоречит интуитивному представлению человека. 

Вероятность `50%` достигается уже для группы из `23` человек. Такое утверждение может показаться неочевидным, так как вероятность совпадения дня рождения человека с некоторым днём в году `1 / 365 = 0,27%`, умноженная на число человек в группе `23`, даёт лишь `(1 / 365) × 23 = 6,3%`. Однако по условию выбираются пары любых двух человек, и количество пар `( 23 × 22 ) / 2 = 253` , что значительно больше половины числа дней в году (`183`).

