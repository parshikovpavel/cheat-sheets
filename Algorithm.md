# Порядок решения алгоритмических задач

- написать тестовые примеры.
- разработать алгоритмы решения
- оценить алгоритмы решения с точки зрения сложности по памяти (*space*) и времени (*time*)
- как правило, подходит алгоритм с наименьшей сложностью по времени и приемлемой сложностью по памяти.
- написать код

## Примеры граничных случаев

Нужно начать с написания тестовых примеров. Прежде всего продумать граничные случаи. Также эти тестовые примеры можно рассматривать, как способ решения задачи через тестирование (TDD). Они позже позволят протестировать полученный алгоритм.

Например, аргументом является бинарный массив. 

Граничные варианты:

```php 
[1, 1, 1, 1, 1] # одни 1
[0, 0, 0, 0, 0] # одни 0
[1, 1, 1, 0, 0] # 1 в начале
[0, 0, 1, 1, 1] # 1 в конце
[]              # пустой массив 
```

## Разработать алгоритм

Если требуется написать некоторый алгоритм, его можно выразить на листе в виде функции:

```
f(arg1, arg2, ...) = result
```

Алгоритм следует разрабатывать так:

- написать алгоритм для решения задачи в общем случае. 
- если этот алгоритм не покрывает некоторые граничные случаи (например, пустые входные параметры), отдельно добавить обработку граничных случаев.

### Space-time trade-off

Компромисс времени и памяти(space-time trade-off [speɪs-taɪm ˈtreɪdɒf]) —подход к решению ряда задач, учитывающий обратную зависимость требуемого объёма памяти и скорости выполнения программы. Благодаря значительному снижению стоимости хранения мегабайта (по сравнению с ростом тактовой частоты процессора), в последнее время широко используются приемы, использующие дополнительную память для уменьшения времени вычислений.

Ниже приведены примеры применения:

#### Lookup table

Таблица поиска (lookup table) — это структура данных, обычно массив или ассоциативный массив, используемая с целью заменить вычисления на извлечение значения из памяти.

Некоторые задачи, например, обращение односторонней функции, вычисление функции для отображения элементов одного множества (например, символов) в другое множество (например, в UTF8 коды символов) решаются либо в результате трудоемких вычислений, либо полным перебором. Однако результат может быть получен за константное время с использованием т. н. таблиц поиска (lookup tables). 

Идея такова: вместо того, чтобы производить вычисления или перебирать все варианты на ходу, можно их все вычислить заранее один раз и хранить в памяти. Все предвычисленные варианты сводятся в таблицу поиска, в которой можно найти решение за константное время. Увеличение скорости может быть значительным, так как получить данные из памяти зачастую быстрее, чем выполнить трудоёмкие вычисления.

Классический пример использования таблиц поиска — вычисление значений тригонометрических функций, например, синуса.

Размер таблицы поиска со всеми возможными вариантами может быть довольно большим, настолько что она не влезает в доступную память. Поэтому в некоторых алгоритмах вычисляют только часть вариантов, а затем их как-то используют для нахождения окончательного решения. Т.е. space-time trade-off  увеличивается в сторону time за счет уменьшения space. Например:

·  при построении радужных таблиц. 

·  при построении таблицы непрерывной функции, можно использовать таблицу поиска в сочетании с простыми вычислениями — интерполяцией.

Однако использование таблиц поиска в тех задачах, где извлечение из памяти данных оказывается медленнее, чем их вычисление, приводит к понижению скорости работы. Извлечение из памяти данных может быть неэффективным для больших таблиц поиска, которые не помещаются в кеш, также PHP выполняет ряд служебных операций при обращении к элементу ассоциативного массива.

#### Радужные таблицы

Радужная таблица (rainbow table) — специальный вариант таблиц поиска(lookup table) для обращения односторонних криптографических хеш-функций, использующий механизм разумного компромисса между временем поиска по таблице и занимаемой памятью(space-time trade-off). Р

Использование обычной таблицы поиска невозможно, т.к. ее размер быстро растет с ростом длины строки. Альтернативным вариантом является хранение только первых элементов т.н. «цепочек хешей». Это потребует больше вычислений для поиска пароля, но значительно уменьшит количество требуемой памяти

Пусть у нас есть хеш-функция H с длиной хеша n и конечное множество паролей P. Построить полную таблицу соответствия                                 невозможно, т.к. это требует много памяти.

Цепочки хешей — метод для уменьшения этого требования к объёму памяти. Главная идея — определение функции редукции   , т.е. хешу пароля   сопоставляет какой-то другой пароль   . Заметим, что R не является обращением хеш-функции. Начиная с исходного пароля и попеременно применяя к каждому полученному значению H и R, мы получим цепочку перемежающихся паролей и хешей. Например, цепочка может выглядеть так:

  

Для генерации таблицы мы выбираем случайное множество начальных паролей из P, вычисляем цепочки некоторой фиксированной длины k для каждого пароля и сохраняем только первый и последний пароль из каждой цепочки.

Для каждого хеша h, значение которого мы хотим обратить (найти соответствующий ему пароль), вычисляем последовательность R(…R(H(R(h)))…). Если какое-то из промежуточных значений совпадет с каким-нибудь концом какой-либо цепочки, мы берём начало этой цепочки и восстанавливаем её полностью.

Увеличение длины цепочки уменьшает размер таблицы, но увеличивает время поиска нужного элемента в цепочке.

Недостаток такого алгоритма: возможность слияния цепочек – генерация одного и того же значения в разных цепочках в разных позициях последовательностей. В итоге все значения, сгенерированные после совпадения, будут одинаковыми в обеих цепочках, что сужает количество покрываемых паролей

В радужных таблицах используется усовершенствованный вариант цепочек хешей. Вместо одной функции редукции R, на каждом шаге используется своя функция редукции R1, R2, …, Rk. При таком подходе две цепочки могут слиться только при совпадении значений на одной и той же итерации. Следовательно, достаточно проверять на коллизии только конечные значения цепочек, что не требует дополнительной памяти. На конечном этапе составления таблицы можно найти все слившиеся цепочки, оставить из них только одну и сгенерировать новые, чтобы заполнить таблицу необходимым количеством различных цепочек.

Такой алгоритм позволяет наращивать таблицу поиска без опасности получить большое количество слияний цепочек. Процесс генерации радужных таблиц легко поддается распараллеливанию.

##### Защита с помощью соли

Методом защиты от взлома с помощью радужных таблиц является использование в функции хеширования «соли».  Соль  (salt [sɔːlt])— строка, которая подмешивается в хеш-функцию вместе с паролем. Применение соли называется «посолить» пароль.

Существует множество возможных схем смешения затравки и пароля. Например:

$hash = *md5*($password.$salt);
 $hash = *md5*(*md5*($password).$salt);

Для восстановления такого пароля взломщику необходимы таблицы для всех возможных значений соли. По сути, соль увеличивает длину и сложность пароля. Для высокой надежность должна использоваться только динамическая соль, иначе она не имеет смысла, т.к. возможно будет известна злоумышленнику.

Подробнее про реализацию в PHP смотреть Хэширование паролей.

В большинстве UNIX-систем в качестве односторонней функции используется системная библиотека crypt(3). Результатом работы функции является строка, содержащая метку алгоритма хеширования, соль, собственно хеш и, опционально, другие данные (например, число раундов хеш-функции).

 

https://ru.wikipedia.org/wiki/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_%D0%BF%D0%BE%D0%B8%D1%81%D0%BA%D0%B0

 

 

### Сложность алгоритмов. Большое О

Существуют инструменты, измеряющие, насколько быстро работает код. Это программы, называемые *профайлерами* (profilers), которые определяют время выполнения в миллисекундах, помогая нам выявлять узкие места и оптимизировать их. Сложность алгоритма — это то, что основывается на сравнении двух алгоритмов на идеальном уровне, игнорируя низкоуровневые детали вроде реализации языка программирования, «железа», на котором запущена программа, или набора команд в данном CPU.

Максимальный элемент массива

var M = A[ 0 ]; 

for ( var i = 0; i < n; ++i ) { 

if ( A[ i ] >= M ) { 

M = A[ i ]; 

} }

В процессе анализа данного кода, имеет смысл разбить его на простые инструкции — задания, которые могут быть выполнены процессором тотчас же или близко к этому.

Для первой строки в коде выше: var M = A[ 0 ]; 

требуются две инструкции: для поиска A[0] и для присвоения значения M (мы предполагаем, что nвсегда как минимум 1). 

если мы проигнорируем содержимое тела цикла, то количество инструкций у этого алгоритма `4 + 2n` — четыре на начало цикла `for `и по две на каждую итерацию, которых мы имеем `n`штук. математическую функцию `f(n)` такую, что зная `n`, мы будем знать и необходимое алгоритму количество инструкций. Для цикла `for `с пустым телом `f( n ) = 4 + 2n`. 

Когда мы анализируем алгоритмы, мы чаще всего рассматриваем наихудший сценарий, в данном случае когда массив упорядочен по возрастанию, как, например, `A = [ 1, 2, 3, 4 ]. `*анализ наиболее неблагоприятного случая*, в наихудшем случае в теле цикла из нашего кода запускается четыре инструкции, и мы имеем `f( n ) = 4 + 2n + 4n = 6n + 4`. При анализе сложности важность имеет только то, что происходит с функцией подсчёта инструкций при значительном возрастании `n `**мы отбрасываем те элементы функции, которые при росте** `**n**` **возрастают медленно, и оставляем только те, что растут сильно.** отбросим 4 и оставим только `f( n ) = 6n`. Второй вещью, на которую можно не обращать внимания, является множитель перед `n`. Так что наша функция превращается в `f( n ) = n`. константный множитель имеет смысл отбрасывать, если мы думаем о различиях во времени компиляции разных языков программирования (ЯП). (в одном языке на операцию приходится 3 элементарных, в другом одна). Описанные выше фильтры — «отбрось все факторы» и «оставляй только наибольший элемент» — в совокупности дают то, что мы называем *асимптотическим поведением*. Для `f( n ) = 2n + 8` оно будет описываться функцией `f( n ) = n`. Говоря языком математики, нас интересует предел функции `f` при `n`, стремящемся к бесконечности. 

Например, f( n ) = 109 даст f( n ) = 1.
 Мы отбрасываем множитель в 109 * 1 , но 1 по-прежнему нужен, чтобы показать, что функция не равна нулю

юбая программа, не содержащая циклы, имеет `f( n ) = 1`, потому что в этом случае требуется константное число инструкций (конечно, при отсутствии рекурсии — см. далее). Одиночный цикл от `1` до `n`, даёт асимптотику `f( n ) = n`

Следующая [PHP](http://php.net/)-программа проверяет, содержится ли в массиве `A` размера `n `заданное значение:

 


<?php

  $exists = false;

  for ( $i = 0; $i < n; ++$i ) {

​    if ( $A[ $i ] == $value ) {

​      $exists = true;

​      break;

​    }

  }

?>

Такой метод поиска значения внутри массива называется *линейным поиском*. Это обоснованное название, поскольку программа имеет `f( n ) = n`.

Два вложенных цикла дадут нам асимптотику вида `f( n ) = n`2.

если у нас имеется серия из последовательных for-циклов, то асимптотическое поведение программы определяет наиболее медленный из них.

Когда мы выясняем точную асимптотику `f`, мы говорим, что наша программа — `Θ( f( n ) )`. Например, в примерах выше программы `Θ( 1 )`, `Θ( n`2 `)` и `Θ( n`2` )`, соответственно. `Θ( n )` произносится как «тета от n»

Можно так же написать, что `2n ``∈`` ``Θ``( n )`, что произносится: «два n принадлежит тета от n».

\1.  n6 + 3n ∈ Θ( n6)

\2.  2n + 12 ∈ Θ( 2n )

\3.  3n + 2n ∈ Θ( 3n )

\4.  nn + n ∈ Θ( nn )

**Мы называем эту функцию (т.е. то, что пишем** `**Θ( здесь )**`**) \*временной сложностью\*, или просто \*сложностью\* нашего алгоритма.** Таким образом, алгоритм с `Θ( n )` имеет сложность `n`. Также существуют специальные названия для `Θ( 1 )`, `Θ( n )`, `Θ( n`2` )` и `Θ( log( n ) )`, потому что они встречаются очень часто. Говорят, что `Θ( 1 )` — алгоритм с *константным временем*, `Θ( n )` — *линейный*, `Θ( n`2` )` — *квадратичный*, а `Θ( log( n ) )` — *логарифмический* (не беспокойтесь, если вы ещё не знаете, что такое логарифм — скоро мы об этом поговорим).

В реальной жизни иногда проблематично выяснить точное поведение алгоритма тем способом, который мы рассматривали выше. Особенно для более сложных примеров. Однако, мы можем сказать, что поведение нашего алгоритма никогда не пересечёт некой границы.

`O( n`2` )` произносится как «большое О от `n` в квадрате». Это говорит о том, что наша программа асимптотически не хуже, чем `n`2. Она будет работать или лучше, или также. Таким образом, `Θ( n`2`)` для программы — это и `O( n`2` )` тоже.

А вот обратное верно не всегда. Например, любой код с `Θ( n )` также и `O( n`2` )` в дополнение к `O( n )`. Обобщая: любая программа с `Θ( a )` является `O( b )` при `b` худшем, чем `a`. 

Поскольку `О`-сложность алгоритма представляет собой *верхний предел* его настоящей сложности, которую, в свою очередь, отображает `Θ`, то иногда мы говорим, что `Θ` даёт нам *точную оценку*. Если мы знаем, что найденная нами граница не точна, то можем использовать строчное `о`, чтобы её обозначить. Например, если алгоритм является `Θ( n )`, то его точная сложность — `n`. Следовательно, этот алгоритм `O( n )` и `O( n`2` )` одновременно. Поскольку алгоритм `Θ( n )`, то `O( n )` определяет границу более точно. А `O( n`2` )` мы можем записать как `о( n`2` )` (произносится: «маленькое o от n в квадрате»), чтобы показать, что мы знаем о нестрогости границы.

Если же мы поступим противоположным образом, сделав имеющийся код **лучше**, и найдём сложность того, что получится, то мы задействуем Ω-нотацию. Таким образом, `Ω` даёт нам сложность, лучше которой наша программа быть не может. `Ω` даёт нам *нижнюю границу* сложности нашего алгоритма  Аналогично `ο`, мы можем писать `ω`, если знаем, что этот предел нестрогий. Например, `Θ( n`3` )` алгоритм является `ο( n`4` )` и `ω( n`2` )`. `Ω( n )` произносится как «омега большое от n», в то время как `ω( n )` произносится «омега маленькое от n».

\1.  Θ( √n ) Строгие пределы будут теми же, что и Θ-сложность, т.е. O( √n ) и Ω( √n ) соответственно.

*Логарифм* — это операция, которая при применении её к числу делает его гораздо меньше (подобно взятию квадратного корня). Далее: подобно тому, как взятие квадратного корня является операцией, обратной возведению в квадрат, логарифм — обратная операция возведению чего-либо в степень. 

Как только вы проанализировали сложность вашего алгоритма, так сразу можете получить и грубую оценку того, как быстро он будет работать, приняв, что в секунду выполняется 1 000 000 команд. Их количество считается из полученной вами функции асимптотической оценки, описывающей алгоритм. Например, вычисление по алгоритму с Θ( n ) займёт около секунды при n = 1 000 000.

Рекурсивная функция вычисления факториала  будет вычисляться `n` раз. эта функция является `Θ( n )`.

Поиск в отсортированном массиве может выполняться половинным делением (бинарный поиск)

Наихудшим сценарием для данной задачи будет вариант, когда массив в принципе не содержит искомое значение. В этом случае мы начинаем с массива размером `n` на первом рекурсивном вызове, `n / 2` на втором, `n / 4` на третьем и так далее. В общем, наш массив разбивается пополам на каждом вызове до тех пор, пока мы не достигнем единицы.

0-я итерация: `n`
 1-я итерация: `n / 2`
 2-я итерация: `n / 4`
 3-я итерация: `n / 8`
 …
 i-я итерация: `n / 2`i
 …
 последняя итерация: `1`

на i-й итерации массив имеет `n / 2`i элементов. Количество итераций разбиения массива `1 = n / 2`i  i = log( n ) улучшение асимптотического времени выполнения программы часто чрезвычайно повышает её производительность.

Сортировка слиянием


Сложность слияния Θ( n ) в каждой строке, а всего строк – г*лубина* рекурсивного дерева, будет `log( n )`. общая сложность `mergeSort`: `Θ( n * log( n ) )`

## Алгоритмы кеширования

Также **алгоритмы/политики вытеснения/замещения** – программа для управления кэшем. Идея алгоритмов: когда кэш заполнен нужно выбрать, что именно удалить, чтобы записать новую, более актуальную информацию.  

**Least recently used (LRU, вытеснение давно неиспользуемых)**: в первую очередь вытесняются записи, неиспользованные дольше всех. Требует отслеживания того, что и когда использовалось, что может оказаться довольно накладно.

**Most Recently Used (****MRU, наиболее недавно использовавшийся)**: в первую очередь вытесняется последний использованный элемент. Полезны, когда чем старше элемент, тем больше обращений к нему происходит.

**Least-Frequently Used (****LFU, наименее часто используемый)** – подсчитывает, как часто используется элемент. Те элементы, обращения к которым происходят реже всего, вытесняются в первую очередь.

Существуют улучшенные версии алгоритмов, промежуточные между LRU и LFU. Кроме того можно учитывать: 

·  Хранить дольше данные, которые дороже получать.

·  Вытеснять быстрее записи большого размера, т.к. это позволит сохранить несколько записей поменьше. 

## Проблемы многозадачности

### Проблема ABA

Проблема ABA возникает при синхронизации, когда ячейка памяти читается дважды, оба раза прочитано одинаковое значение, и признак «значение одинаковое» трактуется как «ничего не менялось». Однако, другой поток может выполниться между этими двумя чтениями, поменять значение (A->B), сделать что-нибудь ещё (например, обратиться к разделяемой памяти) и восстановить старое значение (B->A). Таким образом, первый поток обманется, считая, что не поменялось ничего, хотя второй поток уже разрушил это предположение.

Пример для примера с разделяемой памятью:

·  Процесс   читает значение A из разделяемой памяти

·    , вытесняется, позволяя выполняться   

·    меняет значение A на B и обратно на A перед вытеснением

·    возобновляет работу, видит, что значение не изменилось, и продолжает работу с разделяемой памятью, хотя, возможно, что его поведение будет неправильным из-за других, скрытых изменений общей памяти (которые он не отслеживал).

Обычно с проблемой ABA сталкиваются при реализации lock-free структур и алгоритмов. Пример для lock-free стека на C++ и функции выталкивания с вершины стека:

Obj* Pop() {

   while(1) {

​    Obj* ret_ptr = top_ptr;

​    if (!ret_ptr) return NULL;

​    Obj* next_ptr = ret_ptr->next;

​    // Если верхний элемент - всё ещё ret, считаем, что никто не менял стек.

​    // (Это утверждение не всегда истинно из-за проблемы ABA)

​     // Атомарно заменяем top на next.

​    if (CompareAndSwap(top_ptr, ret_ptr, next_ptr)) {

​     return ret_ptr;

​    }

​    // Иначе - стек изменён, пробуем заново.

   }

}

Пример проблемной последовательности:

На стеке *top* → A → B → C

Процесс   :

ret_ptr = A;

next_ptr = B;

Вытеснение   . Процесс   меняет стек на *top* → A → C. Вытеснение   .

Процесс   :

CompareAndSwap(A, A, B)

Эта инструкция выполняется успешно, поскольку top == ret (оба равны A), в результате top_ptr присваивается значение указателя next_ptr, на уничтоженное значение B. 

Общий способ избавиться от проблемы: добавить к объекту, изменение которого критично, номер версии. Например, алгоритм CAS ([compare-and-swap](https://ru.wikipedia.org/wiki/Сравнение_с_обменом)) может помимо изменения самого значения также сохранять в младших битах номер версии, так чтобы последующий CAS не смог быть выполнен из-за несовпадения этих бит. Этот прием используется в распределенных транзакциях (смотрите Без требования идемпотентности (из документации Mongo, CouchDB…) )

### Сравнение с обменом (compare and swap, CAS)

Атомарная инструкция, сравнивающая значение в памяти с одним из аргументов, и в случае успеха записывающая второй аргумент в память. Поддерживается в семействах процессоров x86.

/* Псевдокод работы инструкции, возвращающей булево значение в синтаксисе языка C */

int cas( int* addr, int old, int new )

{

 if ( *addr != old )

  return 0;

 

 *addr = new;

 return 1;

}

**Пример использования**: У нас есть n процессоров, каждый из которых иногда хочет получить доступ к какому-то общему ресурсу, например, к разделяемой памяти. До начала основной работы назначим им уникальные номера от 0 до n-1. Выберем ячейку памяти, которая будет указывать, какой процессор сейчас использует ресурс. Значение -1 будет указывать, что ресурс никем не занят. Изначально поместим в неё −1. При основной работе каждый процессор должен проверить, что в ячейке находится −1, и если это так, то записать в неё свой номер. Если же ячейка занята — процессор обязан ждать, пока она не освободится.

### Неблокирующая синхронизация

Отказ от традиционных примитивов блокировки, таких, как семафоры, мьютексы и события. Разделение доступа между потоками идёт за счёт атомарных операций и специальных, разработанных под конкретную задачу, механизмов блокировки. Преимущество неблокирующих алгоритмов — в лучшей масштабируемости по количеству процессоров.``

# Структуры данных

## Hash table

*Hash table* (хеш таблица) — это структура данных, которая может отображать ключи на значения, т.е. позволяет реализовывать ассоциативный массив. 

<u>Вычисление index'а bucket'а</u>

Имеется массив из элементов (*entry's*) размером `number_of_entries`. *Entries*  размещаются среди множества бакетов (*bucket)* размером `number_of_buckets`. Ключ (*index*) нужного *bucket*'а вычисляется на основе ключа (`key`) *entry* и размера массива *bucket*'ов (`number_of_buckets`):

```
index = f(key, number_of_buckets)
```

Часто эта функция реализуется с помощью некоторой хеш-функции (*hash function*):

```
index = hash_function(key) % number_of_buckets
```

![hashtable](https://parshikovpavel.github.io/img/algorithm/hashtable.png)

<u>`number_of_buckets`</u>

Рекомендуется выбирать `number_of_buckets = 2^N`. Тогда вместо взятия по модулю `%`, достаточно наложить маску (*mask*, *bitmask*). 

Основной показатель *hash tabl*'ицы – коэффициент загрузки (*load factor*):

```
load_factor = number_of_entries / number_of_buckets
```

При увеличении *load factor*'а выше некоторого порога, `number_of_buckets` увеличивается (обычно удваивается).

<u>Коллизии</u>

В идеале *hash function* будет назначать каждому ключу уникальный *bucket*, но как правило возникают хеш-коллизии (*hash collision*). *Hash collision* – ситуация, при которой *hash function* генерирует один и тот же *index* для нескольких ключей. Причем вероятность *collision* быстро нарастает с увеличением числа *entry*'s. Эта проблема называется "[парадокс дней рождений](#парадокс-дней-рождений)".

В *HashTable* должен быть реализован механизм разрешения коллизий. Есть два подхода:

- Метод цепочек (*separate chaining*). 
- Открытая адресация (*open adressing*)

### Метод цепочек

Метод цепочек (*separate chaining*)

<u>Связные списки</u>

Наиболее частая реализация – с использованием связных списков (*linked list*). Каждый *bucket* хранит *linked list of entry*'s, которые отражаются хеш-функцией на один *index*. *Collision* приводит к тому, что появляется цепочка длиной более одной *entry*.

При поиске и удалении, просматриваются все *entry's* в соответствующем *bucket*'е, чтобы найти *entry* с нужным ключом. При добавлении, *entry* помещается в *linked list* соответствующего *bucket*'а.

![hashtable](https://parshikovpavel.github.io/img/algorithm/hashtable1.png?1)

Получаем формулу для времени поиска *entry*:

```
время_поиска = время_поиска_bucket'а + время_поиска_в_linked_list
```

Поэтому алгоритмическая сложность `O(1 + load_factor)`.

В хорошей *hash tabl*'ице каждый *bucket* хранит *linked list* размером 0 или 1 *entry*, т.е. `load_factor < 1`. Хотя даже если `load_factor > 1` поиск с использованием *hash tabl*'ицы гораздо быстрее последовательного поиска.

<u>Разновидности</u>

Если `load_factor >> 1`, можно для хранения *entry*'s внутри *bucket*'а вместо *linked list* использовать более быстрые структуры поиска, например *search tree's*

### Характеристики

По всем характеристикам *hash tabl*'ицы в среднем более эффективны, чем *search tree*'s (в т.ч. *binary search tree*'s) или *lookup table* структуры. 

| Algorithm        | Средний | Худший |
| ---------------- | ------- | ------ |
| *Space* (память) | `O(n)`  | `O(n)` |
| *Search*         | `O(1)`  | `O(n)` |
| *Insert*         | `O(1)`  | `O(n)` |
| *Delete*         | `O(1)`  | `O(n)` |

Применение:

- построение *associative array*'s
- database indexing
- построение *cache*'й

## Heap

Куча (*heap*) — структура данных типа дерево (*tree-based data structure*), которая удовлетворяет следующему свойству (свойство кучи, *heap property*): 

- для *max-heap*: если `B` является узлом-потомком узла `A`, то `ключ(A) ≥ ключ(B)`. Т.е. элемент с наибольшим ключом всегда является корневым узлом кучи
- для *min-heap*: если `B` является узлом-потомком узла `A`, то `ключ(A) ≤ ключ(B)`. Т.е. элемент с наименьшим ключом всегда является корневым узлом кучи .

Не существует никаких ограничений относительно того, сколько узлов-потомков имеет каждый узел кучи, хотя на практике их число обычно не более двух. 

Структуру данных *heap* не следует путать с понятием *heap* в динамическом распределении памяти. В некоторых ранних популярных языках программирования (Lisp) обеспечивалось динамическое распределение памяти с использованием структуры данных «куча», которая и дала своё имя выделяемому объёму памяти.

Кучи обычно реализуются в виде массивов, что исключает наличие указателей между её элементами.

Над *heap* производят операции:

- *find min (max)* – найти максимум (минимум)
- *delete max (min)* – удалить максимум (минимум)
- *increase (decrease) key* – увеличить (уменьшить) ключ – обновить некоторый ключ в *heap*
- *insert* добавить – добавление нового ключа в кучу
- *meld* слияние – соединение двух куч с целью создания новой кучи, содержащей все элементы обеих исходных.

| Operation | find min | delete min | increase key | insert    | meld   |
| --------- | -------- | ---------- | ------------ | --------- | ------ |
| Binary    | `O(1)`   | `O(logN)`  | `O(logN)`    | `O(logN)` | `O(N)` |

<ins>Применение</ins>:

- пирамидальная сортировка (*heapsort*)
- очередь с приоритетами (*priority queue*, [1](#priority-queue))

## Priority queue

*Priority queue* (очередь с приоритетом) – структура данных, которая похожа на обычный queue или stack, но каждый элемент имеет связанный с ним *priority* (приоритет).  Элемент с более высоким *priority* должен быть извлечен из *priority queue* перед элементом с более низким приоритетом.

Основные операции:

- `insert($value, $priority)` – добавить значение `$value` в очередь со связанным приоритетом `$priority`.
- `extract()` – извлечь элемент (его `$value` и `$priority`) из начала очереди (с наивысшим или наинизшим приоритетом) и пересортировать очередь. 
- `top()` (также `find_max()` или `find_min()` )  – вернуть элемент с наивысшим приоритетом, всегда выполняется за время `O(1)` . 

<ins>Примеры</ins>:

- список задач работника c приоритетом каждой задачи. Когда он заканчивает одну задачу, он переходит к очередной — самой приоритетной то есть выполняет операцию извлечения максимума. 

<ins>Реализация</ins>:

Чаще всего реализация выполняется на основе *heap* структуры, где производительность `O(log N)` для вставок и удалений и `O(N)` для первоначального построения *heap* из `N` элементов.

# Математические проблемы

## Парадокс дней рождений

Парадокс дней рождения (*birthday problem*) – вероятность совпадения дня рождения у двух любых человек в группе не равна `1 / 365 * N`,  что противоречит интуитивному представлению человека. 

Вероятность `50%` достигается уже для группы из `23` человек. Такое утверждение может показаться неочевидным, так как вероятность совпадения дня рождения человека с некоторым днём в году `1 / 365 = 0,27%`, умноженная на число человек в группе `23`, даёт лишь `(1 / 365) × 23 = 6,3%`. Однако по условию выбираются пары любых двух человек, и количество пар `( 23 × 22 ) / 2 = 253` , что значительно больше половины числа дней в году (`183`).

