

Cервис кэширования данных в оперативной памяти на основе хеш-таблицы.

Поддерживает распределение ключей по нескольким серверам в зависимости от хэша ключа. Вычисление хеша от ключа данных и выбора сервера для хранения ключа выполняет клиентская библиотека. 

Ситуация сбоя сервера трактуется как *cache miss*, что позволяет повышать отказоустойчивость комплекса за счет наращивания количества *memcached* серверов и возможности производить их горячую замену. 

При нехватке памяти, ключи из кеша удаляются согласно политике [LRU]()

Изначально разработан под *Livejournal*.

Существуют:

- *Memcached* – сам кеширующий сервер.

- *memcached* — расширение PHP для работы с сервером.

- *memcache* — расширение PHP для работы с этим сервером, используется чаще всего. *Memcache* ограниченней *Memcached* и не использует возможности сервера *Memcached* в полную силу, поэтому *Memcache* легче и производительней *Memcached* (примерно на 10%).

## Установка 

1. Установка сервера:

   `brew install memcached`

2. Установка `zlib`

   `brew install zlib`

3. Может быть нужна установка `pkg-config`????

   ```bash
   brew install pkg-config
   ```

   

4. Установка через `pecl install memcache` не работает.

   Требуется ручная установка:

   ```bash
   pecl download memcache
   open memcache-4.0.5.2.tgz  
   cd memcache-4.0.5.2/memcache-4.0.5.2
   phpize
   ./configure --with-zlib-dir=/usr/local/Cellar/zlib/1.2.11
   make
   sudo make install
   ```

5. Добавить в `php.ini`

   ```conf
   extension=memcache.so
   ```

   



### Опции *config* файла

Путь к *config* файлу /etc/memcached.conf

`-t threads` – число потоков, по умолчанию `4`. 

### Требования к ключу в Memcached

Ключ можно конструировать следующими способами:

·  в строковом человекопонятном формате ‘user_158

·  в обертке над memcached можно делать над ключом операцию:

$key = md5(serialize($options))

### Кластеризация в memcached (consistent hashing)

Может испольоваться для распределения нагрузки и достижения отказоустойчивости. Общий объем кэша будет равен сумме объемов кэшей всех memcached, входящих в кластер. Процесс memcached может быть запущен на сервере, где слабо используется процессор и не загружена до предела сеть.

При работе с кластером каждый сервер обрабатывает часть общего массива ключей проекта. При отказе сервера данные с части ключей будут утеряны и заново перегенерятся на других серверах. В случае необходимости важные ключи можно дублировать на нескольких серверах.

Алгоритм consistent hashing реализуетеся не в сервере, а на уровне клиента memcached в языке программирования, однако в каждом языке реализация своя, что приводит к несовместимости хэширования, невозможности доступа к кластеру из разных языков. 

Более подробно см. Консистентное кеширование

### Атомарность операций

Все одиночные запросы атомарны в силу корректных внутренних блокировок при нескольких потоках. Т.е. при записи значение ключа никогда не будет смесью двух записей. Однако в случае нескольких операций, например, считать значение, инкрементировать, записать, возможно возникновение состояния гонки.  Решение проблемы – использование синхронизационных примитивов (семафоров, мутексов и т.п.), однако в memcached они отсутствуют. Другим вариантом решения задачи является использование встроенных операций, которые заменяют неатомарную последовательность get/set, например, incr/decr, обеспечивающие атомарное увеличение (уменьшение). Атомарными являются также append/prepend (добавляет строку в конец/начало существующей записи), add и replace (задать/заменить значение ключа). 

### Счетчики

Реализация через обновление поля в БД не будет работать на высокой нагрузке. 

Остается воможность сбора статистики в memcache с последующим обновлением счетчика в базе данных. 

#### Счетчик через логгирование

Если не требуется отображение значения в «реальном времени», то можно реализовать через логирование.

Например, если требуется логгировать количество показов блока на сайте возможны варианты:

·  если блок вставляет скрипт на php, то он пишет это в лог (хотя можно сразу счетчик инкрементировать в кеше)

·  если эти блоки подгружает ajax-ом js, то он может сделать также запрос на сервер, который попадет в access log (хотя лучше запросом сразу инкрементировать счетчик, у нас так работает подсчет показов видео)

·  можно в каждый блок вставлять картинку 1х1 и считать по логам число запросов к этой картинке. Это более универсально, но зато забивает канал клиента лишними запросами (хотя также проще сразу логгировать, так реализован подсчет статистики в рассылках).

#### Счетчик пользователей

Для показа текущего значения счетчика просмотров следует выполнить операцию  incr над ключом. Если выполнение было успешным, это означает, что соответствующий ключ находится в memcached, мы просмотр засчитали, также получили новое значение счетчика как результат операции incr, которое мы можем показать пользователю. Если же операция incr вернула ошибку, то ключ счетчика в данный момент отсутствует в memcached, мы можем выбрать в качестве начального значения число просмотров из базы данных, увеличить его на единицу, и выполнить операцию set, устанавливая новое значение счетчика. При последующих просмотрах ключ уже будет находиться в memcached, и мы будем просто увеличивать его значение с помощью incr. В этой схеме присутствует состояние гонки (race condition). Если два клиента одновременно обращаются к счетчику, одновременно обнаруживают его отсутствие, и сделают две операции set, мы потеряем один просмотр.  В случае необходимости можно воспользоваться блокировками в memcached. Также можно воспользоваться операцией add вместо set, чтобы отследить ситуацию, если ключ успеет добавить другой клиент. 

#### Счетчик пользователей онлайн

Мы хотим рассчитать, сколько уникальных сессий (пользователей) обратилось к нашему сайту за последние 5 минут. Уникальность обращения пользователя с данной сессией в течение 5 минут можно отследить, сохраняя в сессии время последнего засчитанного обращения, если прошло более 5 минут – значит это новое (уникальное) обращение.

Выделим в memcached шесть ключей с именами, например, c_0, c_1, c_2, …, c_5. Текущим изменяемым ключом мы будем считать счетчик с номером, равным остатку от деления текущей минуты на 6 (например, 4). Если incr вернет ошибку (счетчика еще нет), установим его значение в 1 с помощью set, обязательно указав время жизни 6 минут. Значением счетчика онлайнеров будем считать сумму всех ключей, кроме текущего (т.е. c_0, c_1, c_2, c_3 и c_5).

Когда наступит следующая минута, текущим изменяемым ключом станет ключ c_5, при этом его предыдущее значение исчезнет (т.к. он был создан 6 минут назад с временем жизни те же 6 минут). Значением счетчика станет сумма ключей c с_0 по c_4, т.е. только что рассчитанное значение ключа с_4 уже начнет учитываться в отображаемом значении счетчика. Принцип аналогичен тому, как прорисовывается кадр в видеоконтроллере (двойная буферизация).

Такой счетчик может быть построен и на меньшем числе ключей. Минимально возможными для данной схемы являются два ключа: один обновляется, значение другого показывается, затем по прошествии 5 минут счетчики меняются местами, при этом тот, который только что обновлялся, сбрасывается. В приведенной схеме с многими ключами обеспечивается некоторое «сглаживание», которое обеспечивает более плавное изменение счетчика в случае резкого притока или оттока посетителей.

**Вариант:** Вести учет в счетчике в shared memory. Уникальность учета пользователя проверять через timestamp в сессии. Каждый сервер знает только свое количество пользователей online. Каждые $timeout минут веб-приложение сбрасывает счетчик в БД (можно в таблицу MEMORY) или memcache в виде кортежа {‘server_id’: $server_i_id, ‘online’: $local_online }. Общее количество пользователей получается суммированием в БД или кеше.

#### Учет пользователей онлайн

**У нас.** При каждом запросе зарегистированного пользователя в его сессии проверяет поле, которое хранит время последнего обновления его статуса-онлайн (хранится в кеше). Если поле отсутствует или время последнего обновления вышло за пределы (60 сек), то установить текущее время в качестве времени последнего обновления и запустить процедуру сохранения статуса-онлайн в кеш. Через replace в ключ «_{$user_id}» делается попытка перезаписать время обновления статуса-онлайн, если информацию о статусе-онлайн отсутстствует запускается следующая процедура:

·  Ключ «_{$user_id}» с timestamp статуса-онлайн пользователя записывает в кеш

·  определяется номер бакета, где будет храниться информация о наличии кеша со статусом-онлайн пользователя bucket_id=user_id%bucket_size

·  стартует критическая секция (реализована через мемкеш)

·  загружается из памяти бакет «_{$bucket_id}»

·  в него дописывается добавляемый user_id

·  завершается критическая секция

Схема с хранением списков user_id, которые онлайн, с шардированием этих списков по бакетам, позволяет сделать сброс данных о пользователя-онлайн в базу и вывести список пользователей онлайн.

**Вариант:** Если не требуется сбрасывать статус пользователя в БД, то проще делать его оффлайн удалением ключа не перебором всех ключей из бакетов по крону, а просто ключу "_{$user_id}" поставить expire=5 мин. 

**Вариант:** идентифицировать пользователей по IP-адресу

**Вариант:** делать учет пользователя не в момент обновления страницы, а по AJAX делать запросы с интервалом в 5 секунд.

 

### Требования к ключам и данным: 

·  длина ключей максимум 250 байт;

·  запрещены символы ord(char) < 33 и с ord(char) >= 127 (в том числе пробел=32);

·  объем данных, который можно хранить под одним ключом, ограничивается 1 Мб. 

### Одновременное перестроение кешей

Есть выборка из БД, которая используется на многих страницах, сама выборка является относительно сложной, её вычисление заметно нагружает backend (БД). В какой-то момент времени ключ в memcached будет удален, в этот момент несколько frontend’ов (несколько, т.к. выборка часто используется) обратятся в memcached по этому ключу, обнаружат его отсутствие и попытаются построить кэш заново, осуществив выборку из БД. То есть в БД одновременно попадет несколько одинаковых запросов, каждый из которых заметно нагружает базу данных, при превышении некоторого порога запрос не будет выполнен за разумное время, еще больше frontend’ов обратятся к кэшу, обнаружат его отсутствие и отправят еще больше запросов в базу данных, с которыми база данных тем более не справится. В результате сервер БД получил критическую нагрузку, и «прилёг». Такая ситуация называется «проблема стаи собак» (dog-pile effect или cache stampede, паническое бегство кеша). 

Способы решения проблемы.

#### Перестроение кеша в критической секции

При отсутствии данных в кэше процесс, который хочет их загрузить, должен захватить лок, который не даст сделать то же самое другим параллельно выполняющимся процессам. 

Чтобы обслужить процессы, которые ожидают перестроение кеша захватившим блокировку процессом, – не ограничиваем время жизни ключа с кэшом в memcached. Ключ будет там находиться до тех пор, пока не будет вытеснен другими ключами. Но вместе с данными кэша мы записываем и реальное время его жизни, например:

{

  годен до: 2008-11-03 11:53,

   данные кэша:

  {

​    ...

  }

}

\1.   Получаем доступ к кэшу cache, его срок жизни истёк.

\2.     Пытаемся заблокироваться по ключу cache_lock.

·    Не удалось получить блокировку:

·    вернуть старое значение ключа;

​               или

·    ждем снятия блокировки

·    не дождались: возвращаем старые данные кэша;

·    дождались: выбираем значения ключа заново, возвращаем новые данные (построенный кэш другим процессом).

·    Удалось получить блокировку:

·    возвращаем false, как будто в кеше ничего нету. Процесс перестраивает кеш.

#### Вынос обновлений в фон

Перестроение кеша происходит в фоновом процессе, который запускается в crontab. 

Преимущества:

·  запросы пользователей не ожидают перестроения кеша

·  конкуренция и одновременное перестроение кешей отсутствуют в принципе, т.к. обновлением занимается один процесс

Недостаток:

·  требуются «ресурсы» на мониторинг скрипта, осуществляющего перестроение кеша

#### Вероятностные методы обновления

Данные в кэше обновляются не только при отсутствии, но и с какой-то вероятностью при их наличии. Это позволит обновлять их до того, как закэшированные данные «протухнут» и потребуются сразу всем процессам.

Для корректной работы такого механизма нужно, чтобы в начале срока жизни закэшированных данных вероятность пересчёта была небольшой, но постепенно увеличивалась. Добиться этого можно с помощью алгоритма XFetch, который использует экспоненциальное распределение. Его реализация выглядит примерно так:

**function** xFetch($key, $ttl, $beta = 1)
 {
   [$value, $delta, $expiry] = cacheRead($key);
   **if** (!$value || (*time*() - $delta * $beta * *log*(*rand*())) > $expiry) {
   $start = *time*();
   $value = recomputeValue($key);
   $delta = *time*() - $start;
     $expiry = *time*() + $ttl;
     cacheWrite(***key\***, [$value, $delta, $expiry], $ttl);
   }
 
   **return** $value;
 }

где

$ttl — это время жизни значения в кэше, 

$delta — время, которое потребовалось для генерации кэшируемого значения,

$expiry — время, до которого значение в кэше будет валидным, 

$beta — параметр настройки алгоритма, изменяя который, можно влиять на вероятность пересчёта (чем он больше, тем более вероятен пересчёт при каждом запросе).

Для этого алгоритма существует вероятность параллельных обновлений, однако она значительно снижается. Чтобы исключить полностью параллельные обновления, можно выполнять вероятностное перестроение кеша в критической секции. 

### Критические секции

Критические секции в memcache – механизм распределенной синхронизации. Они реализуются через блокировку (мьютекса, двоичного семафора). Варианты реализации. 

Первый некорректный. Пусть мы хотим заблокироваться по ключу ‘lock’: пытаемся получить значения ключа с помощью операции get. Если ключ не найден, значит блокировки нет, и мы с помощью операции `set `устанавливаем значение этого ключа, например, в единицу, а время жизни устанавливаем в небольшой интервал времени, который превышает максимальное время жизни блокировки, например, в 10 секунд. Теперь, если frontend завершится аварийно и не снимет блокировку, она автоматически уничтожится через 10 секунд. Выполнили все необходимые действия, после этого снимаем блокировку просто удаляя соответствующий ключ командой `del`. Если на первой операции `get` мы получили значение ключа, это означает, что блокировка уже установлена другим процессом, наша операция блокировки неуспешна. 

Описанный способ обладает недостатком: наличием состояния гонки (race condition). Два процесса могут одновременно сделать `get`, оба могут получить ответ, что «ключа нет», оба сделают `set`, и оба будут считать, что установили блокировку успешно. В ситуациях, как одновременное перестроение кэшей, этого может быть допустимо, т.к. здесь цель не исключить все другие процессы, а резко уменьшить количество одновременных запросов к БД, что может обеспечить и этот простой, некорректный вариант.
 Второй вариант корректен, и даже проще первого. Для захвата блокировки достаточно выполнить одну команду: `add`, указав имя ключа и время жизни (такое же маленькое, как и в первом варианте). Команда `add` будет успешной только в том случае, если ключа в memcached еще нет, то есть наш процесс и есть тот единственный процесс, которому удалось захватить блокировку. Если `add` вернет ошибку «такой ключ уже существует», значит, блокировка была захвачена раньше каким-то другим процессом. Она атомарно выполняет проверку существования ключа и установку его значения.

После каждой неудачной попытки захвата мьютекса (ключа в мемкеш) процесс засыпает на время. Это могут быть случайные интервалы времени, лучше линейно наращивать время сна от малых значений к большим.

Также критические секции можно реализовать через хранение мьютекса (lock-ключа) в shared memory. При наличии одного бэкенда это будет честная блокировка. Если бекэндов немного, то блокировка отдельно на каждом может быть приемлема для некоторых задач, например, одновременного перестроения кешей, тогда количество параллельных запросов в базу резко снижается.

### Тегирование ключей

Используется для сброса группы кэшей, используется методика инвалидации при чтении (через версионирование). Решаемая проблема: необходимо сбрасывать кеш сразу же за изменением.

Например, когда один из авторов создает новый пост, меняется большое количество выборок с разными лентами. Явная инвалидация при записи очень накладна. Введем понятие тега кеша – это некоторое имя и связанная с ним версия (число). Каждый кэш связан с некоторым списком тэгов. Версия тэга может только монотонно увеличиваться. Группой кэшей мы будем называть кэши, имеющие один общий тэг. Для того чтобы сбросить группу кэшей, достаточно увеличить версию соответствующего тэга.

При написании программы мы в соответствии с бизнес-логикой связываем кэш с тэгами tag1 и tag2. При создании кэша мы записываем в него кроме данных еще текущие (на момент создания кэша) версии тэгов tag1 и tag2. При получении кэша мы считаем его валидным, если текущии версии тэгов tag1 и tag2 равны версиям, записанным в кэше. Таким образом, если мы изменяем (увеличиваем) версию тэга tag1, все кэши, связанные с этим тэгом, которые были построены ранее, перестанут быть валидными (т.к. в них записана меньшая версия тэга tag1). Версии тегов сохраняем в кеше вместе с данными (и сроками годности для решения «проблемы одновременного перестроения»):

[

   срок годности: 2008-11-07 21:00 

   данные кэша: [ … ] 

   тэги: [ 

​       tag1: 25 

​       tag2: 63 

   ] ]

 

Текущее значение версии тега хранится также в кеше в ключе с именем, совпадающим с именем тэга.

В качестве версии тэга можно было бы использовать просто числа, инкрементируя их при изменении версии тэга, но это может привести к некорректному поведению при условии возможной потери ключей.

В качестве версии удобнее использовать текущее время (с достаточной точностью, например, до миллисекунд). Тогда увеличение версии тэга будет всегда давать новую, бóльшую версию, даже в случае потери предыдущей версии. Версия тэга формируется на бекендах, их системные часы должны быть синхронизованы (без этого не будет работать и другая функциональность, например, корректное вычисление срока годности кэшей с коротким временем жизни),

Использование текущего времени в качестве версии тэга даёт еще одно преимущество в ситуации, когда БД проекта устроена по схеме мастер-слейв репликации. При изменении исходного объекта в БД мы изменяем версию тэга, связанного с ним (записываем туда текущее время, то есть время изменения). В другом процессе мы обнаруживаем, что кэш устарел, то есть его надо перестроить, перестроение – это читающий запрос (SELECT), который необходимо отправить на слейв-сервер БД, но в силу задержек репликации слейв-сервер еще мог не получить актуальную версию объекта в БД, в результате мы кэш сбросили, но при его перестроении снова закэшировали старый вариант объекта, что неприемлемо. Можно использовать версию тэга при решении вопроса, на какой сервер БД отправить запрос: если разница между текущим временем и версией какого-либо тэга кэша меньше некоторого интервала, определяемого максимальной задержкой репликации, мы отправляем запрос на мастер-сервер БД вместо слейва.

Использование такой схемы тэгирования увеличивает количество запросов к memcached, т.к. нам необходимо для каждого кэша получать версии его тэгов. Накладные расходы можно сократить за счет использование multi-get запросов memcached,

### Статистика работы memcached

Самая простая команда, stats, позволяет получить элементарную статистику: время работы сервера (uptime), объем используемой памяти, количество get запросов и количество хитов (hits), т.е. попаданий в кэш. Их соотношение позволяет нам судить об эффективности кэширования в целом. Параметр evictions – сколько ключей было удалено раньше истечения срока жизни может сигнализировать о недостаточности объема памяти memcached.

### Распределение памяти

Для распределения памяти под значения ключей memcached использует вариант slab-аллокатора. Распределение slab – это механизм эффективного распределения памяти и устранения значительной фрагментации. Основой этого алгоритма является сохранение выделенной памяти, содержащей объект определенного типа, и повторное использование этой памяти при следующем выделении для объекта того же типа

Распределитель slab хранит информацию о размещении этих участков, которые называются кэши. Таким образом, если поступает запрос на выделение памяти для объекта данных определенного размера, он может мгновенно удовлетворить запрос уже выделенным слотом. Однако, уничтожение объектов не освобождает память, а только открывает слот, который помещается в список свободных слотов распределителем slab. Следующий вызов для выделения памяти того же размера вернет слот памяти, не используемый в данный момент. Этот процесс устраняет необходимость в поиске подходящего участка памяти и значительно снижает фрагментацию памяти.

Когда процесс обращается за новым объектом ядра, система делает попытку найти свободное место для этого объекта в частично занятом slab’е в кэше для этого типа объектов. Если такого места не находится, система выделяет новый slab из смежных физических страниц и передает их в кэш. Новый объект размещается в этом slab’е, а это местоположение помечается как «частично занятое».

Отсутствует внутренняя фрагментация памяти. Распределение происходит быстро, поскольку система создает объекты заранее и легко выделяет их из slab’а.

Например, slab для хранения объектов размером 256 байт, при этом сам slab имеет размер 1 Мб, таким образом он может сохранить 4096 таких объектов. Память внутри такого slab’а выделяется только по 256 байт. Если у нас есть slab’ы для объектов размером 64, 128, 256, 1024 и 2048 байт, то максимальный размер объекта, который мы можем сохранить – 2048 байт (в последнем slabе). Если мы хотим сохранить объект размером 65 байт, под него будет выделена память в slab’е-128, 1 байт – в slab’е 64.

Чтобы добиться эффективного использования памяти memcached для хранения наших ключей и значений, мы должны быть уверены в правильном выборе размеров slab’ов, который выделил memcached, а также в их разумном наполнении. Для этого мы можем попросить memcached предоставить статистику по slab’ам 

$mc->getExtendedStat(‘slab’). 

Как правило, больше всего slab’ов выделено под ключи с относительно небольшими значениями – до 20 Кб, для больших по размеру ключей slab’ов гораздо меньше. Если график отличается от того, который ожидается по логике задачи, это повод для беспокойства.

### Мониторинг работы с memcached

Для мониторинга необходимо писать отладочную информацию о каждом событии в файл в режиме append или в БД (отдельно по конкретному ключу или сразу по всему кешу). Возможные события: кэш устарел (если дата устаревания хранится в кеше) (или не найден), попытка заблокироваться, запись (и построение) нового кэша, удаление блокировки, успешный запрос кэша.

### Другие примеры

**Кеширование категорий пользователей**. Если имеется набор фотографий и каждой назначена приватность по списку: владельцу, друзьям, выбранному кругу лиц. Кеширование списка можно организовать так: если типов доступа не много хранить отдельно три списка. Если много, то хранить весь список (если большой, то возможно с некоторым лимитом) и на стороне приложения делать фильтрацию списка по нужной группе пользователей.

### Борьба с холодным кешем

При неправильном распределении нагрузки возможна ситуация чрезмерного охлаждения кеша и соответственно перегрузки БД. Причем кеш может охладиться как из-за протухания одного очень популярного ключа, так и из-за одновременного протухания большого количества множества ключей. Это может произойти, если:

·  выгружен свежий «популярный» функционал на холодный кеш

·  множество ключей стартовали в одно и то же время и имеют одинаковый срок жизни. После истечения срока жизни ключей все запросы пользователей придут в базу и положат ее.

Основной способ решения заключается в поддержании кеша все время в горячем состоянии. Для это нужно «размазать» по времени протухание кеша и соответственно нагрузку на БД. Добиться этого можно несколькими способами:

·  плавным включением нового функционала. Для этого новый функционал выкатывается включённым на небольшую часть пользователей и постепенно количество пользователей увеличивается. В это случае решаются обе проблемы: (1) кеш прогревается постепенно и (2) ключи пользователей будут протухать также постепенно.

·  разным временем жизни разных элементов набора данных. Этот вариант технически более простой, т.к. функционал может быть включен на всех пользователей сразу. Однако система должна быть в состоянии выдержать пик, который наступит при выкатке всего функционала. Идея заключается в умножении время жизни кэша на случайный множитель. Например, в примере ниже на значение                                 :

$random_factor = *rand*(950, 1050) / 1000;
 $random_ttl = $ttl * $random_factor;

В случае падения системы перед включение нагрузки необходимо прогреть кеш, либо вручную, либо автоматически. 

###  «Горячие» ключи

Как правило, статистика обращений на чтение к ключам в кеше соответствует степенному распределению (и также в принципе, закону 20/80), т.е. большинство обращений идет к небольшому количеству ключей. 

Проблема: имеется один неделимый ключ, на который идет такое большое количество запросов на чтение, что он перегружает свой кеш-сервер (примеры таких популярных ключей: количество просмотров за день, количество зарегистрированных пользователей). Даже при использовании кластеризации в memcached (см. Кластеризация в memcached (consistent hashing)) атомарный ключ невозможно разделить по нескольким серверам и все запросы к нему будут идти на один сервер.

Решение: единственный способ снизить нагрузку на ключ – это задублировать этот ключ на разных кеширующих серверах. 

Самый простой, ручной вариант – писать этот ключ во все кеширующие сервера сразу. Это идеально подходит, если этот ключ может обновлять сразу на всех серверах фоновым процессом. Так как ключ лежит сразу на всех серверах, то его можно выбирать multi_get запросом вместе с другими ключами.

Если используется автоматическое consistent hashing , то можно реализовать автоматическое размазывание кеша. Для этого задублируем ключ, добавив к его названию число. Например, из одного ключа name получим 10 ключей [name0,… ,name10]. В каждый из этих ключей запишем одни и те же данные.  Т.к. эти ключи имеют разные название, при консистентном хэшировании они будут распределены между разными серверами и соответственно нагрузка также разделиться между разными серверами.

Подбирая количество дублирующихся ключей, можно найти компромисс между преимуществами и недостатками. Много дублей – усиливает недостатки (расход памяти, проблемы с консистентностью), мало дублей – снижает преимущества (нагрузка недостаточно размазывается по серверам). 

Недостаток: 

·  больший расход памяти под дублирующиеся ключи. Эта задача пример классического space-vs-time tradeoff.

·  нужно синхронизировать значение этого ключа на разных серверах