Микросервисы – функциональная декомпозиция системы.

## Преимущества

- удобная работа для больших команд
- простая архитектура, новые участники быстро погружаются в разработку
- быстрый и простой ci/cd
- легкая масштабируемость приложения
- низкая связанность и возможность быстрого внедрения новых технологий
- легкая Graceful degradation. Если упало что-то одно, то не падает все.
- Можно понять качество каждой команды (сервиса) по потреблению ресурсов (памяти кеша, ЦПУ)

![monolith-vs-microservice](/Users/paparshikov/repos/cheat-sheets/img/monolith-vs-microservice.png)

## Недостатки

- Требуется больше ресурсов (запросы в сеть, серверов)
- Сложность отладки
- Требуется больше людей
- Много сервисов - много грязной работы по их обслуживанию (подъем версий Go)



## Что нужно выносить

- то, что меняется очень часто и требуется изменять как можно быстрее. Повышается time to market. Это самый правильный путь.
- то что легко выносится и слабо зацеплено с остальным функционалом
- на то, что приходится наибольшая нагрузка

## Когда использовать







Примеры вынесения частей кода в микросервис:

- код, который не связан с *request* и выполняет в *cron* что-то в фоне. Например:
  - отправка писем из очереди
  - обращение к внешним сервисам
  - *suggest* в списке

Требования к сервисам в микросервисной архитектуре:

- они должны быть максимально изолированы от других сервисов. Вплоть до своей БД.

Применяемые техники:

- *Null object pattern*. Это *design pattern*.

  Проблема: Если маленький сервис (например, сервис с возвратом блока текущего пользователя) отказывает, то это может привести к каскадному отказу всей системы. В этом случае необходимо, чтобы вместо результата работы сервиса было подставлено какое-то *default* значение.

  Решение: Возвращаемое значение является не конкретным экземпляром класса, а *interface*. Существует две реализации этого interface: реализация с полным корректным содержанием, которая возвращается когда сервис работает, и реализация по умолчанию, которая возвращается когда сервис не работает.

- *Circuit breaker*

- *Health checker* – проверка работоспособности сервиса. Сервис должен проверить работоспособность всех зависимых подсервисов и после этого вернуть OK.

## Принципы микросервисной архитектуры

- На каждый микросервис – своя база данных
- каждая служба должна иметь лишь небольшой набор обязанностей (SRP)
- graceful degradation - если что-то упало, то все не деградирует
- приложение должно использовать шаблон Saga . Сервис публикует событие при изменении его данных. Другие службы используют это событие и обновляют свои данные. 
- *Design for failure* — т.к. необходимо разрабатывать код для распределенной системы, составные элементы которой взаимодействуют через сеть. А сеть ненадежна по своей природе. Сеть может просто отказать, может работать плохо, может вдруг перестать пропускать какой-то тип сообщений, потому что изменились настройки файрвола. 

## Проблемы в микросервисной архитектуре

- service discovery - найти сервис, который должен обслуживать запросы. есть централизованный registry с данными о сервисах

## Трассировка

*Distributed tracing*, также называемая распределенной трассировкой запросов, - это метод, используемый для профилирования и мониторинга приложений, особенно созданных с использованием микросервисной архитекту. Распределенная трассировка помогает точно определить, где происходят сбои и что вызывает снижение производительности.

### AV

Используются два инструмента:

- *OpenTracing* ([1](https://opentracing.io/specification/)) – это набор из:
  - *API specification*, независимая от *vendor*'а
  - *framework*'s и *library*'s в которых реализована *specification*
- Jaeger UI ([1](https://www.jaegertracing.io/)) – для визуализации 

Для работы tracing'а сервис должен пробрасывать специальные заголовки, например:

```
X-Request-Id
```

"Проброс" означает, что если в сервис пришел HTTP-запрос с указанным header'ом, то его необходимо добавить ко всем исходящим HTTP-запросам (без каких либо изменений).

### Общее

При логировании таймингов отдельных операций в лог-файл в микросервисной архитектуре, как правило, сложно понять что привело к вызову этих операций, отследить последовательность действий или смещение во времени одной операции относительно другой в разных сервисах.

Для удобства используются инструменты трассировки. 

Трассировка позволяет:

1. Найти узкие места в производительности как внутри одного сервиса, так и во всем дереве выполнения между всеми участвующими сервисами. Например:

   - Много коротких последовательных вызовов между сервисами, например, на геокодинг или к базе данных.
   - Долгие ожидания ввода вывода, например, передача данных по сети или чтение с диска.
   - Долгий парсинг данных.
   - Долгие операции, требующие cpu.
   - Участки кода, которые не нужны для получения конечного результата и могут быть удалены, либо запущены отложенно.

2. Наглядно понять в какой последовательности что вызывается и что происходит когда выполняется операция.

   ![img](https://habrastorage.org/webt/_1/hn/wo/_1hnwonn-nynd8j5orur89ouw9s.jpeg)

   Видно что, например, Запрос пришел в сервис WS -> сервис WS дополнил данные через сервис R -> дальше отправил запрос в сервис V -> сервис V загрузил много данных из сервиса R -> сходил в сервис P -> сервис Р еще раз сходил в сервис R -> сервис V проигнорировал результат и пошел в сервис J -> и только потом вернул ответ в сервис WS, при этом продолжая в фоне вычислять что-то еще.

   Без такого трейса или подробной документации на весь процесс очень сложно понять, что происходит, первый раз взглянув на код, да и код разбросан по разным сервисам и скрыт за кучей бинов и интерфейсов.

3. Сбор информации о дереве исполнения для последующего отложенного анализа. На каждом этапе выполнения в трейс можно добавить информацию, которая доступна на данном этапе и дальше разобраться какие входные данные привели к подобному сценарию. Например:

   - ID пользователя
   - Права
   - Тип выбранного метода
   - Лог или ошибка исполнения

4. Превращение трейсов в подмножество метрик и дальнейший анализ уже в виде метрик.

Основными понятиями в спецификации OpenTracing являются Trace, Span, SpanContext, Carrier, Tracer.

- Trace. Это временной интервал, в течение которого выполнялся один или несколько Span'ов, связанных между собой одним идентификатором traceId. Span'ы так же могут быть связаны между собой ссылками двух оcновных типов. **ChildOf** это обычная связь родитель — потомок. Она говорит о том, что для завершения родительского span'a требуется завершение дочернего. Связь **FollowsFrom** говорит лишь о том, что родительский span запустил другой span, но на завершение текущего он не влияет.
- **Span**. Это основная и минимальная единица информации в спецификации OpenTracing. Span описывает интервал во времени, в котором происходила работа. Например, вызов функции, которая делает запрос в БД за данными, можно описать как span, сохранив в нем необходимую информацию. При создании интервала обязательным полем является имя (например название функции), также неявно в Span записывается timestamp создания интервала и идентификатор spanId. Каждый интервал содержит traceId, если span является дочерним, то в него записывается traceId родительского интервала, если родительского spana'а нет, генерируется новый. Когда функция завершила свою работы, у объекта span мы должны вызвать метод finish. Этот метод запишет в Span timestamp завершения работы, а так же отправит получившийся span в Трассировщик (если это предусмотренно конкретной реализацией). 
- **SpanContext**. Это объект, описанный в спецификации OpenTracing, который содержит информацию, необходимую для связывания span'ов между собой при межсервисном взаимодействии. Контекст содержит идентификаторы traceId, spanId, а также любую информацию вида key:value, которую мы хотим передавать между микросервисами. 
- **Tracer**. Это конкретная имплементация спецификации OpenTracing, которая непосредственно предоставляет методы по созданию span'ов, генерации идентификаторов, создания контекстов и отправку завершенных интервалов на хранение в трассировщик (distributed tracing system) например **Jaeger** 

В трассировке есть понятие спан, это аналог одного лога, в консоль. У спана есть:

- Название, обычно это название метода который выполнялся
- Название сервиса, в котором был сгенерирован спан
- Собственный уникальный ID
- Какая-то мета информация в виде key/value, которую залогировали в него. Например, параметры метода или закончился метод ошибкой или нет
- Время начала и конца выполнения этого спана
- ID родительского спана

Каждый спан отправляется в collector спанов для сохранения в базу для последующего просмотра как только он завершил свое выполнение. В дальнейшем можно построить дерево всех спанов соединяя по id родителя. При анализе можно найти, например, все спаны в каком-то сервисе, которые заняли больше какого-то времени. Дальше, перейдя на конкретный спан, увидеть все дерево выше и ниже этого спана.

![image](https://habrastorage.org/webt/ev/64/g_/ev64g_pg2xffwnxvcasi9vn2tl0.png)

Есть общий стандарт *Opentracing*, который описывает как и что должно собираться, не привязываясь трассировкой к конкретной реализации в каком-либо языке. 

Имплементации Opentracing:

- Jaeger . 

Он состоит из нескольких компонент:

![image](https://habrastorage.org/webt/59/f0/93/59f093addc6fd068923241.png)



- Jaeger-agent — локальный агент, который обычно стоит на каждой машине и в него логируют сервисы на локальный дефолтный порт. Если агента нет, то трейсы всех сервисов на этой машине обычно выключены
- Jaeger-collector — в него все агенты посылают собранные трейсы, а он кладет их в выбранную БД
- База данных — предпочтительная у них cassandra, но у нас используется elasticsearch, есть реализации еще под пару других бд и in memory реализация, которая ничего не сохраняет на диск
- Jaeger-query — это сервис который ходит в базу данных и отдает уже собранные трейсы для анализа
- Jaeger-ui — это веб интерфейс для поиска и просмотров трейсов, он ходит в jaeger-query


![image](https://habrastorage.org/webt/_j/wu/ql/_jwuqlf6l_3wayc-urkvlgfsyuo.png)

Отдельным компонентом можно назвать реализацию opentracing jaeger под конкретные языки, через которую спаны отправляются в jaeger-agent.

Каждый сервис собирает тайминги и доп.инфу в спаны и скидывает их в рядом стоящий jaeger-agent по udp. Тот, в свою очередь, отправляет их в jaeger-collector. После этого трейсы доступны в jaeger-ui. 

Традиционное решение — присваивать транзакции (логу) на входе в систему уникальный ID. Затем этот ID (контекст) пробрасывается через всю систему по цепочке вызовов внутри сервиса или между сервисами

В качестве хранилища трассировок Jaeger может использовать **Cassandra**, **Elasticsearch** а также просто хранить трейсы в памяти, что удобно для тестов. 

## Шина данных

Преимущества:

- развязывание клиентов и серверов. Один высоконагруженный сервис развязывается от клиентов, что позволяет его ненагружать, сокращать количество клиентов. Клиенты которым нужна дополнительныя информация, могут самостоятельно обратиться снова к серверу.
- Отличичие шины от очереди:
  - Из очереди сообщения вычитываются навсегда, а из шины сообщения вычитываются многократно
  - Из шины данных сообщения вычитываются по порядку, шина данных 
- At least once delivery, нужно организовывать идемпотентность

# Pattern

## Транзакционный обмен сообщениями

*Servic*'у часто нужно публиковать *message* как часть *transaction*, обновляющей *database*. Например, *service* может публиковать *domain event*, когда обновляет или создает *business entity*. Обновление *database* и отправка *message* должны происходить в пределах одной *transaction*, иначе *service* может обновить *database* и, например, упасть до того, как *message* будет отправлено. Если не выполнять эти две операции атомарно, такая проблема может оставить систему в неконсистентном состоянии.

Традиционный подход – использование *distributed transaction*, которые охватывают *database* и *message broker*. Но *distributed transaction* — неудачное решение для современных приложений. Кроме того, они не поддерживаются во многих современных *message broker*'ах, таких как Apache Kafka.

### Transactional outbox

связано c [распределенные транзакции](Sharding.md#распределенные-транзакции)

#### Context

В логике *service* иногда требуется обновить *database* и отправить *message/event*. Например, *service*, участвующий в [saga](https://microservices.io/patterns/data/saga.html) должен атомарно обновить *database* и отправить *message/event*. Аналогично, *service*, который публикует [domain event](https://microservices.io/patterns/data/domain-event.html) должен атомарно обновлять [aggregate](https://microservices.io/patterns/data/aggregate.html) и публиковать *event*.

*Service* должна атомарно обновлять *database* и отправлять *message*, чтобы избежать неконсистентности данных и ошибок. Однако нецелесообразно использовать традиционные *distributed transaction* (2PC), которые охватывают *database* и *message broker*. *Message broker* может не поддерживать 2PC. 

Но без использования 2PC:

- отправка *message* во время выполнения *transaction* – не *reliable*. Нет гарантии, что *transaction* будет *commit*. 
- отправка *message* после *transaction commit*. Нет гарантии, что *service* не упадет перед отправкой *message*.

Кроме того, *message*'s должны отправляться *message broker*'у в том порядке, в котором они были отправлены *servic*'ом. Например,  [aggregate](https://microservices.io/patterns/data/aggregate.html) обновляется последовательностью *transaction*'s `T1`, `T2`и т.д. Эти *transaction*'s могут быть выполнены одним и тем же *service instance* или различными *service instance*'s. Каждая *transaction* публикует соответствующий *event*: `T1 -> E1`, `T2 -> E2` и т.д. Так как `T1`предшествует `T2`, *event* `E1` должен быть опубликован до `E2`.

#### Problem

Как *reliably/atomically* обновлять *database* и отправлять *message/event*?

#### Forces

- 2PC – не подходит
- Если происходит *transaction commit* – *message* должен быть отправлен. Иначе, если происходит *transaction rollback*, *message* НЕ должен быть отправлен.
- Сообщения должны отправляться брокеру сообщений в том порядке, в котором они были отправлены службой. Этот порядок должен быть сохранен для нескольких экземпляров службы, которые обновляют один и тот же агрегат.





3.3. Взаимодействие с помощью асинхронного обмена сообщениями 133

В связи с этим приложения должны задействовать другой механизм для надежной

публикации сообщений. Рассмотрим его.

