# Шардирование

Шардирование данных (*data sharding*) – разбиение данных на меньшие куски, или секции (shards) и хранение их на разных узлах. При этом образуются «глобальные» значения, которые не секционируются вовсе (например, списки городов). Обычно такие данные хранятся на одном узле, доступ к которому часто организуется через кэш, например *memcached*.

Проще с самого начала закладывать идеологию секционирования, если есть основания полагать, что это понадобится в будущем. Можно заранее прикинуть сколько будет данных и на основе этого выбирать решение. Данных как правило не так много, ведь людей всего 7-8 миллиардов. Т.е. у любой системы есть потолок, при достижении которого рост замедляется.

В секционированных приложениях часто применяется библиотека абстрагирования базы данных, которая упрощает взаимодействие между программой и секционированным хранилищем, однако она не может быть универсальной, т.к. часто приложение знает много информации о способах секционирования.

## Общие принципы

<u>Какие ограничения позволяет преодолеть:</u>

- данные могут физически не помещаться в память одного сервера;

- один сервер может не успевать обработать запросы

- так как данные поделены между несколькими узлами, выход из строя одного из узлов не уронит всю систему. 

<u>Проблемы, которые нужно решать:</u>

- обеспечить удобство системного администратора и снизить стоимость поддержки

- с увеличением количества узлов, возрастает вероятность выхода из строя;

- резевирование данных через репликацию. . Однако для больших кластеров слишком накладно, т.к. увеличивает количество серверов в два раза. Проще отключить эту часть системы на время восстанавления из RAID массива.

- проблема членства – определение членов кластера при постоянном изменении конфигурации

<u>Архитектура шардинг-кластера определяется:</u>

- выбор ключа шардирования
- выбор функции шардирования, т.е. распределения данных
- метод роутинга запросов
- решардинг

## Виды шардирования

http://backend.ruhighload.com/Шардинг+и+репликация

https://ruhighload.com/Вертикальный+шардинг

https://ruhighload.com/Горизонтальный+шардинг

### Вертикальный шардинг

Вертикальный шардинг — это разнесение отдельных таблиц или группы табли на отдельный сервер. 

Например, если есть таблицы:

- `users` — данные пользователей
- `photos` — фотографии пользователей
- `albums` — альбомы пользователей

Таблицу `users` оставляем на одном сервере, а таблицы `photos` и `albums` переносим на другой сервер. 

![vertical_sharding](https://parshikovpavel.github.io/img/sharding/vertical_sharding_1.jpg?2)

Такое шардирование не требует существенного изменения архитектуры приложения. Достаточно использовать разные соединения для доступа к разным таблицам:

```php
$users_connection = mysql_connect('10.10.0.1', 'root', 'pwd');
$photos_connection = mysql_connect('10.10.0.2', 'root', 'pwd';

$q = mysql_query('SELECT * FROM users WHERE ...', $users_connection);
$q = mysql_query('SELECT * FROM photos WHERE...', $photos_connection);
```

![vertical_sharding](https://parshikovpavel.github.io/img/sharding/vertical_sharding_2.jpg?2)

### Горизонтальный шардинг

Горизонтальный шардинг — это разделение одной таблицы на разные сервера. Это необходимо использовать для огромных таблиц, которые не умещаются на одном сервере. Разделение таблицы на куски делается по такому принципу:

- На нескольких серверах создается одна и та же таблица (только структура, без данных).
- В приложении выбирается условие, по которому будет определяться нужное соединение (например, четные на один сервер, а нечетные — на другой).
- Перед каждым обращением к таблице происходит выбор нужного соединения.

Многие NoSQL БД имеют встроенное горизонтальное шардирование, например Memcached ([1](Memcached.md#кластеризация-в-memcached-consistent-hashing))

### Шардинг + репликация

Шардинг и репликация часто используются совместно.

Такая схема часто используется не только для масштабирования, но и для обеспечения отказоустойчивости. Так, при выходе из строя одного из серверов шарда, всегда будет запасной.

## Примеры шардирования

Реализация обратного индекса для поиска сообщений пользователя по ключевым словам. Ключ секционирования – это соединение user_id и ключевого слова.  

## Выбор ключа секционирования

Цель выбора, чтобы самые важные и часто встречающиеся запросы затрагивали как можно меньше секций. Ключ определяет, в какую секцию попадает та или иная строка. От выбора ключа зависит какие будем делать запросы: в одну секцию, в несколько секций, перебирать все секции. Выбор ключа делается в начале построения архитектуры и позже его практически невозможно изменить.

Варианты выбора ключа:

- хеш первичного ключа таблицы. Плохо масштабируется, поскольку часто заставляет вас искать информацию во всех секциях, данные равномерно распределены по всем секциям. Имеет смысл, когда важно равномерное распределение данных, например, в распределенных программах поиска, где межсекционные запросы являются нормой. Однако там довольно простая схема данных и задача заключается в распределении ключевых слов по нодам.
- идентификатор важной сущности (единицы секционирования, *unit of  sharding*). Чаще всего, идентификатор пользователя. Можно изобразить модель данных в виде диаграммы сущность–связь и выявить ключ, по которому больше связей сущностей и больше связей в запросах. Например, *mail.ru* хранит все сессии одного пользователя на одном шарде, что позволяет разлогинить пользователя на всех устройствах, показывать сессии в профиле.
  - для объявлений лучше по  региону пользователя, т.к. часто выбирают объявления из одного региона. А в соцсети часто общаются люди из одного региона

Если в качестве ключа выбран `user_id`, то можно в этот же шард упаковывать все его посты, комментарии и пр. Это упрощает запросы и вместо нескольких запросов можно сделать один запрос с `JOIN`. Недостатки размещения всех связанных данных в один шард: 

- в какой-то момент шарды станут неравномерными, если использовать фиксированные распределение. Придется использовать дополнительное шардирование, например, по комментам, либо использовать динамическое распределение.

- степень теплоты шардов со временем станет неравномерной. И если появятся очень активные пользователи, то объем трафика на один ключ шардинга, может стать больше чем может обработать один сервер. Необходимо выбрать достаточно маленький ключ шардирования, чтобы он нагрузка на него влезала на один сервер. Например, что-то меньшее по размеру чем популярный пользователь.

Не существует строго способа выбора ключа, нужно смотреть не на данные, а на важный use-case в бизнес-логике, самый производительный способ. Нужно найти компромисс: часть запросов будет выполняться быстро, а часть медленно на всем кластере.

Встречаются запросы, требующие агрегирования данных, хранящихся в разных секциях. Например, топ популярных пользователей требует данных из всех секций таблицы пользователей. Желательно такой запрос расщепить и параллельно выполнить в каждой секции. Так как такие запросы дороги, как правило, результаты кешируются, либо memcached либо сводными таблицами.



### Дублирование данных в нескольких shard'ах

Как правило, существует более одного ключа секционирования, т.е. на данные можно взглянуть с позиции разных ключей, поэтому зачастую приходится дублировать данные, чтобы избежать межсекционных запросов. Например, данные приложения для ведения блогов можно секционировать по идентификатору пользователя и по идентификатору поста, поскольку и то и другое – осмысленные способы взглянуть на данные. Действительно, часто вы хотите видеть все сообщения одного пользователя и все комментарии к посту. Но секционирование по пользователям не поможет в поиске комментариев к посту, а секционирование по постам бесполезно для поиска сообщений данного пользователя. 

Если вы хотите, чтобы оба запроса затрагивали только одну секцию, то придется секционировать обоими способами. Можно либо полностью задублировать комментарии, либо хранить полные комментарии только в одном виде секционирования, например вместе с данными о пользователях. Тогда в секции данных о постах достаточно хранить только заголовок и идентификатор комментария. При этом для построения большинства страниц, содержащих комментарии к постам, не придется обращаться к обоим секциям, ну а если потребуется вывести полный текст комментария, то его можно будет достать из хранилища данных о пользователях (схема используется в Twitter).

### Генерация глобально уникальных идентификаторов

Для шардированной таблицы необходимо генерировать глобально уникальные идентификаторы на разных машинах. Если на одном сервере можно использовать `AUTO_INCREMENT`, то для шардированной таблицы необходимо использовать приемы.

Приемы:

- Использовать  конфигурационные  параметры  `auto_increment_increment` и  `auto_increment_offset`, которые говорят серверу MySQL, на какую величину увеличивать автоинкрементный столбец и с какого значения начинать нумерацию. Можно их настроить, например, на чередование. Недостаток – требует внимания при конфигурировании серверов.


- Использовать глобальный генератор идентификатора. При этом необходимо следить за тем, чтобы эта единственная точка конкурентного доступа не стала узким местом приложения. Для снижения нагрузки с узла генерации идентификаторов можно выделять значения шардам не одиночно, а сериями. 

  Разновидности:

  - Использовать ключ ` memcached`, удобно использовать функцию `incr()`. При перезагрузке сервера memcached придется заново инициализировать начальное значение.

  - Создать таблицу с `AUTO_INCREMENT` столбцом глобальном узле. Самый простой вариант – таблица MyISAM вида:

    ```mysql
    mysql> CREATE TABLE single_row ( 
    	col1 int NOT NULL AUTO_INCREMENT, 
    	col2 int NOT NULL, 
    	PRIMARY KEY(col1), 
    	UNIQUE KEY(col2) 
    ) ENGINE=MyISAM;
    mysql> REPLACE INTO single_row(col2) VALUES(1);
    ```

    При вставке очередной строки в эту таблицу:

    ```mysql
    REPLACE INTO single_row(col2) VALUES(1);
    ```

    В таблице останется одна строка, а значение столбца `col1` будет инкрементировано.
    
    Чтобы база данных, генерирующая ключи, не стала единственной точкой отказа можно определить две такие базы данных, одна из которых будет генерировать четные идентификаторы, а другая - нечетные. Для MySQL следующий сценарий может определять такие последовательности:
    
    ```
    KeyGeneratingServer1:
    auto-increment-increment = 2
    auto-increment-offset = 1
    
    KeyGeneratingServer2:
    auto-increment-increment = 2
    auto-increment-offset = 2
    ```
    
    Мы можем поставить балансировщик нагрузки перед обеими этими базами данных для циклического переключения между ними и устранения простоев.

- Сгенерировать  GUID (глобально уникальные идентификаторы) с помощью функции `UUID_SHORT()`, которая возвращает короткие и последовательные значения, приемлемые в качестве первичных ключей (в отличии от функции `UUID()`, возвращающей длинные непоследовательные значения).

- [Использовать сервис генерации ключей (KGS)](SystemDesign.md)

## Выбор функции шардирования

Существует способы распределения данных по секциям: фиксированное (детерминированное, sharding function), динамическое (недерминированное, table function), явное. Эти способы задаются функцией секционирования:

```
f(key,count_servers) = shard
```

принимает на вход ключ секционирования строки и количество серверов и возвращает номер секции, в которой эта строка находится. Хорошая функция секционирования делает равномерные секции с равной нагрузкой. 

### Фиксированное распределение

Фиксированное распределение (детерминированное, sharding function) означает, что применяется функция разбиения. Аргумент этой функции – ключ секционирования. 

Важно: ключ секционирования это не значит ПЕРВИЧНЫЙ ключ. ([1](#выбор-ключа-секционирования), [2](Highload.md#шардирование данных))

Такие функции отображают значения ключей секционирования на конечное число «ячеек» (*buckets*), в которых хранятся данные. 

Пример плохой *sharding function*: использование первой буквы ключа секционирования.

<u>Достоинства:</u>

- простота и низкие накладные расходы. 


- В новых приложениях предпочтительней пользоваться динамическим распределением. Но при добавлении секционирования в существующее приложение бывает проще применить фиксированную стратегию. 


<u>Недостатки:</u>

- нельзя управлять размещением записей, а это важно в приложениях, где нагрузка на единицы секционирования неравномерна. В результате может получиться «перекос» как в объёме хранимых данных, так и в количестве запросов к ним. Стратегия фиксированного распределения не позволяет снять нагрузку, переместив часть данных в другую секцию.


- сложный решардинг требуется перераспределить всех ключей, что дорого особенно при хранении данных на диске.

Badoo: также задача перемещения данных между секциям возникает, если имеется 2 датацентра, удаленные друг от друга, и пользователь перемещается ближе к другому датацентру. Отсутствует возможность переместить индивидуально пользователя с одного бакета в одном ДЦ в другой бакет в другом ДЦ.



#### Хеширование по модулю

Хеширование по модулю (*modulo hashing*).

Реализуется следующим образом:

- Для числовых ключей – нахождение остатка от деления самого ключа на количество серверов:


  ```
f(key) = key % count_servers
  ```

- Для строковых ключей – нахождения остатка от деления числового представления ключа, полученного с помощью хеш-функции:


```
f(key) = crc(32) % count_servers
```

В качестве хеш-функции выбирают CRC32, т.к. считается быстро и не требуется криптостойкости. 

<u>Преимущество:</u>

- равномерное распределение данных по серверам


<u>Недостатки:</u>

- при решардинге (изменении количества серверов) значительная часть кеша теряется.


#### Консистентное кеширование

Консистентное кеширование (consistent hashing) – это специальный тип *sharding function*. 

В алгоритме консистентного кеширования добавляется промежуточное отображение, *sharding function* состоит сразу из двух функций:

- функции отображения ключей на слоты, количество которых намного больше количества серверов (например, 100-200 слотов на сервер). 


- функции отображения слотов на физические сервера.

При изменении количества серверов количество слотов не меняется. Изменяется только отображение задаваемое функцией отображения слотов на физические сервера:

- если один из серверов выходит из строя, то все слоты, которые к нему относились, распределяются между оставшимися;


- если добавляется новый сервер, то ему передаётся часть слотов от уже имеющихся серверов.

Преимущество:

- Позволяет гибко!!! управлять распределением слотов по шардам, путем задания параметров для формулы *consistent hashing*
- Упрощается решардинг. Добавление или удаление сервера влечет за собой перераспределение только части ключей (k / N, где k — общее количество ключей, а N — количество серверов). 


Обычно идею согласованного хеширования визуализируют с помощью числовой оси, «закрученной» в кольцо. Если хеш-функция может принимать целые значения от 0 до 2^32, то на кольце склеиваем 0 и 2^32. Каждому сервера из пула сопоставляем точку на кольце. Точки показывают границы диапазонов слотов. В качестве сервера для хранения ключа выбираем сервер в точке, ближайшей к точке ключа в направлении по часовой стрелке. Если сервер удаляется из пула или добавляется в пул, на оси появляется или исчезает точка сервера, в результате чего лишь часть ключей перемещается на другой сервер. В реальности, т.к. число слотов гораздо больше числа серверов, одному серверу ставится в соответствие 100-200 точек на оси, что улучшает равномерность распределения ключей по серверам в случае изменения их конфигурации. Слоты одного сервера могут быть расположены на числовой оси не подряд, в непрерывном диапазоне, а перемежаться со слотами других серверов.

- 


### Динамическое (недетерминированное, table function)

Описание порядка секционирования хранится отдельно в виде отображения ключа секционирования на номер секции (*table function*). Примером может служить таблица с двумя столбцами (ее называют *config*): идентификатор пользователя и идентификатор секции:

```mysql
CREATE TABLE user_to_shard ( 
   user_id INT NOT NULL, 
   shard_id INT NOT NULL, 
   PRIMARY KEY (user_id) 
); 
```

Функцией разбиения служит сама таблица. 

Недостатки:

- накладные расходы, так как требуется обращение к внешнему ресурсу, например серверу каталогов (узлу, на котором хранится отображение). Ускорить доступ к таблице разбиения может распределенная система кэширования (*memcached*) или даже просто хранить эту карту в коде (если она не меняется часто).

Преимущества:

- более точное управление местом хранения данных. Это упрощает равномерное разделение данных по секциям и позволяет гибко адаптироваться к непредвиденным изменениям. Например, если имеют место запросы к списку постов одной категории, можно помещать эту категорию в одну секцию. Можно  создавать несбалансированные секции, если мощность серверов разная, или на некоторых серверах имеется дополнительная нагрузка. Также удобно если степень теплоты секций разная, т.к. иначе холодные секции будут простаивать и вычислительные ресурсы используются неэффективно. Можно перебалансировать секции в любой момент, делать секции нужного размера (типа badoo). 


- Динамическое распределение и применение правила близости секций может предотвратить рост межсекционных запросов по мере масштабирования. В случае шардирования в *Memcached* можно выбрать несколько ключей за один *multi_get* запрос к серверу. 

### Использование виртуальных шардов (vshard, бакетов, vbuckets, спотов, spot)

Имеет смысл сделать размер секции значительно меньше, чем емкость узла, так чтобы в одном узле можно было хранить несколько секций. Секции меньшего размера удобнее перемещать, что упрощает задачу перераспределения емкости и поиска оптимального баланса между секциями и узлами. Перемещение данных между секциями обычно намного сложнее, чем перемещение самих секций. Упрощается резервное копирование и восстановление информации, а кроме того, для небольших таб лиц быстрее завершаются такие действия, как изменение схемы. При перемещении секции необходимо ее перевести в режим чтения, выгрузить данные и переместить их в другой узел. Размер секции должен быть таков, чтобы для регулярного выполнения задач обслуживания (`ALTER TABLE`, `CHECK TABLE`, `OPTIMIZE TABLE`) достаточно было 5–10 минут.

Означает, что в отображении key→shard появляется промежуточное отображение `key → vbucket → shard`. Т.е. ключ отображается на виртуальный *bucket* (*vbucket*), а виртуальный *bucket* – на физический шард (реальный бакет, *bucket*). Виртуальные – т.к. не прикреплены к реальному физическому серверу. 

Число *vbucket* определяется заранее, берется достаточно большим и обычно является степенью двойки (например, в *Couchbase* по умолчанию 1024 *vbucket*’а)

Существует два варианта отображения:

1. `key → vbucket` с помощью *sharding function*, `vbucket  → shard` с помощью *table function*

2. `key → vbucket` с помощью *table function*, `vbucket → shard` с помощью *table function*. Используется если нужно перемещать данные между vbucket, например, пользователя в другой дата-центр.

Отображение `vbucket → shard` называется словарем

Преимущества:

- В 1 варианте позволяет снизить объем *table function*, при этом оставив возможность управления распределением.


- Имея большое количество *vbuckets* можно упростить решардинг.


Недостаток:

- Более сложный поиск *shard* по известному `key`


### Явное

Cпособ выбора секции при создании строки. Номер секции кодируется в ее идентификаторе. Например, так:

```
первичный_ключ = номер_секции<<56 + номер_записи
```

Преимущество:

- идентификатор каждого объекта уже несет в себе собственный ключ секционирования, тогда как остальные решения подразумевают соединение таблиц или иную операцию поиска для нахождения такого ключа. Можно также для соответствия 1НФ (только одно значение в одном атрибуте) хранить ключ секционирования в отдельном столбце. Можно скомбинировать с динамическим распределением – в строке указывается не номер секции, а номер некоторый группы (ячейка), а уже в какую секцию отражается эта группа (ячейка) определеяется динамическим методом. Это позволяет удобно управлять взаимосвязанными данными.

## Способы именования в шардах

- Размещать на каждом узле одну базу данных и называть все эти базы одинаково. Каждая секция повторяла структуру исходной базы данных.


- Использовать одну базу данных на каждую секцию и включать в включать номер секции в имя каждой таблицы (`comments_1`, `messages_1`).


- Использовать одну базу данных на каждую секцию и включать в эту базу все таблицы. Номер секции тогда является частью имени базы, а не таблицы (`fishki_23.comments`, `fishki_23.messages`).


- Использовать одну базу данных на каждую секцию и включать номер секции как в имя базы, так и в имена таблиц (`fishki_1.comments_1`).


Более удобен последний вариант, т.к. секция целиком находится в одной базе данных и ее легко перемещать. При этом имена таблиц глобально уникальны.

## Роутинг запросов

Роутинг – механизм поиска шардов в кластере по ключу.

### Умный клиент (smart client)

Table function (config) зашито в ваш клиент

  

Преимущества:

·  Простота, скорость

Недостатки:

·  Решардинг очень сложен, т.к. table function внутри работающих приложений, которые надо завершить и обновить

·  Table function дублируется для всех клиентов на разных языках программирования

### Прокси (proxy)

Логика роутинга выносится в специальный прокси, который скрывает особенности работы с шард-кластером, сам взаимодействует с шардами. Клиент думает, что работает с одной нодой, отправляет туда запросы и получает оттуда данные.

  

Преимущества:

·  Удобство работы с одной нодой для клиента

·  Управление шардингом (решардинг, перебалансировка) в одной точке

Недостатки:

·  удвоение трафика внутри площадки, можно избежать этого разместив прокси на том же хосте, на котором работает само приложение

·  прокси – точка отказа

### Координатор (coordinator)

Координатор – это место хранения логики функции шардирования. Координатор по ключу шардирования возвращает номер шарда. После этого клиент сам устанавливает соединение с нужной нодой.

  

Преимущества:

·  «Координатор» гораздо проще «прокси»: в самом простом виде – высокопроизводительный сервер с базой данных или in-memory БД, или простой скрипт.

·  Т.к. координатор прост, его легко резервировать

### Роутинг внутри БД (intra-database rounting)

Используется в больших базах данных (100 тысяч узлов и более), в которых отсутствует возможность хранить функцию хеширования в одном месте. Роутинг реализуется методом случайного поиска в облаке нод. 

  

Алгоритм:

·  Выбирается случайная нода, на которую адресуется запрос с ключом шардирования

·  Если это ключ данной ноды или ее соседей, о которых она знает, то в ответ на запрос возвращаются данные. Т.е. сама БД выступает как прокси.

·  Если нет, то запрос форвардится на случайную node. 

Процесс сходится за логарифмическое время. Реализован в Redis . 

## Перебалансировка

Перебалансировка – перенесение vbuckets на другие физические node. Требует изменение словаря (т.е. отображения vbuckets -> node). 

Варианты организации:

·  Только для чтения. Проще всего перевести переносимый vbucket перевести в режим только для чтения. Перенос можно выполнить во время малой активности пользователей и при выполнении изменений выдать сообщение о maintenance. Если vbucket’ы достаточно маленькие, перенос займет малое количество времени.

·  Все данные неизменяемые. Полностью изменить архитектуру системы – все изменения данных осуществлять путем INSERT в виде лога событий. Если что-то надо изменить, то вместо UPDATE в таблицу лога изменений вставляется новое значение, если удалить – вместо DELETE вставить метку, что данных больше нет. При таком подходе при перебалансировке данные пишутся в vbucketsTo, а читаются из vbucketsFrom. Данные также удобно версионировать. 

·  Реплицировать vbucket. До полной синхронизации все операции выполняются с vbucketFrom, затем переключаемся на vbucketTo. Это наиболее универсальное решение. 

## Решардинг

Решардинг – изменение схемы шардирования и ее параметров (в том числе изменение числа vbuckets/vnode).

Лучший вариант – не поддерживать решардинг и заранее выбирать достаточно большее количество vbuckets. 

При нарастании нагрузки необходимо делить один шард на два, распределяя по ним данные. При этом удваиваться постоянно очень дорого, гораздо удобней использовать схему с виртуальными шардами.  Процесс переноса vbuckets выполняетсяпростыми операциями: дампами, репликацией. 

Проблемы решардинга:

·  существенная нагрузка на сеть и на node

·  чаще всего требуется временно отключить часть фукционала. При этом клиенты не должны этого заметить.

·  В процессе решардинга копии данные временно находятся на двух узлах. 

Лучшие схемы решардинга – которые выполняются автоматически. 

### Схемы решардинга

#### Автоматический решардинг

##### Перемещение при обновлении (Update is a move)

При изменении значения ключа он переносится с одного шарда на другой. После того как все данные после обновления переехали на новые шарды, старые шарды можно вывести из строя.

##### Протухание данных (data expiration)

Может быть использована в случае, если допустимо удаление старых данных. Аналогично, как в memcached, протухшие данные можно просто удалить, а заново сгенерированные данные помещаются в старые или новые шарды. 

##### Новые данные на выбранные сервера (new data on selected servers)

При заполненности шард данными добавляются новые сервера и некоторое время новые пользователи льются только на них. Когда секция достаточно заполнена, можно выставить флаг, который говорит приложению, что больше туда не нужно добавлять информацию. После этого открывается регистрация на старые сервера (возможно все даже в ручном режиме). Если по прошествии времени узел окажется недогруженным, то флаг можно сбросить. В Badoo считают теплоту секции и сопоставляют по бенчмаркам ее с производительностью сервера. Если секция слишком нагрелась, то в нее перестают писать данные. По мере охлаждения секции и снижения активности пользователей, запись в нее снова может быть открыта.

#### Ручной решардинг

##### Деление vbucket на несколько

Например, удвоить количество vbucket можно разделив каждый vbucket на два. Например, если у нас изначально один vbucket, изменяем отображение key -> vbucket так, чтобы половина данных указывала на тот же первый vbucket, а половина – на новый второй vbucket. В отображение vbucket -> node нужно изначально указать, что оба vbucket лежат на исходной node.

Если отображение key -> vbucket задано через sharding function:

  

то его можно изменить на 

  

При этом также половина ключей будут соответствовать все тому же номеру бакета, а половина переедет в новые vbuckets. В этом случае новый словарь с отображением vbucket -> node легко получить присоединив с конца его копию:

$dictionary = $dictionary+$dictionary

После удвоения vbuckets их можно переносить между node, т.е. выполнять перебалансировку

Уменьшение числа vbucket’ов происходит в обратном порядке — сначала перенести vbucket на node к другому vbucket попарно, а потом соединить пару в один vbucket путем обновления словаря. 

Если единицами данных являются базы данных, то при увеличении количества vbucket необходимо решить проблему деления каждой базы на две. 

## Распределенные транзакции

смотреть [Микросервисы.Распределенные транзакции](Microservice.md#распределенные-транзакции)

