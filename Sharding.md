# Шардирование

Шардирование данных (*data sharding*) – разбиение данных на меньшие куски, или секции (shards) и хранение их на разных узлах. При этом образуются «глобальные» значения, которые не секционируются вовсе (например, списки городов). Обычно такие данные хранятся на одном узле, доступ к которому часто организуется через кэш, например *memcached*.

Проще с самого начала закладывать идеологию секционирования, если есть основания полагать, что это понадобится в будущем. Можно заранее прикинуть сколько будет данных и на основе этого выбирать решение. Данных как правило не так много, ведь людей всего 7-8 миллиардов. Т.е. у любой системы есть потолок, при достижении которого рост замедляется.

В секционированных приложениях часто применяется библиотека абстрагирования базы данных, которая упрощает взаимодействие между программой и секционированным хранилищем, однако она не может быть универсальной, т.к. часто приложение знает много информации о способах секционирования.

<u>Какие ограничения позволяет преодолеть:</u>

- данные могут физически не помещаться в память одного сервера;

- один сервер может не успевать обработать запросы

- так как данные поделены между несколькими узлами, выход из строя одного из узлов не уронит всю систему. 

<u>Проблемы, которые нужно решать:</u>

- обеспечить удобство системного администратора и снизить стоимость поддержки

- с увеличением количества узлов, возрастает вероятность выхода из строя;

- резевирование данных через репликацию. . Однако для больших кластеров слишком накладно, т.к. увеличивает количество серверов в два раза. Проще отключить эту часть системы на время восстанавления из RAID массива.

- проблема членства – определение членов кластера при постоянном изменении конфигурации

<u>Архитектура шардинг-кластера определяется:</u>

- выбор ключа шардирования
- выбор функции шардирования, т.е. распределения данных
- метод роутинга запросов
- решардинг

## Виды шардирования

http://backend.ruhighload.com/Шардинг+и+репликация

https://ruhighload.com/Вертикальный+шардинг

https://ruhighload.com/Горизонтальный+шардинг

### Вертикальный шардинг

Вертикальный шардинг — это разнесение отдельных таблиц или группы табли на отдельный сервер. 

Например, если есть таблицы:

- `users` — данные пользователей
- `photos` — фотографии пользователей
- `albums` — альбомы пользователей

Таблицу `users` оставляем на одном сервере, а таблицы `photos` и `albums` переносим на другой сервер. 

![vertical_sharding](https://parshikovpavel.github.io/img/sharding/vertical_sharding_1.jpg?2)

Такое шардирование не требует существенного изменения архитектуры приложения. Достаточно использовать разные соединения для доступа к разным таблицам:

```php
$users_connection = mysql_connect('10.10.0.1', 'root', 'pwd');
$photos_connection = mysql_connect('10.10.0.2', 'root', 'pwd';

$q = mysql_query('SELECT * FROM users WHERE ...', $users_connection);
$q = mysql_query('SELECT * FROM photos WHERE...', $photos_connection);
```

![vertical_sharding](https://parshikovpavel.github.io/img/sharding/vertical_sharding_2.jpg?2)

### Горизонтальный шардинг

Горизонтальный шардинг — это разделение одной таблицы на разные сервера. Это необходимо использовать для огромных таблиц, которые не умещаются на одном сервере. Разделение таблицы на куски делается по такому принципу:

- На нескольких серверах создается одна и та же таблица (только структура, без данных).
- В приложении выбирается условие, по которому будет определяться нужное соединение (например, четные на один сервер, а нечетные — на другой).
- Перед каждым обращением к таблице происходит выбор нужного соединения.

Многие NoSQL БД имеют встроенное горизонтальное шардирование, например Memcached ([1](Memcached.md#кластеризация-в-memcached-consistent-hashing))

### Шардинг + репликация

Шардинг и репликация часто используются совместно.

Такая схема часто используется не только для масштабирования, но и для обеспечения отказоустойчивости. Так, при выходе из строя одного из серверов шарда, всегда будет запасной.

## Примеры шардирования

Реализация обратного индекса для поиска сообщений пользователя по ключевым словам. Ключ секционирования – это соединение user_id и ключевого слова.  

## Выбор ключа секционирования

Цель выбора, чтобы самые важные и часто встречающиеся запросы затрагивали как можно меньше секций. Ключ определяет, в какую секцию попадает та или иная строка. От выбора ключа зависит какие будем делать запросы: в одну секцию, в несколько секций, перебирать все секции. Выбор ключа делается в начале построения архитектуры и позже его практически невозможно изменить.

Варианты выбора ключа:

- хеш первичного ключа таблицы. Плохо масштабируется, поскольку часто заставляет вас искать информацию во всех секциях, данные равномерно распределены по всем секциям. Имеет смысл, когда важно равномерное распределение данных, например, в распределенных программах поиска, где межсекционные запросы являются нормой. Однако там довольно простая схема данных и задача заключается в распределении ключевых слов по нодам.

- идентификатор важной сущности (единицы секционирования, *unit of  sharding*). Чаще всего, идентификатор пользователя. Можно изобразить модель данных в виде диаграммы сущность–связь и выявить ключ, по которому больше связей сущностей и больше связей в запросах. Например, *mail.ru* хранит все сессии одного пользователя на одном шарде, что позволяет разлогинить пользователя на всех устройствах, показывать сессии в профиле.

Если в качестве ключа выбран `user_id`, то можно в этот же шард упаковывать все его посты, комментарии и пр. Это упрощает запросы и вместо нескольких запросов можно сделать один запрос с `JOIN`. Недостатки размещения всех связанных данных в один шард: 

- в какой-то момент шарды станут неравномерными, если использовать фиксированные распределение. Придется использовать дополнительное шардирование, например, по комментам, либо использовать динамическое распределение.

- степень теплоты шардов со временем станет неравномерной. И если появятся очень активные пользователи, то объем трафика на один ключ шардинга, может стать больше чем может обработать один сервер. Необходимо выбрать достаточно маленький ключ шардирования, чтобы он нагрузка на него влезала на один сервер. Например, что-то меньшее по размеру чем популярный пользователь.

Не существует строго способа выбора ключа, нужно смотреть не на данные, а на важный use-case в бизнес-логике, самый производительный способ. Нужно найти компромисс: часть запросов будет выполняться быстро, а часть медленно на всем кластере.

Встречаются запросы, требующие агрегирования данных, хранящихся в разных секциях. Например, топ популярных пользователей требует данных из всех секций таблицы пользователей. Желательно такой запрос расщепить и параллельно выполнить в каждой секции. Так как такие запросы дороги, как правило, результаты кешируются, либо memcached либо сводными таблицами.



### Дублирование данных в нескольких shard'ах

Как правило, существует более одного ключа секционирования, т.е. на данные можно взглянуть с позиции разных ключей, поэтому зачастую приходится дублировать данные, чтобы избежать межсекционных запросов. Например, данные приложения для ведения блогов можно секционировать по идентификатору пользователя и по идентификатору поста, поскольку и то и другое – осмысленные способы взглянуть на данные. Действительно, часто вы хотите видеть все сообщения одного пользователя и все комментарии к посту. Но секционирование по пользователям не поможет в поиске комментариев к посту, а секционирование по постам бесполезно для поиска сообщений данного пользователя. 

Если вы хотите, чтобы оба запроса затрагивали только одну секцию, то придется секционировать обоими способами. Можно либо полностью задублировать комментарии, либо хранить полные комментарии только в одном виде секционирования, например вместе с данными о пользователях. Тогда в секции данных о постах достаточно хранить только заголовок и идентификатор комментария. При этом для построения большинства страниц, содержащих комментарии к постам, не придется обращаться к обоим секциям, ну а если потребуется вывести полный текст комментария, то его можно будет достать из хранилища данных о пользователях (схема используется в Twitter).

### Генерация глобально уникальных идентификаторов

Для шардированной таблицы необходимо генерировать глобально уникальные идентификаторы на разных машинах. Если на одном сервере можно использовать `AUTO_INCREMENT`, то для шардированной таблицы необходимо использовать приемы.

Приемы:

- Использовать  конфигурационные  параметры  `auto_increment_increment` и  `auto_increment_offset`, которые говорят серверу MySQL, на какую величину увеличивать автоинкрементный столбец и с какого значения начинать нумерацию. Можно их настроить, например, на чередование. Недостаток – требует внимания при конфигурировании серверов.


- Использовать глобальный генератор идентификатора. При этом необходимо следить за тем, чтобы эта единственная точка конкурентного доступа не стала узким местом приложения. Для снижения нагрузки с узла генерации идентификаторов можно выделять значения шардам не одиночно, а сериями. 

  Разновидности:

  - Использовать ключ ` memcached`, удобно использовать функцию `incr()`. При перезагрузке сервера memcached придется заново инициализировать начальное значение.

  - Создать таблицу с `AUTO_INCREMENT` столбцом глобальном узле. Самый простой вариант – таблица MyISAM вида:

    ```mysql
    mysql> CREATE TABLE single_row ( 
    	col1 int NOT NULL AUTO_INCREMENT, 
    	col2 int NOT NULL, 
    	PRIMARY KEY(col1), 
    	UNIQUE KEY(col2) 
    ) ENGINE=MyISAM;
    mysql> REPLACE INTO single_row(col2) VALUES(1);
    ```

    При вставке очередной строки в эту таблицу:

    ```mysql
    REPLACE INTO single_row(col2) VALUES(1);
    ```

    В таблице останется одна строка, а значение столбца `col1` будет инкрементировано.
    
    Чтобы база данных, генерирующая ключи, не стала единственной точкой отказа можно определить две такие базы данных, одна из которых будет генерировать четные идентификаторы, а другая - нечетные. Для MySQL следующий сценарий может определять такие последовательности:
    
    ```
    KeyGeneratingServer1:
    auto-increment-increment = 2
    auto-increment-offset = 1
    
    KeyGeneratingServer2:
    auto-increment-increment = 2
    auto-increment-offset = 2
    ```
    
    Мы можем поставить балансировщик нагрузки перед обеими этими базами данных для циклического переключения между ними и устранения простоев.

- Сгенерировать  GUID (глобально уникальные идентификаторы) с помощью функции `UUID_SHORT()`, которая возвращает короткие и последовательные значения, приемлемые в качестве первичных ключей (в отличии от функции `UUID()`, возвращающей длинные непоследовательные значения).

- [Использовать сервис генерации ключей (KGS)](SystemDesign.md)

## Выбор функции шардирования

Существует способы распределения данных по секциям: фиксированное (детерминированное, sharding function), динамическое (недерминированное, table function), явное. Эти способы задаются функцией секционирования:

```
f(key,count_servers) = shard
```

принимает на вход ключ секционирования строки и количество серверов и возвращает номер секции, в которой эта строка находится. Хорошая функция секционирования делает равномерные секции с равной нагрузкой. 

### Фиксированное распределение

Фиксированное распределение (детерминированное, sharding function) означает, что применяется функция разбиения. Аргумент этой функции – ключ секционирования. 

Такие функции отображают значения ключей секционирования на конечное число «ячеек» (*buckets*), в которых хранятся данные. 

Пример плохой *sharding function*: использование первой буквы ключа секционирования.

<u>Достоинства:</u>

- простота и низкие накладные расходы. 


- В новых приложениях предпочтительней пользоваться динамическим распределением. Но при добавлении секционирования в существующее приложение бывает проще применить фиксированную стратегию. 


<u>Недостатки:</u>

- нельзя управлять размещением записей, а это важно в приложениях, где нагрузка на единицы секционирования неравномерна. В результате может получиться «перекос» как в объёме хранимых данных, так и в количестве запросов к ним. Стратегия фиксированного распределения не позволяет снять нагрузку, переместив часть данных в другую секцию.


- сложный решардинг требуется перераспределить всех ключей, что дорого особенно при хранении данных на диске.


Badoo: также задача перемещения данных между секциям возникает, если имеется 2 датацентра, удаленные друг от друга, и пользователь перемещается ближе к другому датацентру. Отсутствует возможность переместить индивидуально пользователя с одного бакета в одном ДЦ в другой бакет в другом ДЦ.

#### Хеширование по модулю

Хеширование по модулю (*modulo hashing*).

Реализуется следующим образом:

- Для числовых ключей – нахождение остатка от деления самого ключа на количество серверов:


  ```
f(key) = key % count_servers
  ```

- Для строковых ключей – нахождения остатка от деления числового представления ключа, полученного с помощью хеш-функции:


```
f(key) = crc(32) % count_servers
```

В качестве хеш-функции выбирают CRC32, т.к. считается быстро и не требуется криптостойкости. 

<u>Преимущество:</u>

- равномерное распределение данных по серверам


<u>Недостатки:</u>

- при решардинге (изменении количества серверов) значительная часть кеша теряется.


#### Консистентное кеширование

Консистентное кеширование (consistent hashing) – это специальный тип *sharding function*. 

В алгоритме консистентного кеширования добавляется промежуточное отображение, *sharding function* состоит сразу из двух функций:

- функции отображения ключей на слоты, количество которых намного больше количества серверов (например, 100-200 слотов на сервер). 


- функции отображения слотов на физические сервера.

При изменении количества серверов количество слотов не меняется. Изменяется только отображение задаваемое функцией отображения слотов на физические сервера:

- если один из серверов выходит из строя, то все слоты, которые к нему относились, распределяются между оставшимися;


- если добавляется новый сервер, то ему передаётся часть слотов от уже имеющихся серверов.

Преимущество:

- Позволяет гибко!!! управлять распределением слотов по шардам, путем задания параметров для формулы *consistent hashing*
- Упрощается решардинг. Добавление или удаление сервера влечет за собой перераспределение только части ключей (k / N, где k — общее количество ключей, а N — количество серверов). 


Обычно идею согласованного хеширования визуализируют с помощью числовой оси, «закрученной» в кольцо. Если хеш-функция может принимать целые значения от 0 до 2^32, то на кольце склеиваем 0 и 2^32. Каждому сервера из пула сопоставляем точку на кольце. Точки показывают границы диапазонов слотов. В качестве сервера для хранения ключа выбираем сервер в точке, ближайшей к точке ключа в направлении по часовой стрелке. Если сервер удаляется из пула или добавляется в пул, на оси появляется или исчезает точка сервера, в результате чего лишь часть ключей перемещается на другой сервер. В реальности, т.к. число слотов гораздо больше числа серверов, одному серверу ставится в соответствие 100-200 точек на оси, что улучшает равномерность распределения ключей по серверам в случае изменения их конфигурации. Слоты одного сервера могут быть расположены на числовой оси не подряд, в непрерывном диапазоне, а перемежаться со слотами других серверов.

- 


### Динамическое (недетерминированное, table function)

Описание порядка секционирования хранится отдельно в виде отображения ключа секционирования на номер секции (*table function*). Примером может служить таблица с двумя столбцами (ее называют *config*): идентификатор пользователя и идентификатор секции:

```mysql
CREATE TABLE user_to_shard ( 
   user_id INT NOT NULL, 
   shard_id INT NOT NULL, 
   PRIMARY KEY (user_id) 
); 
```

Функцией разбиения служит сама таблица. 

Недостатки:

- накладные расходы, так как требуется обращение к внешнему ресурсу, например серверу каталогов (узлу, на котором хранится отображение). Ускорить доступ к таблице разбиения может распределенная система кэширования (*memcached*) или даже просто хранить эту карту в коде (если она не меняется часто).

Преимущества:

- более точное управление местом хранения данных. Это упрощает равномерное разделение данных по секциям и позволяет гибко адаптироваться к непредвиденным изменениям. Например, если имеют место запросы к списку постов одной категории, можно помещать эту категорию в одну секцию. Можно  создавать несбалансированные секции, если мощность серверов разная, или на некоторых серверах имеется дополнительная нагрузка. Также удобно если степень теплоты секций разная, т.к. иначе холодные секции будут простаивать и вычислительные ресурсы используются неэффективно. Можно перебалансировать секции в любой момент, делать секции нужного размера (типа badoo). 


- Динамическое распределение и применение правила близости секций может предотвратить рост межсекционных запросов по мере масштабирования. В случае шардирования в *Memcached* можно выбрать несколько ключей за один *multi_get* запрос к серверу. 

### Использование виртуальных шардов (vshard, бакетов, vbuckets, спотов, spot)

Имеет смысл сделать размер секции значительно меньше, чем емкость узла, так чтобы в одном узле можно было хранить несколько секций. Секции меньшего размера удобнее перемещать, что упрощает задачу перераспределения емкости и поиска оптимального баланса между секциями и узлами. Перемещение данных между секциями обычно намного сложнее, чем перемещение самих секций. Упрощается резервное копирование и восстановление информации, а кроме того, для небольших таб лиц быстрее завершаются такие действия, как изменение схемы. При перемещении секции необходимо ее перевести в режим чтения, выгрузить данные и переместить их в другой узел. Размер секции должен быть таков, чтобы для регулярного выполнения задач обслуживания (`ALTER TABLE`, `CHECK TABLE`, `OPTIMIZE TABLE`) достаточно было 5–10 минут.

Означает, что в отображении key→shard появляется промежуточное отображение `key → vbucket → shard`. Т.е. ключ отображается на виртуальный *bucket* (*vbucket*), а виртуальный *bucket* – на физический шард (реальный бакет, *bucket*). Виртуальные – т.к. не прикреплены к реальному физическому серверу. 

Число *vbucket* определяется заранее, берется достаточно большим и обычно является степенью двойки (например, в *Couchbase* по умолчанию 1024 *vbucket*’а)

Существует два варианта отображения:

1. `key → vbucket` с помощью *sharding function*, `vbucket  → shard` с помощью *table function*

2. `key → vbucket` с помощью *table function*, `vbucket → shard` с помощью *table function*. Используется если нужно перемещать данные между vbucket, например, пользователя в другой дата-центр.

Отображение `vbucket → shard` называется словарем

Преимущества:

- В 1 варианте позволяет снизить объем *table function*, при этом оставив возможность управления распределением.


- Имея большое количество *vbuckets* можно упростить решардинг.


Недостаток:

- Более сложный поиск *shard* по известному `key`


### Явное

Cпособ выбора секции при создании строки. Номер секции кодируется в ее идентификаторе. Например, так:

```
первичный_ключ = номер_секции<<56 + номер_записи
```

Преимущество:

- идентификатор каждого объекта уже несет в себе собственный ключ секционирования, тогда как остальные решения подразумевают соединение таблиц или иную операцию поиска для нахождения такого ключа. Можно также для соответствия 1НФ (только одно значение в одном атрибуте) хранить ключ секционирования в отдельном столбце. Можно скомбинировать с динамическим распределением – в строке указывается не номер секции, а номер некоторый группы (ячейка), а уже в какую секцию отражается эта группа (ячейка) определеяется динамическим методом. Это позволяет удобно управлять взаимосвязанными данными.

## Способы именования в шардах

- Размещать на каждом узле одну базу данных и называть все эти базы одинаково. Каждая секция повторяла структуру исходной базы данных.


- Использовать одну базу данных на каждую секцию и включать в включать номер секции в имя каждой таблицы (`comments_1`, `messages_1`).


- Использовать одну базу данных на каждую секцию и включать в эту базу все таблицы. Номер секции тогда является частью имени базы, а не таблицы (`fishki_23.comments`, `fishki_23.messages`).


- Использовать одну базу данных на каждую секцию и включать номер секции как в имя базы, так и в имена таблиц (`fishki_1.comments_1`).


Более удобен последний вариант, т.к. секция целиком находится в одной базе данных и ее легко перемещать. При этом имена таблиц глобально уникальны.

## Роутинг запросов

Роутинг – механизм поиска шардов в кластере по ключу.

### Умный клиент (smart client)

Table function (config) зашито в ваш клиент

  

Преимущества:

·  Простота, скорость

Недостатки:

·  Решардинг очень сложен, т.к. table function внутри работающих приложений, которые надо завершить и обновить

·  Table function дублируется для всех клиентов на разных языках программирования

### Прокси (proxy)

Логика роутинга выносится в специальный прокси, который скрывает особенности работы с шард-кластером, сам взаимодействует с шардами. Клиент думает, что работает с одной нодой, отправляет туда запросы и получает оттуда данные.

  

Преимущества:

·  Удобство работы с одной нодой для клиента

·  Управление шардингом (решардинг, перебалансировка) в одной точке

Недостатки:

·  удвоение трафика внутри площадки, можно избежать этого разместив прокси на том же хосте, на котором работает само приложение

·  прокси – точка отказа

### Координатор (coordinator)

Координатор – это место хранения логики функции шардирования. Координатор по ключу шардирования возвращает номер шарда. После этого клиент сам устанавливает соединение с нужной нодой.

  

Преимущества:

·  «Координатор» гораздо проще «прокси»: в самом простом виде – высокопроизводительный сервер с базой данных или in-memory БД, или простой скрипт.

·  Т.к. координатор прост, его легко резервировать

### Роутинг внутри БД (intra-database rounting)

Используется в больших базах данных (100 тысяч узлов и более), в которых отсутствует возможность хранить функцию хеширования в одном месте. Роутинг реализуется методом случайного поиска в облаке нод. 

  

Алгоритм:

·  Выбирается случайная нода, на которую адресуется запрос с ключом шардирования

·  Если это ключ данной ноды или ее соседей, о которых она знает, то в ответ на запрос возвращаются данные. Т.е. сама БД выступает как прокси.

·  Если нет, то запрос форвардится на случайную node. 

Процесс сходится за логарифмическое время. Реализован в Redis . 

## Перебалансировка

Перебалансировка – перенесение vbuckets на другие физические node. Требует изменение словаря (т.е. отображения vbuckets -> node). 

Варианты организации:

·  Только для чтения. Проще всего перевести переносимый vbucket перевести в режим только для чтения. Перенос можно выполнить во время малой активности пользователей и при выполнении изменений выдать сообщение о maintenance. Если vbucket’ы достаточно маленькие, перенос займет малое количество времени.

·  Все данные неизменяемые. Полностью изменить архитектуру системы – все изменения данных осуществлять путем INSERT в виде лога событий. Если что-то надо изменить, то вместо UPDATE в таблицу лога изменений вставляется новое значение, если удалить – вместо DELETE вставить метку, что данных больше нет. При таком подходе при перебалансировке данные пишутся в vbucketsTo, а читаются из vbucketsFrom. Данные также удобно версионировать. 

·  Реплицировать vbucket. До полной синхронизации все операции выполняются с vbucketFrom, затем переключаемся на vbucketTo. Это наиболее универсальное решение. 

## Решардинг

Решардинг – изменение схемы шардирования и ее параметров (в том числе изменение числа vbuckets/vnode).

Лучший вариант – не поддерживать решардинг и заранее выбирать достаточно большее количество vbuckets. 

При нарастании нагрузки необходимо делить один шард на два, распределяя по ним данные. При этом удваиваться постоянно очень дорого, гораздо удобней использовать схему с виртуальными шардами.  Процесс переноса vbuckets выполняетсяпростыми операциями: дампами, репликацией. 

Проблемы решардинга:

·  существенная нагрузка на сеть и на node

·  чаще всего требуется временно отключить часть фукционала. При этом клиенты не должны этого заметить.

·  В процессе решардинга копии данные временно находятся на двух узлах. 

Лучшие схемы решардинга – которые выполняются автоматически. 

### Схемы решардинга

#### Автоматический решардинг

##### Перемещение при обновлении (Update is a move)

При изменении значения ключа он переносится с одного шарда на другой. После того как все данные после обновления переехали на новые шарды, старые шарды можно вывести из строя.

##### Протухание данных (data expiration)

Может быть использована в случае, если допустимо удаление старых данных. Аналогично, как в memcached, протухшие данные можно просто удалить, а заново сгенерированные данные помещаются в старые или новые шарды. 

##### Новые данные на выбранные сервера (new data on selected servers)

При заполненности шард данными добавляются новые сервера и некоторое время новые пользователи льются только на них. Когда секция достаточно заполнена, можно выставить флаг, который говорит приложению, что больше туда не нужно добавлять информацию. После этого открывается регистрация на старые сервера (возможно все даже в ручном режиме). Если по прошествии времени узел окажется недогруженным, то флаг можно сбросить. В Badoo считают теплоту секции и сопоставляют по бенчмаркам ее с производительностью сервера. Если секция слишком нагрелась, то в нее перестают писать данные. По мере охлаждения секции и снижения активности пользователей, запись в нее снова может быть открыта.

#### Ручной решардинг

##### Деление vbucket на несколько

Например, удвоить количество vbucket можно разделив каждый vbucket на два. Например, если у нас изначально один vbucket, изменяем отображение key -> vbucket так, чтобы половина данных указывала на тот же первый vbucket, а половина – на новый второй vbucket. В отображение vbucket -> node нужно изначально указать, что оба vbucket лежат на исходной node.

Если отображение key -> vbucket задано через sharding function:

  

то его можно изменить на 

  

При этом также половина ключей будут соответствовать все тому же номеру бакета, а половина переедет в новые vbuckets. В этом случае новый словарь с отображением vbucket -> node легко получить присоединив с конца его копию:

$dictionary = $dictionary+$dictionary

После удвоения vbuckets их можно переносить между node, т.е. выполнять перебалансировку

Уменьшение числа vbucket’ов происходит в обратном порядке — сначала перенести vbucket на node к другому vbucket попарно, а потом соединить пару в один vbucket путем обновления словаря. 

Если единицами данных являются базы данных, то при увеличении количества vbucket необходимо решить проблему деления каждой базы на две. 

## Распределенные транзакции

При правильно выбранной схеме шардирования распределенные транзакции должны выполняться редко, поскольку они всегда недешевы. 

Для одной базы данных проблема консистентности решается через транзакции, которые поддерживаются СУБД. Распределенные транзакции не могут быть выполнены легко, с сохранением свойств ACID. В результате база данных может перейти в некосистентное состояние по причине

### Предполагать, что сбои невозможны

Часто можно смириться с некоторой неконсистентью данных в случае некоторых временных сбоев. 

Преимущество: простота. Недостаток: недовольство пользователей некорректностью данных.

### Двухфазный коммит, ХА-транзакции

Распределенные (XA) транзакции позволяют распространить некоторые ACID свойства вовне подсистемы хранения и даже вовне базы данных посредством механизма двухфазной фиксации (Two-phase commit protocol, 2PC).

В MySQL существует два вида XA-транзакций.

**Внутренние XA-транзакции.** Транзакции, выполняемые в таблицах различных подсистем хранения. В роли еоординатора выступает сервер MySQL.

**Внешние XA-транзакции**. Сервер MySQL может быть участником распределенной транзакции, но не может ей управлять. 

Архитектура системы внешних распределенных транзакций включает:

·  Несколько менеджеров ресурсов (Resource Manager, RM) – распределенные экземпляры MySQL с возможностью локальных транзакций.

·  Один координатор транзакций (Transaction Manager, TM) – координирует распределенную транзакцию, взаимодействуя с RM, которые обрабатывают ветви (branches) распределенной транзакции. Это клиентская программа, которая подключается к RM. 

**Недостаток 2****PC:** 2PC protocol является блокирующим. Кроме того, строгие гарантии, которые он предоставляет, зачастую не нужны. Например, если допустить что пользователи не могут видеть состояние счета других пользователей, то нет ничего страшного, что данные на один счет зачисляться немного позже, чем спишутся с другого. 

#### Процесс коммита

Фаза 1. Координатор транзакции просит всех RM подготовиться к коммиту. Каждый RM должен сохранить требуемые от его ветви действия в надежное хранилище. RM сигнализирует TM о готовности ко 2 фазе.

Фаза 2. TM сообщает всем RM требуется совершить commit или rollback. Если все RM сигнализируют о готовности к commit, то все ветви коммитятся. Если какая-то ветвь сигонализирует о невозможности commit, то все ветви откатываются. 

Команды начинаются с ключевого слова XA, и во многих из них требуется значение xid – идентификатор транзакции. Значения xid генерируются TM (приложением) и должны быть глобально уникальными. Все команды транзакции должны содержать одинаковое значение xid.

Команда XA RECOVER возвращает список XA транзакций на сервере в состоянии PREPARED (gtrid – глобальный идентификатор транзакции, bqual – идентификатор ветви (branch qualifier). Для примера программы ниже:

mysql> XA RECOVER;

+----------+--------------+--------------+----------------------------+

| formatID | gtrid_length | bqual_length | data            |

+----------+--------------+--------------+----------------------------+

|    1 |      26 |      0 | tx-5abb8001d61997.65176918 |

+----------+--------------+--------------+----------------------------+

#### Состояния XA транзакции

\1.  Старт транзакции в состоянии ACTIVE:

XA START 'xatest';

\2. Исполняются команды SQL,составляющие транзакцию. В конце исполняется команда XA END, которая помещает транзакцию в состояние IDLE.

INSERT INTO tabl (value) VALUES (1);

XA END 'xatest';

\3. Для транзакции в состоянии IDLE возможны вариант:

o вызвать XA PREPARE и поместить транзакцию в состояние PREPARED. Команда XA RECOVER будет выводить значение xid транзакции в своем выходе.

XA PREPARE 'xatest';

o вызвать XA COMMIT ... ONE PHASE, которая за одну фазу делает prepared и сразу commit. Транзакция в этом случае завершается.

\4. Для транзакции в состоянии PREPARED исполняется команда XA COMMIT или XA ROLLBACK.

XA COMMIT 'xatest';

#### Пример реализации

**function** wait() {
   **echo** **"Sleep. Press enter...****\n****"**;
   gets(***STDIN\***);
 }
 */\* Соединение с Resource Manager \*/
\* **class** MysqlConnect
 {

   **protected** **$link** = **null**;

   **public function** __construct($host, $database, $username, $password)
   {
     $this->**link** = **new** mysqli($host, $username, $password, $database);
   }

   **public function** query($query)
   {
     $this->**link**->query($query) **or die**(**"ERROR: "** . $this->**link**->**error**);
   }


 }

 */\* Класс с логикой Transaction Manager \*/
\* **class** TransactionManager
 {
   **protected** **$connections** = **array**();

   **protected** **$lastTxId** = **null**;

   **public function** __construct($connections)
   {
     $this->**connections** = $connections;
   }

   **public function** query($query)
   {
     **foreach** ($this->**connections** **as** $connection) {
       $connection->query($query);
     }
   }

   **protected function** getTxIdent()
   {
     **return** *uniqid*(**"tx-"**, **true**);
   }

   **public function** txBegin()
   {
     $tid = $this->getTxIdent();
     $this->**lastTxId** = $tid;
     **foreach** ($this->**connections** **as** $connection) {
       $connection->query(**"XA START '**{$tid}**'"**);
     }
   }

   **public function** txCommit()
   {
     $tid = $this->**lastTxId**;
     **foreach** ($this->**connections** **as** $connection) {
       $connection->query(**"XA END '**{$tid}**'"**);
       $connection->query(**"XA PREPARE '**{$tid}**'"**);
     }

     wait();
     
     **foreach** ($this->**connections** **as** $connection) {
       $connection->query(**"XA COMMIT '**{$tid}**'"**);
     }
   }

   **public function** txRollback()
   {
     $tid = $this->**lastTxId**;
     **foreach** ($this->**connections** **as** $connection) {
       $connection->query(**"XA END '**{$tid}**'"**);
       $connection->query(**"XA RALLBACK '**{$tid}**'"**);
     }
   }

 }

 $c1 = **new** MysqlConnect(**"localhost"**, **"test"**, **"root"**, **"root"**);
 $c2 = **new** MysqlConnect(**"localhost"**, **"test"**, **"root"**, **"root"**);

 $manager = **new** TransactionManager([$c1, $c2]);
 $manager->txBegin();

 $c1->query(**"INSERT INTO tabl (value) VALUES (1)"**);
 $c2->query(**"INSERT INTO tabl (value) VALUES (2)"**);

 $manager->txCommit();

### Автоматическая проверка согласованности

Можно подготовить сценарий, который будет периодически запускаться и проверять согласованность информации в разных секциях. 

### Критическая секция и мьютекс в распределенной памяти

Организовать критическую секцию для монопольного доступа к изменяемым данным с целью изоляции транзакции и отката ее в случае неудачи. Распределенная синхронизация может быть организована через мьютекс в распределенной памяти, например, в *memcached* через атомарную операцию `add` (аналог *compare and set, CAS*). 

Недостаток: плохо масштабируется.

### Согласованные в конечном счете транзакции (eventually consistent)

Подробнее [eventually consistent]()

#### 1 пример

В системе ведется учет денег пользователей на счетах, для этого имеется таблица `account`:

```mysql
CREATE  TABLE `account` (
  `account_id` INT NOT NULL AUTO_INCREMENT ,
  `balance` DECIMAL(19,2) NOT NULL ,
PRIMARY KEY (`account_id`) );
```

Шардинг выполнен по `account_id`. Счета распределены по нескольким физически распределенным *nod*'ам. Перевод денег между счетами пользователей требует организации распределенной транзакции.

Пусть требуется перевести 10 рублей со счета `accountA` на счет `accountB`, хранящиеся на шардах `SA` и `SB` соответственно. Для незаметности временного рассогласования данных на двух шардах желательно, чтобы каждый пользователь читал данные только из одного шарда.

##### Для идемпотентных и коммутативных операций

Операция 

```mysql
UPDATE account SET balance = balance + 10
```

не является идемпотентной.

Ее можно преобразовать к идемпотентному виду следующими способами:

Xранить в каждом шарде лог транзакций в виде таблицы:

```mysql
CREATE TABLE `transaction_log` (
  `transaction_log_id` INT NOT NULL AUTO_INCREMENT,
  `transaction_id` INT NOT NULL,   -- Что данная транзакция уже применялась
  `account` INT NOT NULL,          -- Account к которому применяется
  `value` decimal(19,2) DEFAULT NULL,  -- на сколько изменить   
  PRIMARY KEY (`transaction_log_id`),
  UNIQUE INDEX `transaction_id_unique` (`transaction_id` ASC) -- Уникальный индекс
); 
```

При каждом обновлении значения, делать по этой таблице  `SELECT sum(value) FROM transactions_log…` и полученное значение обновлять в таблице `account` через `UPDATE account SET balance = concrete_balance ...`.

Либо вообще не хранить значение `balance` в таблице `account` и получает его подсчетом из таблицы `transaction_log`  (подход с неизменяемыми данными из [Перебалансировка](#Перебалансировка)). 

Коммутативность: для примера с балансом – обработка коммутативная. Неважно в каком порядке изменять `balance`, от перемены мест слагаемых ничего не меняется.

Пример некоммутативности: событие – установить пользователю значение возраст `user.age=XXX`. Если порядок событий меняется, то вместо последнего значения возраста, установленного в одном *spot*'е, у пользователя может быть установлено предпоследнее значение возраста. Данные неконсистентны. Преобразование к коммутативному виду: вместо указания в событии конкретного `age`, которое надо установить, нужно кидать событие, что `age` у `user_id` изменился. *Spot* при получении такого события должен залезть в spot-первоисточник и проверить текущее значение `age`. В результате по получении такого события `age` всегда будет соответствовать `age` в другом *spot*'е.

Алгоритм следующий:

1. Создать единую для всех таблицу *transaction*:

   ```mysql
   CREATE TABLE `transaction` (
     `transaction_id` int(11) NOT NULL AUTO_INCREMENT,
     `state` enum('created','committed','failed') NOT NULL -- Состояние транзакции,
     `accountA` int(11) NOT NULL                           -- Account откуда,
     `accountB` int(11) NOT NULL                           -- Account куда,
     `value` decimal(19,2) DEFAULT NULL                    -- Сколько перевести,
     PRIMARY KEY (`transaction_id`)
   ) ENGINE=InnoDB DEFAULT CHARSET=utf8
   
   ```

   При старте транзакции в таблицу заносим полную информацию о транзакции: откуда и куда нужно перевести, сколько перевести и ставим `state=created`. Эта таблица может использоваться для переноса задач в очередь сообщений (описано ниже).

2. Каждый шард добавляем таблицу с логом примененных транзакций

   ```mysql
   CREATE  TABLE `transaction_log` (
     `transaction_log_id` INT NOT NULL AUTO_INCREMENT ,
     `transaction_id` INT NOT NULL ,      -- Что данная транзакция уже применялась
     PRIMARY KEY (`transaction_log_id`) ,
     UNIQUE INDEX `transaction_id_unique` (`transaction_id` ASC) -- Уникальный индекс
   ); 
   
   ```

   В шарде *S<sub>A</sub>* запускаем локальную транзакцию (все изменения в одной транзакции!!!), в рамках которой выполняем:

   - Вставляем в лог транзакций запись, о применяемой транзакции. Если эта транзакция уже применялась, то будет нарушение уникального индекса и транзакция откатится без применения:

     ```mysql
     INSERT INTO `transaction_log` VALUES (:transaction_id)
     ```

   - Выполняем изменение баланса в таблице `account`

   - Фиксируем локальную транзакцию

3. Аналогично в шарде *S<sub>B</sub>* выполняем локальную транзакцию, со вставкой в таблицу `transaction_log` и изменением баланса таблице `account`.

4. Меняем статус транзакции на `commited`:

   ```mysql
   UPDATE transaction SET state='commited' WHERE transaction_id = :transaction_id
   ```

   

Шаги (2) и (3) могут выполняться параллельно. Если выполнение кода прервется на шаге (2), (3) или (4), «транзакцию» должен докатить специальный служебный процесс. Это возможно по той причине, что операции (2) и (3) идемпотентны — их повторное выполнение приводит к тому же результату. Если пользователь читает данные только из одного бакета, с его точки зрения данные всегда консистентны (нет временной неконсистентности).

Вместо служебного процесса можно использовать асинхронную обработку в *Message broker*, особенно в случае если шаги 2-4 невозможно выполнить из-за временной недоступности шарда *S<sub>B</sub>*. Для этого в очередь помещаются номера транзакций, ожидающих выполнения, шаги 2-4 выполняет асинхронно *consumer*.

Если операцию невозможно докатить из-за недоступности одного из шардов, служебный процесс или *consumer* из *Message broker* должен откатить все выполненные на шардах изменения (или сделать «сторно») и в таблице `transaction` указать статус транзакции `failed`. 

В итоге система в незначительное время будет находиться в неконсистентном состоянии, что является платой за неиспользование блокирующих механизмов. Можно в таблице `account` сделать пометку, что «эти данные станут видны в момент времени `time() + T`».

##### Без требования идемпотентности

В соответствии с  алгоритмом из документации Mongo, CouchDB… (подробное описание http://rystsov.info/2012/09/01/cas.html). 

Отличия от предыдущего:

- не требует идемпотентности операций, т.к. транзакция фиксируется атомарно


- в случае сбоя докатка и откат транзакции выполняет не служебным процессом, а в процессе чтения данных


Необходимо создать таблицу транзакций:

```mysql
CREATE TABLE `transaction` (
  `transaction_id` int(11) NOT NULL AUTO_INCREMENT,
  `state` enum('created','committed','failed') NOT NULL,-- Состояние транзакции
  `version` int(11) NOT NULL,                           -- Версия транзакции
  PRIMARY KEY (`transaction_id`)
)
```

Изменить в каждом шарде структуру таблицы `account`:

```mysql
CREATE TABLE `account` (
  `account_id` int(11) NOT NULL AUTO_INCREMENT,
  `balance` decimal(15,2) NOT NULL,
  `updated` decimal(15,2) DEFAULT NULL,
  `transaction_id` int(11) DEFAULT NULL,
  `version` int(11) NOT NULL,
  PRIMARY KEY (`account_id`)
)
```

Во время транзакции поле `updated` содержит добавляемое к `balance` значение, поле `transaction_id` – *id* транзакции, по которой выполняется изменение.

В каждую таблицу добавлено поле `version`, для того чтобы в промежутке между чтением и записью в таблицу не возникло race condition. После чтения строки из таблицы значение `version` сохраняется в скрипте и при `UPDATE version` указывается в условии `WHERE` и одновременно инкрементируется. Если `version` была изменена, то `UPDATE` не изменит содержимое строки, о чем будет сигнализировать значение количества обработанных строк (функция `PDOStatement::rowCount`).

Возможны 3 состояния строки в таблице `account`:

1. *clean* (`updated IS NULL AND transaction_id IS NULL`) строка только создана или операция завершилась.

2. *dirty uncommitted* (`updated IS NOT NULL AND transaction_id IS NOT NULL`, и в таблице `transaction` состояние `state='created'`) – возникли проблемы, изменения необходимо докатить (или откатить).

3. *dirty committed* (`updated IS NOT NULL AND transaction_id IS NOT NULL`, и в таблице `transaction` состояние `state=’committed’`) – транзакция зафиксирована, но служебные данные не очищены

Алгоритм выполнения транзакции:

1. `INSERT` в таблицу `transaction` со значением `state='created'`

2. `UPDATE` в таблицу `account` для счета `accountA` с указанием `updated=-$value`, `transaction_id=$transaction_id`

3. `UPDATE` в таблицу `account` для счета `accountB` с указанием `updated=$value`, `transaction_id=$transaction_id`

4. `UPDATE` в таблицу `transaction` c указанием `state='committed'`

5. `UPDATE` в таблицах `account` для счетов `accountA` и `accountB` в шардах *S<sub>A</sub>* и *S<sub>B</sub>* с указанием `balance = balance + updated`, `updated = null`, `operation_id=null`

6. `DELETE` в таблице `transaction`

Шаги 1-3 подготавливают операцию, указывают в поле `updated` будущие изменения в случае успеха операции.

Шаг 4 – ключевой, на этом атомарном шаге операция переводится в состояние `commited` и все изменения считаются примененными.

Шаги 5-6 выполняются уже после подтверждения изменений и применяют локальные изменения к `account`, а затем транзакция удаляется. Шаги 5-6 могут быть также выполнены в момент чтения, как описано ниже.

Алгоритм чтения из таблицы `account`:

1. Чтение строки из таблицы

2. Проверить состояние строки

   - *clean* – строка корректна и дополнительных действий не требуется

   - *dirty committed* – требуется выполнить шаги 5-6 предыдущего алгоритма, чтобы применить локальные изменения к `account` и `transaction`

   - *dirty uncommitted* – возникла проблема, транзакцию нужно откатить (либо просигнализировать о проблеме и поместить в очередь задачу для попытки накатить повторно). 

     Порядок отката транзакции:

     1. `UPDATE` в таблице `transaction` с указанием `state='failed'`
     2. `UPDATE` в таблицах `account` для счетов `accountA` и `accountB` в шардах *S<sub>A</sub>* и *S<sub>B</sub>* с указанием `updated = null`, `operation_id = null`
     3. Строка в итоге находится в состоянии до выполнения операции

Недостаток этого алгоритма – необходимость организовать специальную процедуру чтения данных из таблиц `users` и `transactions`. Алгоритм чтения должен учитывать в каком из 3 состояний находится запись в таблице.

Указанные шаги алгоритма могут выполняться асинхронно с использованием *Message broker*. В этом случае в алгоритм добавляются шаги по чтению сообщения из очереди и, по завершении, удалению сообщения из очереди. При падении скрипта после создания операции (1 шаг) и до ее фиксации (5 шаг) строки останутся в состоянии *dirty uncommitted*, изменения будут откачены, а сообщение останется в очереди и операция будет создана повторно.

#### 2 пример

Из badoo2020