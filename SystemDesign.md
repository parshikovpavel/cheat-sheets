**Взять ручку и листок!!!!**

Материал:

- Grokking the system design interview (https://www.educative.io/courses/grokking-the-system-design-interview/m2yDVZnQ8lG, есть копия в папке tricks
- Высоконагруженные приложения. Программирование, масштабирование, поддержка | Клеппман Мартин
- https://leetcode.com/discuss/general-discussion/125038/system-design-interview-prep

- https://engineering.videoblocks.com/web-architecture-101-a3224e126947

# Общее

Проектирование системы состоит из шагов.

## Выяснение требований

Вопросы о дизайне открытые, и у них нет ОДНОГО правильного ответа. 

Поэтому важно задавать вопросы, чтобы:

- найти точный масштаб проблемы.
- понять на каких частях системы сосредоточиться 

### Нагрузка

- Сколько пользователей будут использовать систему?
- Количество read/write на пользователя?
- Количество read/write на систему (получается, это (1) * (2))
  - нужно уточнение. RPS не равномерно распределяется в течении дня ([1](#метрики-для-rps))

### Функциональные требования

**Посмотреть функции на fishki!!!**

Проговорить словами, что будет делать система!!!

- что размещать на сервисе (твиты, посты, комментарии, тексты)?
- что получают в ответ от сервиса (URL адрес)
- что могут делать пользователи на сервисе (create, read)
- что происходит в момент *create* и *read*
- необходимо ли размещать большой контент или медиа в *entity* (большой текст, фото, видео)
- необходимо ли отдельно загружать/скачивать медиа (фото, видео)
- ограничение размера *entity* (размера URL, размера текста)
- можно ли задавать `expiration_time`, удаляются ли entity автоматически через некоторое время.
- можно ли задавать `custom_entity_hash`, `custom_URL`
- пользователи
  - какие типы пользователей (usergroup), группы пользователей, 
  - ограничение доступа к entity для пользователей
  - как задается ограничение пользователей к entity (дискреционная система, мандатная, матрица доступа)
  - 
- личные сообщения
- комментарии
- suggestion
- подписки на контент или пользователей.
- теги
- необходимо ли разрабатывать интерфейс (???)
- требуются ли какие-то ленты с *entity*'s (лента новостей друзей, общие ленты новостей сайта )
- требуется ли поиск
  - полнотекстовый
  - по названиям
  - по тегам
- какие сортировки в лентах (hot, best, recent, trend)
- требуются ли блоки с *hot entity*'s (лучшие новости, топ пользователи)
- *notification* пользователей?
- как будем бороться со злоупотреблениями (антиспам, большое количество запросов от пользователя/IP)

Дополнительные требования:

- какие дополнительные настройки у *entity*:
  - выбор *custom ID* для *entity*
  - *expiration time* для *entity*

### Нефункциональные требования:

- требуемое время генерации страницы
- *durability*, почему и как нужно обеспечить
- *high availability*, почему нужно и как обеспечить
- непредсказуемость генерации *entity ID*
- соотношение consistency - availability

### Дополнительные требования

- Нужны ли сводные аналитические отчеты? Какие? Мониторинг?
- Доступ через REST API для сторонних сервисов



## Определение API системы

Позволяет:

- установить точный контракт, ожидаемый от системы.
- более точно формулирования требования к системе (входные и выходные данные)

Входные данные определяются на основе раздела функциональные требования!!!:

- смотреть [Построение data model](#построение-data-model)

Выходные данные:

- для `create` – URL на *entity* (`string`)
- для `delete ` – статус `OK`, `Error`

Выходные данные для сервисов:

- разные коды HTTP:
  - 302 Redirect
  - 403 Forbidden — запрещен доступ из-за ограничений
  - 404 Not found
  - 

Типичные методы:

- `create`, `post`, 
- `read`, `get`
- `update`
- `delete`
- `generate` (сгенерировать ленту)
- `mark` (пометить как какой-то статус, *favorite*, *moderate*,..)
- `setPermission` (установить права на entity)

Особенности работы:

- как будем бороться со злоупотреблениями (антиспам, большое количество запросов от пользователя/IP)

## Оценка характеристик системы

### Типы ресурсов

- Процессор


- Память (RAM)


- Disk IO


- Сеть

### Оценка нагрузки

- Задаем соотношение `создание_entity : чтение_entity` (например, `1:100`)

- Задаем RPM (*Request per month*) для `создание_entity`

- Считаем RPM для `чтение_entity`:

  ```
  чтение_entity_в_месяц = создание_entity_в_месяц * (создание_entity : чтение_entity)
  ```

- Считаем RPS для `создание_entity` и `чтение_entity`:

  ```
  создание_entity_в_секунду = создание_entity_в месяц / (30 * 24 * 3600)
  чтение_entity_в_секунду = чтение_entity_в_месяц / (30 * 24 * 3600)
  ```

### Оценки Entity

Задаем количество лет `хранить_лет`, в течении которых будем хранить *entity*'s. 

Задаем `размер_entity`:

- Если нет внешних объектов (фото, тексты), то `размер_entity` считается по размеру записи в БД
- Если есть внешние объекты (фото, тексты), то учитываем их размер, нужно оценить средний размер *entity*

Общее количество entity:

```
общее_количество_entity = создание_entity_в_месяц * 12 месяцев * хранить_лет
```



Размер `entity_id`:

```
размер_entity_id = log (общее_количество_entity)
```







### Оценка storage

Считаем общий размер *storage*:

```
размер_storage = общее_количество_entity * размер_entity
```

### Оценка *network bandwidth* (пропускная способность сети)

- входящий трафик (*incoming traffic*)

  ```
  входящий_трафик = создание_entity_в_секунду * размер_entity
  ```

- исходящий трафик (*outgoing traffic*)

  ```
  исходящий_трафик = чтение_entity_в_секунду * размер_entity
  ```

### Оценка memory cache (кеша в памяти)

Согласно правилу 80-20, 20% *entity*'s генерируют 80% трафика, необходимо кэшировать эти 20% горячих entity's.

Размер *memory cache* для кэширования горячих *entity*'s в течении 24 часов:

```
размер_memory_cache = чтение_entity_в_день * размер_entity * 20%
```









 

- количество серверов
- как часто надо перегенерировать *secondary information* (блоки, ленты) 

- 

- 



## Построение data model

В MongoDB можно хранить `array` в документах!!!

На этом шаге нужно посчитать размер *Entity*. 

В БД хранится только метаинформация, большие данные (статика, тексты) выносятся в облако.

Типичные объекты:

- 

- общие
  - `id`, `hash`, `session_id` 
  - `date` или `timestamp ` (дата)
    - `creation_date`
    - `expiration_date` (время протухания)
  - `location` (местоположение)
    - `latitude` (широта)
    - `longitude` (долгота) 

- `User` 
  - `user_id`
  -  `name`
  -  `email`
  -  `registration_date`
  -  `last_activity`
  -  `birthday`
- `Entity` 
  - `entity_id`
  - `entity_hash`
  - `custom_entity_hash`,  `custom_URL`  (заданный пользователем)
  - `user_id`
  - `name` или `data` (контент)
  -  `content`
  - `path_to_file` 
  -   (для NoSQL) `[user_ids]` с разрешением на доступ
  - рейтинг
    - `likes ` (лайки)
    - `dislikes` (дизлайки)
    - `rates` (рейтинг)
- `UserFollow` (подписки)
- `Favorite`
- `Permissions` (имеющиеся разрешения)
  - `permission_id`
  - `permission_name`
- `User_permissions`  (разрешения пользователей на отдельные `entity`)
  - `user_id`
  - `permission_id`
  - `entity_id`
- `User_groups` (типы пользователей по правам)

### Выбор СУБД

Аргументы в пользу документно-ориентированной NoSQL (Mongo):

- хранятся большое количество строк
- между сущностями мало связей
- легко выполняется, встроенное шардирование
- можно вкладывать в документ массивы сущностей `{ user_photos: [ {....}, {...}] }`

Необходимо выбрать СУБД

- NoSQL
- RDBMS (MySQL, Postgres)

 Выбор *Storage* для хранения статики:

- HDD
- SDD

### Оценка storage под БД

Аналогично [Оценка storage](#оценка-storage)

- `unsigned int` – 4 байта (4,2 млрд), 8 байт
- `date`
  - `datetime` – 8 байт
  - `timestamp` – 4 байт

- `string` (1 байт или 1-3 байта на символ)
  - `name` – 20 байт
  - `email` – 32 байт
- вещественные
  - `float` (4 байт)
  - `double` (8 байт)
  - `decimal` – отдельно требуется место под целую и дробную часть, каждые 9 цифр - 4 байта. Все что больше 9 цифр, упаковывается в 1-4 байта. 

Отдельно считается каждая таблица

## Алгоритм работы

### Генерация внешнего `entity_hash`

Также [Генерация глобально уникальных идентификаторов](Sharding.md#генерация-глобально-уникальных-идентификаторов)

Варианты:

- использовать свойства Entity для получения `entity_hash` (например, `URL` или `name`). 

  Недостаток: предсказуемость `entity_hash`.

- генерировать `entity_hash` от случайных значений.

  Недостаток: возможные дубликаты ключей

- Использовать сервис генерации ключей (KGS), которая заранее генерирует множество `entity_hash` и сохраняет их в базе данных (назовем ее key-DB). Backend'ы берем один из уже сгенерированных *hash*'ей и используем его. А лучше берет сразу пачку *hash*'ей.

  Можно посчитать – сколько место будут занимать все заранее сгенерированные хеши:

  ```
  место_на_диске = количество_хешей * размер_хеша
  ```

  

  Преимущества: нет дубликатов, они проверяются заранее.

Для получения `entity_hash` необходимо рассчитать каким-то образом *hash*:

- MD5. Хеш содержит 128 бит (16 байт) и обычно представляется как последовательность из 32 шестнадцатеричных цифр

  ```
  MD5("md5") = 1BC29B36F623BA82AAF6724FD3B16718
  ```

  

- SHA256

А затем *binary hash* необходимо представить в виде набора отображаемых символов, которые допустимо использовать в URL. Например, с помощью:

- base64 ([AZ, az, 0-9] «-» «.»)  (1 символ - 6 бит)

На основе основания кодировки рассчитываем количество возможных хешей:

```
количество_хешей = 64 ^ количество_символов_в_хеше
```

Можно рассчитать нужное `количество_символов_в_хеше`

`количество_символов_в_хеше` = *log <sub>64</sub>* `общее_количество_entity`



## High-level design (стандартная архитектура)



Нужно нарисовать блок-схему из 5-6 блоков, представляющих основные компоненты системы.

При построении архитектуры важно рассмотреть различные подходы, их плюсы и минусы, понять trade-off (компромиссы).

Основные компоненты архитектуры.

### Backend (web application server)

*backend*'s (*web application servers*) для обработки *dynamic requests*'s. 

*backend*'s можно разделить на группы (отдельные на схеме), в зависимости от типа нагрузки:

- *write* (как у нас `adm1` для записи статики, сохранениям комментов) – т.к. загрузка это тяжелая операция и если поступает много операций write, то пользователи не смогу *read*
- *read*

### Load balancing

Где использовать балансировку:

- между клиентами и *application server*'ами
  - *Front Load Balancer*. Перед *backend*'s размещается несколько программных балансировщиков в виде *nginx* на отдельном узле. 
  - DNS-балансировка. Для балансировки нагрузки на программные балансировщики *nginx*.
- между *application server*'ами и *database server*'ами
- между *application server*'ами и *cache server*'ами

Алгоритмы балансировки:

- Round-robin

### Nginx


- *nginx* для проектирования запросов на *backend*'ы. *nginx* проксирует запросы к `*.php` на PHP-FPM (*Apache*). В итоге *Nginx* сам поддерживает большое количество `keep-alive` соединений небольшим количеством воркеров за счет эффективной архитектуры. До *Apache* долетает лишь малое количество очень коротких по времени проксированных запросов от nginx. Ответ *Apache* вычитывается в буфер *nginx* и отдается с требуемой клиентом скоростью.


- *nginx* для отдачи статики.

### Облако (статика, storage)

Варианты реализации:


- собственная система статики. Статика (картинки, видео) и динамика (php-скрипты) разносятся по разным серверам. Запросы к API-приложения, которые обрабатываются серверами динамики, небольшие по размеру и должны выполняться очень быстро. Запросы к статике, наоборот, тяжелые и для них не требуется значительная оперативность. Поэтому для них требуется разная конфигурация серверов по процессору и памяти. Если статика и динамика расположены на одном сервере, тяжелые запросы к статике начнут тормозить легкие оперативные запросы к API.
- Часто статика выносится в облако (Amazon S3), для того чтобы использовать готовую реализацию хранения и обработки статики.

Собственная система:

- *durability*, дублирование данных на разных серверах
- 

### CDN

TODO!!!

### База данных

База данных вынесена на отдельный сервер. 

Если есть внешние объекты (тексты, фото), то здесь хранятся *Entity Metadata* (так пишется на схеме)

Вопросы:

- каким образом масштабировать базу данных:
  - масштабировать не нужно, все влезает в одну БД. Не позволяет наращивать масштаб системы, имеет ограничение по RPS.
  - репликация. **В архитектуре replicas**. Выбираем для масштабирования по *read*. Не позволяет наращивать масштаб системы, имеет ограничение по RPS по write. Есть сервер с master DB и несколько slave DB, на которые осуществляется репликация.
  - шардирование
  - scale back? (*удаление неиспользуемых entity*'s) В архитектуре **Сервис переноса в архив**. Возможно хранить `expiration_time`. Смотреть [Scaling back](Highload.md#scaling-back)
- каким образом проиндексировать данные. Кластерный ключ определяет порядок хранения данных и возможность размещения горячих данных вместе.

#### Шардирование

Также [шардирование](Sharding.md)

Усложняет архитектуру системы.

Вопросы:

- выбор ключа шардирования – идентификатор важной сущности (чаще всего `user_id`), при этом этот обработка данных одного самого активного идентификатора (самого активного пользователя) должна влезать на сервер. 
- выбор функции шардирования:
  - фиксированное – плохо, неравномерно
  - *consistent hashing*  – лучше, позволяет гибко!!! управлять распределением слотов по шардам, путем задания параметров для формулы *consistent hashing*
  - динамическое – еще лучше, но нужно хранить где-то отображение
- дублирование данных в shard'ах
- что делать с горячими *user*'ами , которые много твитят или подписываются на множество людей? Использовать кеширование (смотреть [горячие фотографии](Highload.md#хранение-статики-в-своем-кластере), смотреть [горячие ключи](Highload.md#горячие-ключи), [расчёт степени теплоты секций](Sharding.md#новые-данные-на-выбранные-сервера-new-data-on-selected-servers)). Считать степень теплоты секции и когда она перегревается прекращать туда запись.

На основе [оценки storage под БД]() можно посчитать количество шардов:

```
количество_шард = размер_БД / размер шарда (лучше с запасом)
```



### Кеш

Какие виды кешей использовать.

- Система распределенного кеширования в памяти (memcached, redis) вынесена на отдельный сервер с большим количеством оперативной памяти.

  Современный сервер может иметь 256 ГБ памяти, количество требуемых серверов:

  ```
  количество_серверов = размер_memory_cache / 256 ГБ
  ```

Особенности:

- использовать *replication* для кешей
- использовать *sharding* через *consistent hashing*

#### Алгоритм вытеснения из кеша

Необходимо выбрать алгоритм вытеснения кеша (также [1](Algorithm.md#алгоритмы-вытеснения-из-кеша)). Все методы требуют использования структуры, для сбора статистики по использованию ключей.

- Least recently used (LRU, наименее недавно используемый) – в первую очередь вытесняются записи, неиспользованные дольше всех. 

- Most Recently Used (MRU, наиболее недавно использовавшийся): в первую очередь вытесняется последний использованный элемент. Полезны, когда чем старше элемент, тем больше обращений к нему происходит.


- Least-Frequently Used (LFU, наименее часто используемый) – подсчитывает, как часто используется элемент. Те элементы, обращения к которым происходят реже всего, вытесняются в первую очередь.

Кроме того можно учитывать: 

- Хранить дольше данные, которые дороже получать.


- Вытеснять быстрее записи большого размера, т.к. это позволит сохранить несколько записей поменьше. 

### Очереди


- Очереди для того, чтобы хранить фоновые тяжелые задачи (обработать видео, отправить сообщение). Для их поддержки нужны:

    - *Message broker*'s (часто Rabbit MQ, или просто БД)

    - *backend*'s, которые взаимодействуют с *message broker*'s и выполняют фоновые тяжелые задачи. Это часто группы backend's, каждая из которые выполняет некоторую специфичную задачу и соответствующим образом под нее сконфигурирована. Например, для обработки видео требуется быстрый CPU, а для отправки сообщений – быстрый *network*.

    *Message broker* и эти бекенды размещают на отдельных серверах. 

- Поисковый движок: Apache Solr, Sphinx, ElasticSearch

- набор сервисов (микросервисная архитектура). Они могут быть:


  - внешние (сторонние), например, отправка писем
  - внутренние (собственные), например на Go

- потоковая обработка данных (стримминг). Реализуется Apache Kafka. Собирает поток событий из системы (лайки, публикации, комментарии) и подготавливает их и агрегирует, чтобы дальше они могли быть удобно визуализированы. Например, сколько лайков было ставлено за последний час. То же самое можно сделать и с базой данный, но Kafka позволяет обрисовывать данные из скользящего окна за последний час/день (и т.д.) и не нагружает базу данных.

- хранилище данных – для сбора событий и построения сложных аналитических отчетов. Реализация – Hadoop. Он хранит огромное количество данных, мы запускаемм Map-Reduce и он агрегирует эти данные из хранилища.

## Выявление и устранение bottlenecks (узких мест)

Вопросы:

- Точки отказа (*point of failure*) среди компонентов архитектуры? Устраняются за счет избыточности компонентов.
  
  Необходимо дублировать все сервисы
  
  - служба генерации *hash*'ей
  
- Потеря данных устраняется за счет избыточного хранения данных.

  Необходимо сделать replica's (для БД, кешей) для всех данных, чтобы при выходе из строя одного из них выдержать нагрузку. 

## Мониторинг

- Мониторинг компонентов архитектуры
  - самописное в СУБД
  - ELK-стек –  для анализа логов
  - Prometheus + Grafana
  - Zabbix 

- оповещения когда критические компоненты выходят из строя или их производительность снижается.





Какие http коды должен отдавать сервис



нужно учитывать несколько вещей:

Какие разные архитектурные элементы можно использовать?

Как эти части работают друг с другом?

Как мы можем лучше всего использовать эти части, каковы правильные компромиссы?

Инвестирование в масштабирование до того, как это необходимо, обычно не является разумным бизнес-предложением; однако, некоторые заранее продуманные конструкции

Load Balancer (LB)
Павел
Павел 22:05

помогает распределить трафик по кластеру серверов, чтобы улучшить скорость отклика и доступность

. LB также отслеживает состояние всех ресурсов при распределении запросов. Если сервер недоступен для приема новых запросов или не отвечает или имеет повышенный уровень ошибок, LB прекратит отправку трафика на такой сервер.

Обычно балансировщик нагрузки находится между клиентом и сервером, принимающим входящий сетевой трафик и трафик приложений, и распределяющим трафик между несколькими внутренними серверами, используя различные алгоритмы. Балансируя запросы приложений на нескольких серверах, балансировщик нагрузки снижает нагрузку на отдельный сервер и предотвращает превращение одного сервера приложений в единую точку отказа, что повышает общую доступность и скорость отклика приложений.

Мы можем добавить LB в трех местах:

Между пользователем и веб-сервером

Между веб-серверами и внутренним уровнем платформы, таким как серверы приложений или серверы кэширования

Между внутренним уровнем платформы и базой данных.

Алгоритмы балансировки нагрузки

? Балансировщики нагрузки учитывают два фактора перед отправкой запроса на внутренний сервер. Сначала они гарантируют, что сервер, который они могут выбрать, действительно отвечают соответствующим образом на запросы, а затем используют предварительно настроенный алгоритм, чтобы выбрать один из набора исправных серверов.
29 июля
Павел
Павел 21:18

методов балансировки нагрузки

Метод наименьшего соединения - этот метод направляет трафик на сервер с наименьшим количеством активных соединени

Метод наименьшего времени отклика

ыбирает сервер, который в настоящее время обслуживает наименьший объем трафика

Метод Round Robin

Метод взвешенного циклического перебора -

Каждому серверу присваивается вес,

IP Hash - этот метод вычисляет хэш IP-адреса клиента, чтобы определить, какой сервер получает запрос.

Избыточные балансировщики нагрузки

Балансировщик нагрузки может быть единственной точкой отказа, для преодоления этого второй балансировщик нагрузки может быть подключен к первому

способо

Разработчики могут добавить алгоритм балансировки нагрузки в приложение или клиент базы данных

2. Аппаратные балансировщики нагрузки
. Самым дорогим,

3. Программные балансировщики нагрузки

HAProxy
Павел
Павел 21:47

HAProxy может работать на промежуточном сервере.

HAProxy может работать на той же машин

Очереди

требуются различные компоненты системы для асинхронной работы; общий способ сделать это с помощью очередей.

все входящие задачи добавляются в очередь, и, как только у любого работника появляется возможность обрабатывать, он может выбрать задачу из очереди.

Очереди реализованы по протоколу асинхронной связи.
Павел
Павел 22:09

Очереди также используются для обеспечения отказоустойчивости, поскольку они могут обеспечить некоторую защиту от перебоев в работе и отказов. Например, мы можем создать очень надежную очередь, которая может повторять запросы на обслуживание, которые не были выполнены из-за временных сбоев системы

RabbitMQ, ZeroMQ, ActiveMQ и BeanstalkD.
Павел
Павел 22:16

Нужно выяснить нагрузку



Основное различие между дизайнерскими собеседованиями и остальными заключается в том, что вам не дают полную информацию о проблеме, скорее вам необходимо масштабировать широту и глубину размытой проблемы. Вы должны взять детали и выяснить проблему, задавая пробные вопросы. Ваши вопросы для выяснения проблемы отражают ваши оценочные способности и компетенции, которые могут быть полезны для компании.

дход сверху вниз и модульность могут очень помочь в решении проблемы. Поэтому вы должны разбить проблему на модули, а затем заняться каждым из них независимо. Впоследствии каждый компонент может быть объяснен как подзадача путем снижения его до уровня известного алгоритма. Э

При устранении узких мест вашей системе может потребоваться балансировщик нагрузки со многими машинами за ней, чтобы обрабатывать пользовательские запросы, или данные могут быть настолько огромными, что вам нужно распределить базу данных по нескольким серверам.

ваша способность говорить об этих компромиссах и оценивать их влияние на систему

три этапа:

Рассмотрение проблемы: не делайте предположений; Задайте уточняющие вопросы, чтобы понять ограничения и варианты использования.

Создание абстрактного проекта Иллюстрирование строительных блоков системы и взаимосвязей между ними.

Выявление и устранение узких мест с использованием фундаментальных принципов построения масштабируемой системы.
30 июля
Павел
Павел 1:53

